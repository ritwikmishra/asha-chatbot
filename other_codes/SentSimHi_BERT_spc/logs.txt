the run_IDs are not ordered
==================================================
--------------------------------------------------
			run_ID=2
GPU is available Tesla K80 11 GB

Batch_size: 32
Train set: 11550 | Validation_set: 3850 | Test_set: 3850
Train_batches: 361 | Validation_batches: 361 | Test_batches: 121
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.05 GB
19 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=10, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=10, out_features=5, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=5, out_features=1, bias=True)
)
Memory taken on GPU 0.692 GB
Memory taken on RAM 1.921 GB
9 seconds taken

Epoch: 1 Train Loss 0.698 Time taken: 1488.54 secs

==================================================
--------------------------------------------------
			run_ID=2
GPU is available Tesla K80 11 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.574 GB
1 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=10, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=10, out_features=5, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=5, out_features=1, bias=True)
)
Memory taken on GPU 0.692 GB
Memory taken on RAM 2.006 GB
9 seconds taken

Epoch: 1 Train Loss 0.704 Time taken: 1486.82 secs
Epoch: 1 Validation Loss 0.696 Time taken: 477.85 secs
Epoch: 2 Train Loss 0.695 Time taken: 1484.82 secs
Epoch: 2 Validation Loss 0.695 Time taken: 475.41 secs
3924.915 secs

T\P		 0		 1
0		0014		0000		
1		0018		0000		
Accuracy 43.75
T\P		 0		 1
0		1942		0000		
1		1908		0000		
Accuracy 50.44155844155844
Time taken: 475.05 secs


==================================================
--------------------------------------------------
			run_ID=3
GPU is available Tesla K80 11 GB

Batch_size: 32
Train set: 11550 | Validation_set: 3850 | Test_set: 3850
Train_batches: 361 | Validation_batches: 361 | Test_batches: 121
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.14 GB
21 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=2000, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=2000, out_features=500, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=500, out_features=1, bias=True)
)
Memory taken on GPU 6.389 GB
Memory taken on RAM 1.963 GB
52 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)


======================================================================
----------------------------------------------------------------------
				run_ID=3
GPU is available Tesla K80 11 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.868 GB
2 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=200, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=100, out_features=1, bias=True)
)
Memory taken on GPU 1.235 GB
Memory taken on RAM 1.999 GB
10 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

Epoch: 1 Train Loss 0.735 Time taken: 1516.95 secs
	Epoch: 1 Validation Loss 0.693 Time taken: 479.95 secs
Epoch: 2 Train Loss 0.693 Time taken: 1519.56 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 478.13 secs
3994.617 secs

T\P		 0		 1
0		1951		0000		
1		1899		0000		
Accuracy 50.6753
Time taken: 478.8 secs
Epoch: 3 Train Loss 0.693 Time taken: 1518.48 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 479.69 secs
Epoch: 4 Train Loss 0.693 Time taken: 1520.68 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 479.41 secs
3998.291 secs

T\P		 0		 1
0		1951		0000		
1		1899		0000		
Accuracy 50.6753
Time taken: 479.28 secs


======================================================================
----------------------------------------------------------------------
				run_ID=4
GPU is available Tesla K80 11 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.927 GB
9 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=200, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=100, out_features=1, bias=True)
)
Memory taken on GPU 1.235 GB
Memory taken on RAM 2.009 GB
52 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Epoch: 1 Train Loss 49.777 Time taken: 1601.31 secs
	Epoch: 1 Validation Loss 49.695 Time taken: 503.35 secs
Epoch: 2 Train Loss 49.897 Time taken: 1603.43 secs
	Epoch: 2 Validation Loss 49.695 Time taken: 502.75 secs
Epoch: 3 Train Loss 49.897 Time taken: 1605.79 secs
	Epoch: 3 Validation Loss 49.695 Time taken: 502.79 secs
6319.455 secs

T\P		 0		 1
0		0000		1951		
1		0000		1899		
Accuracy 49.3247
Time taken: 501.1 secs


======================================================================
----------------------------------------------------------------------
				run_ID=5
GPU is available Tesla P100-PCIE-16GB 16 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.935 GB
6 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=200, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=100, out_features=1, bias=True)
)
Memory taken on GPU 1.235 GB
Memory taken on RAM 2.577 GB
55 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)

Epoch: 1 Train Loss 48.208 Time taken: 424.62 secs
	Epoch: 1 Validation Loss 49.695 Time taken: 132.75 secs
Epoch: 2 Train Loss 49.83 Time taken: 424.41 secs
	Epoch: 2 Validation Loss 49.695 Time taken: 132.71 secs
Epoch: 3 Train Loss 49.897 Time taken: 424.43 secs
	Epoch: 3 Validation Loss 49.695 Time taken: 132.66 secs
Epoch: 4 Train Loss 49.897 Time taken: 424.44 secs
	Epoch: 4 Validation Loss 49.695 Time taken: 132.67 secs
2228.726 secs

T\P		 0		 1
0		0000		1951		
1		0000		1899		
Accuracy 49.3247
Time taken: 132.91 secs


======================================================================
----------------------------------------------------------------------
				run_ID=6
GPU is available Tesla P100-PCIE-16GB 16 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.863 GB
1 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=200, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=100, out_features=1, bias=True)
)
Bert weights frozen

Memory taken on GPU 1.235 GB
Memory taken on RAM 2.557 GB
10 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0002
    weight_decay: 0
)

Epoch: 1 Train Loss 1.086 Time taken: 424.73 secs
	Epoch: 1 Validation Loss 0.693 Time taken: 132.99 secs
Epoch: 2 Train Loss 0.693 Time taken: 424.59 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 132.99 secs
Epoch: 3 Train Loss 0.693 Time taken: 424.68 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 132.95 secs
Epoch: 4 Train Loss 0.693 Time taken: 424.73 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 132.96 secs
2230.683 secs

T\P		 0		 1
0		1951		0000		
1		1899		0000		
Accuracy 50.6753
Time taken: 133.04 secs


======================================================================
----------------------------------------------------------------------
				run_ID=7
GPU is available Tesla P100-PCIE-16GB 16 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.863 GB
1 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=200, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=200, out_features=100, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=100, out_features=1, bias=True)
)
Bert weights frozen

Memory taken on GPU 1.235 GB
Memory taken on RAM 2.547 GB
10 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00035
    weight_decay: 0
)

Epoch: 1 Train Loss 1.095 Time taken: 424.66 secs
	Epoch: 1 Validation Loss 0.694 Time taken: 132.93 secs
Epoch: 2 Train Loss 0.694 Time taken: 424.58 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 132.97 secs
Epoch: 3 Train Loss 0.694 Time taken: 424.69 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 132.93 secs
Epoch: 4 Train Loss 0.693 Time taken: 424.69 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 132.92 secs
2230.387 secs

T\P		 0		 1
0		1951		0000		
1		1899		0000		
Accuracy 50.6753
Time taken: 133.02 secs


======================================================================
----------------------------------------------------------------------
				run_ID=8
GPU is available Tesla K80 11 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.867 GB
1 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=768000, out_features=100, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=100, out_features=100, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=100, out_features=1, bias=True)
)
Bert weights frozen

Memory taken on GPU 0.949 GB
Memory taken on RAM 1.999 GB
50 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.00035
    weight_decay: 0
)

Epoch: 1 Train Loss 4.096 Time taken: 1628.66 secs
	Epoch: 1 Validation Loss 0.693 Time taken: 519.39 secs
Epoch: 2 Train Loss 0.693 Time taken: 1633.17 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 515.99 secs
Epoch: 3 Train Loss 0.693 Time taken: 1628.39 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 515.83 secs
Epoch: 4 Train Loss 0.693 Time taken: 1628.72 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 516.38 secs
8586.587 secs

T\P		 0		 1
0		1951		0000		
1		1899		0000		
Accuracy 50.6753
Time taken: 517.76 secs


======================================================================
----------------------------------------------------------------------
				run_ID=9
GPU is available Tesla P100-PCIE-16GB 16 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.911 GB
7 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=1536, out_features=1000, bias=True)
  (dp1): Dropout(p=0.2, inplace=False)
  (fc2): Linear(in_features=1000, out_features=500, bias=True)
  (dp2): Dropout(p=0.2, inplace=False)
  (fc3): Linear(in_features=500, out_features=100, bias=True)
  (dp3): Dropout(p=0.2, inplace=False)
  (fc4): Linear(in_features=100, out_features=1, bias=True)
)
Bert weights frozen

Memory taken on GPU 0.671 GB
Memory taken on RAM 2.544 GB
24 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

Epoch: 1 Train Loss 0.693 Time taken: 414.32 secs
	Epoch: 1 Validation Loss 0.693 Time taken: 132.18 secs
Epoch: 2 Train Loss 0.693 Time taken: 414.23 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 132.2 secs
Epoch: 3 Train Loss 0.693 Time taken: 414.2 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 132.23 secs
Epoch: 4 Train Loss 0.693 Time taken: 414.18 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 132.16 secs
2185.731 secs

T\P		 0		 1
0		1951		0000		
1		1899		0000		
Accuracy 50.6753
Time taken: 132.3 secs


======================================================================
----------------------------------------------------------------------
				run_ID=10
GPU is available TITAN Xp 12 GB

Batch_size: 4
Train set: 12525 | Validation_set: 4175 | Test_set: 4176
Train_batches: 3132 | Validation_batches: 3132 | Test_batches: 1044
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.59 GB
14 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=1536, out_features=2, bias=True)
)
Bert weights NOT frozen

Memory taken on GPU 0.663 GB
Memory taken on RAM 2.064 GB
6 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

Epoch: 1 Train Loss 0.693 Time taken: 1366.37 secs
X X X X X X X X X X X X X X X X X X X X 
CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.91 GiB total capacity; 9.97 GiB already allocated; 12.00 MiB free; 10.34 GiB reserved in total by PyTor
X X X X X X X X X X X X X X X X X X X X 




======================================================================
----------------------------------------------------------------------
				run_ID=11
GPU is available TITAN Xp 12 GB

Batch_size: 16
Train set: 12525 | Validation_set: 4175 | Test_set: 4176
Train_batches: 783 | Validation_batches: 783 | Test_batches: 261
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.587 GB
14 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=1536, out_features=2, bias=True)
)
Bert weights frozen

Memory taken on GPU 0.663 GB
Memory taken on RAM 2.06 GB
6 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

Epoch: 1 Train Loss 0.694 Time taken: 457.56 secs
	Epoch: 1 Validation Loss 0.693 Time taken: 148.55 secs
Epoch: 2 Train Loss 0.693 Time taken: 468.82 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 148.32 secs
Epoch: 3 Train Loss 0.693 Time taken: 469.0 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 148.49 secs
Epoch: 4 Train Loss 0.693 Time taken: 468.95 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 148.22 secs
T\P		 0		 1
0		0056		0000		
1		0056		0000		
Accuracy 50.0
Time taken: 3.9 secs
2461.819 secs


======================================================================
----------------------------------------------------------------------
				run_ID=12
GPU is available TITAN Xp 12 GB

Batch_size: 16
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.937 GB
0 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=1536, out_features=2, bias=True)
)
Bert weights frozen

Memory taken on GPU 0.663 GB
Memory taken on RAM 2.085 GB
6 seconds taken

Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

Epoch: 1 Train Loss 0.694 Time taken: 458.66 secs
	Epoch: 1 Validation Loss 0.693 Time taken: 148.07 secs
Epoch: 2 Train Loss 0.693 Time taken: 467.8 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 147.87 secs
Epoch: 3 Train Loss 0.693 Time taken: 468.07 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 148.08 secs
Epoch: 4 Train Loss 0.693 Time taken: 467.99 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 148.26 secs
T\P		 0		 1
0		2117		0000		
1		2059		0000		
Accuracy 50.6944
Time taken: 148.38 secs
2603.181 secs



======================================================================
----------------------------------------------------------------------
				run_ID=13
GPU is available TITAN Xp 12 GB

Batch_size: 16
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.937 GB
0 seconds taken

myBert(
  (bert):
  (fc1): Linear(in_features=1536, out_features=2, bias=True)
)
Bert weights frozen

Memory taken on GPU 0.663 GB
Memory taken on RAM 2.086 GB
6 seconds taken

SGD (
Parameter Group 0
    dampening: 0
    lr: 0.0001
    momentum: 0
    nesterov: False
    weight_decay: 0
)

Epoch: 1 Train Loss 0.694 Time taken: 448.49 secs
	Epoch: 1 Validation Loss 0.693 Time taken: 149.44 secs
Epoch: 2 Train Loss 0.694 Time taken: 471.87 secs
	Epoch: 2 Validation Loss 0.693 Time taken: 149.71 secs
Epoch: 3 Train Loss 0.693 Time taken: 471.87 secs
	Epoch: 3 Validation Loss 0.693 Time taken: 149.72 secs
Epoch: 4 Train Loss 0.694 Time taken: 471.81 secs
	Epoch: 4 Validation Loss 0.693 Time taken: 149.77 secs
T\P		 0		 1
0		2064		0053		
1		1990		0069		
Accuracy 51.0776
Time taken: 149.68 secs
2612.35 secs




======================================================================
----------------------------------------------------------------------
				run_ID=14
GPU is available TITAN Xp 12 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.141 GB
0 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.087 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.224 Time taken: 1.87 secs
	Epoch: 1 Validation Loss 0.068 Time taken: 0.89 secs
T\P		 0		 1
0		0075		0000		
1		0022		0063		
Accuracy 86.25
Time taken: 0.96 secs
3.734 secs
2020-10-15 00:30:57.517875

======================================================================
----------------------------------------------------------------------
				run_ID=15
GPU is available TITAN Xp 12 GB

Batch_size: 32
Train set: 14613 | Validation_set: 2087 | Test_set: 4176
Train_batches: 457 | Validation_batches: 457 | Test_batches: 131
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.354 GB
35 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.048 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.053 Time taken: 132.92 secs
	Epoch: 1 Validation Loss 0.034 Time taken: 7.18 secs
Epoch: 2 Train Loss 0.053 Time taken: 133.81 secs
	Epoch: 2 Validation Loss 0.058 Time taken: 7.03 secs
Epoch: 3 Train Loss 0.042 Time taken: 135.21 secs
	Epoch: 3 Validation Loss 0.051 Time taken: 6.99 secs
T\P		 0		 1
0		2108		0001		
1		1458		0609		
Accuracy 65.0623
Time taken: 13.75 secs
436.883 secs
2020-10-15 00:42:34.932572


======================================================================
----------------------------------------------------------------------
				run_ID=16
GPU is available TITAN Xp 12 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.141 GB
0 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.089 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.051 Time taken: 132.71 secs
	Epoch: 1 Validation Loss 0.045 Time taken: 13.1 secs| Accuracy 95.256
Epoch: 2 Train Loss 0.042 Time taken: 134.71 secs
	Epoch: 2 Validation Loss 0.032 Time taken: 13.15 secs| Accuracy 95.831
Epoch: 3 Train Loss 0.041 Time taken: 134.63 secs
	Epoch: 3 Validation Loss 0.035 Time taken: 13.4 secs| Accuracy 94.25
T\P		 0		 1
0		2088		0021		
1		0208		1859		
Accuracy 94.5163
Time taken: 13.6 secs
cut_off value = 0.8
455.546 secs
2020-10-15 08:36:27.062769


======================================================================
----------------------------------------------------------------------
				run_ID=17
GPU is available TITAN Xp 12 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.141 GB
0 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.031 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.051 Time taken: 133.66 secs
	Epoch: 1 Validation Loss 0.045 Time taken: 13.23 secs| Accuracy 95.256
T\P		 0		 1
0		1976		0133		
1		0099		1968		
Accuracy 94.4444
Time taken: 12.92 secs
cut_off value = 0.8
159.84 secs
2020-10-15 08:39:38.860008


======================================================================
----------------------------------------------------------------------
				run_ID=18
GPU is available TITAN Xp 12 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.141 GB
0 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.031 GB
12 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.051 Time taken: 132.39 secs
	Epoch: 1 Validation Loss 0.045 Time taken: 13.23 secs| Accuracy 95.256
Epoch: 2 Train Loss 0.042 Time taken: 136.43 secs
	Epoch: 2 Validation Loss 0.032 Time taken: 13.49 secs| Accuracy 95.831
Epoch: 3 Train Loss 0.041 Time taken: 134.92 secs
	Epoch: 3 Validation Loss 0.035 Time taken: 13.89 secs| Accuracy 94.25
Epoch: 4 Train Loss 0.038 Time taken: 137.55 secs
	Epoch: 4 Validation Loss 0.046 Time taken: 13.75 secs| Accuracy 95.065
T\P		 0		 1
0		2002		0107		
1		0101		1966		
Accuracy 95.0192
Time taken: 13.46 secs
cut_off value = 0.8
609.391 secs
2020-10-15 08:50:06.197807


======================================================================
----------------------------------------------------------------------
				run_ID=19
GPU is available TITAN Xp 12 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.14 GB
0 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.078 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.051 Time taken: 134.74 secs
	Epoch: 1 Validation Loss 0.045 Time taken: 13.49 secs| Accuracy 94.729
Epoch: 2 Train Loss 0.042 Time taken: 136.1 secs
	Epoch: 2 Validation Loss 0.032 Time taken: 13.03 secs| Accuracy 95.783
Epoch: 3 Train Loss 0.041 Time taken: 130.41 secs
	Epoch: 3 Validation Loss 0.035 Time taken: 12.94 secs| Accuracy 95.352
T\P		 0		 1
0		2056		0053		
1		0151		1916		
Accuracy 95.1149
Time taken: 13.32 secs
cut_off value = 0.5
454.18 secs
2020-10-15 08:58:07.990847



======================================================================
----------------------------------------------------------------------
				run_ID=20
GPU is available TITAN Xp 12 GB

Max Length:120
Batch_size: 32
Train set: 14613 | Validation_set: 2087 | Test_set: 4176
Train_batches: 457 | Validation_batches: 457 | Test_batches: 131
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.431 GB
15 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.061 GB
66 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.052 Time taken: 147.44 secs
	Epoch: 1 Validation Loss 0.042 Time taken: 23.14 secs| Epoch: 2 Train Loss 0.058 Time taken: 142.84 secs
	Epoch: 2 Validation Loss 0.049 Time taken: 6.35 secs| Epoch: 3 Train Loss 0.043 Time taken: 138.48 secs
	Epoch: 3 Validation Loss 0.038 Time taken: 6.52 secs| T\P		 0		 1
0		2027		0082		
1		0103		1964		
Accuracy 95.5699
Time taken: 13.32 secs
cut_off value = 0.5
478.09 secs
2020-10-20 12:46:10.504180



======================================================================
----------------------------------------------------------------------
				run_ID=22
Mon Jul  5 13:05:44 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.239 GB
0 seconds taken

BertForSequenceClassificationMemory taken on GPU 0.663 GB
Memory taken on RAM 2.169 GB
6 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.101 Time taken: 476.92 secs
	Epoch: 1 Validation Loss 0.073 Time taken: 47.46 secs| Accuracy 89.969
Epoch: 2 Train Loss 0.057 Time taken: 479.23 secs
	Epoch: 2 Validation Loss 0.058 Time taken: 47.5 secs| Accuracy 92.851
Epoch: 3 Train Loss 0.048 Time taken: 479.45 secs
	Epoch: 3 Validation Loss 0.055 Time taken: 47.39 secs| Accuracy 93.049
Epoch: 4 Train Loss 0.043 Time taken: 478.36 secs
	Epoch: 4 Validation Loss 0.064 Time taken: 47.25 secs| Accuracy 92.173
Epoch: 5 Train Loss 0.042 Time taken: 477.83 secs
	Epoch: 5 Validation Loss 0.069 Time taken: 47.34 secs| Accuracy 92.229
Epoch: 6 Train Loss 0.034 Time taken: 478.05 secs
	Epoch: 6 Validation Loss 0.06 Time taken: 47.35 secs| Accuracy 93.755
Epoch: 7 Train Loss 0.032 Time taken: 479.74 secs
	Epoch: 7 Validation Loss 0.066 Time taken: 47.39 secs| Accuracy 92.653
Epoch: 8 Train Loss 0.198 Time taken: 478.41 secs
	Epoch: 8 Validation Loss 0.251 Time taken: 45.06 secs| Accuracy 50.579
Epoch: 9 Train Loss 0.252 Time taken: 478.03 secs
	Epoch: 9 Validation Loss 0.25 Time taken: 45.17 secs| Accuracy 50.579
Epoch: 10 Train Loss 0.252 Time taken: 478.26 secs
	Epoch: 10 Validation Loss 0.25 Time taken: 45.07 secs| Accuracy 50.579
T\P		 0		 1
0		3590		0000		
1		3488		0000		
Accuracy 50.7205
Time taken: 45.58 secs
5313.281 secs

T\P		 0		 1
0		3282		0308		
1		0199		3289		
Accuracy 92.837
Time taken: 49.23 secs


======================================================================
----------------------------------------------------------------------
				run_ID=23
Mon Jul  5 15:37:17 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.096 GB
28 seconds taken

BertForSequenceClassification|random-negative-sents|bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.183 GB
6 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.041 Time taken: 453.84 secs

	Epoch: 1 Validation Loss 0.035 Time taken: 44.42 secs| Accuracy 96.072

Epoch: 2 Train Loss 0.033 Time taken: 460.18 secs

	Epoch: 2 Validation Loss 0.066 Time taken: 44.33 secs| Accuracy 93.331

Epoch: 3 Train Loss 0.028 Time taken: 460.07 secs

	Epoch: 3 Validation Loss 0.042 Time taken: 44.14 secs| Accuracy 95.451

Epoch: 4 Train Loss 0.024 Time taken: 459.78 secs

	Epoch: 4 Validation Loss 0.035 Time taken: 44.33 secs| Accuracy 96.864

Epoch: 5 Train Loss 0.021 Time taken: 460.12 secs

	Epoch: 5 Validation Loss 0.032 Time taken: 44.02 secs| Accuracy 96.892

Epoch: 6 Train Loss 0.02 Time taken: 459.93 secs

	Epoch: 6 Validation Loss 0.044 Time taken: 44.05 secs| Accuracy 95.875

Epoch: 7 Train Loss 0.018 Time taken: 459.94 secs

	Epoch: 7 Validation Loss 0.034 Time taken: 44.1 secs| Accuracy 96.864

Epoch: 8 Train Loss 0.125 Time taken: 459.85 secs

	Epoch: 8 Validation Loss 0.25 Time taken: 42.71 secs| Accuracy 50.579

Epoch: 9 Train Loss 0.252 Time taken: 459.06 secs

	Epoch: 9 Validation Loss 0.25 Time taken: 42.54 secs| Accuracy 50.579

Epoch: 10 Train Loss 0.252 Time taken: 459.28 secs

	Epoch: 10 Validation Loss 0.25 Time taken: 42.55 secs| Accuracy 50.579
T\P		 0		 1
0		3590		0000		
1		3488		0000		

Accuracy 50.7205
Time taken: 42.57 secs
5081.422 secs


On the best model
T\P		 0		 1
0		3439		0151		
1		0055		3433		

Accuracy 97.0896
Time taken: 44.69 secs


======================================================================
----------------------------------------------------------------------
				run_ID=24
Mon Jul  5 14:55:05 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.108 GB
31 seconds taken

BertForSequenceClassification|with-label-smoothing|bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.17 GB
6 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.128 Time taken: 466.81 secs

	Epoch: 1 Validation Loss 0.081 Time taken: 45.47 secs| Accuracy 89.545

Epoch: 2 Train Loss 0.062 Time taken: 470.54 secs

	Epoch: 2 Validation Loss 0.061 Time taken: 45.19 secs| Accuracy 92.738

Epoch: 3 Train Loss 0.05 Time taken: 470.63 secs

	Epoch: 3 Validation Loss 0.077 Time taken: 45.24 secs| Accuracy 90.986

Epoch: 4 Train Loss 0.045 Time taken: 470.77 secs

	Epoch: 4 Validation Loss 0.08 Time taken: 45.78 secs| Accuracy 92.823

Epoch: 5 Train Loss 0.041 Time taken: 471.14 secs

	Epoch: 5 Validation Loss 0.06 Time taken: 45.4 secs| Accuracy 92.314

Epoch: 6 Train Loss 0.217 Time taken: 470.5 secs

	Epoch: 6 Validation Loss 0.251 Time taken: 43.69 secs| Accuracy 49.421

Epoch: 7 Train Loss 0.253 Time taken: 471.22 secs

	Epoch: 7 Validation Loss 0.25 Time taken: 43.93 secs| Accuracy 50.579

Epoch: 8 Train Loss 0.253 Time taken: 470.58 secs

	Epoch: 8 Validation Loss 0.25 Time taken: 43.29 secs| Accuracy 50.579

Epoch: 9 Train Loss 0.252 Time taken: 469.57 secs

	Epoch: 9 Validation Loss 0.25 Time taken: 43.5 secs| Accuracy 50.579

Epoch: 10 Train Loss 0.252 Time taken: 469.14 secs

	Epoch: 10 Validation Loss 0.251 Time taken: 43.51 secs| Accuracy 50.579
T\P		 0		 1
0		3590		0000		
1		3488		0000		

Accuracy 50.7205
Time taken: 43.5 secs
5198.576 secs


On the best model
T\P		 0		 1
0		3219		0371		
1		0161		3327		

Accuracy 92.4838
Time taken: 45.07 secs


======================================================================
----------------------------------------------------------------------
				run_ID=25
Tue Jul  6 18:10:29 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 10617 | Test_set: 7078
Train_batches: 553 | Validation_batches: 553 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.097 GB
28 seconds taken

BertForSequenceClassification| 50% in training, 30% validation, 20% test|bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.192 GB
7 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

Epoch: 1 Train Loss 0.111 Time taken: 342.47 secs

	Epoch: 1 Validation Loss 0.067 Time taken: 143.67 secs| Accuracy 91.401

Epoch: 2 Train Loss 0.058 Time taken: 350.66 secs

	Epoch: 2 Validation Loss 0.07 Time taken: 142.5 secs| Accuracy 92.964

Epoch: 3 Train Loss 0.049 Time taken: 350.12 secs

	Epoch: 3 Validation Loss 0.109 Time taken: 142.98 secs| Accuracy 90.007

Epoch: 4 Train Loss 0.043 Time taken: 350.38 secs

	Epoch: 4 Validation Loss 0.06 Time taken: 142.97 secs| Accuracy 92.729

Epoch: 5 Train Loss 0.04 Time taken: 349.9 secs

	Epoch: 5 Validation Loss 0.06 Time taken: 142.76 secs| Accuracy 92.842

Epoch: 6 Train Loss 0.038 Time taken: 349.9 secs

	Epoch: 6 Validation Loss 0.066 Time taken: 142.37 secs| Accuracy 92.116

Epoch: 7 Train Loss 0.077 Time taken: 349.69 secs

	Epoch: 7 Validation Loss 0.083 Time taken: 142.06 secs| Accuracy 90.459

Epoch: 8 Train Loss 0.047 Time taken: 349.83 secs

	Epoch: 8 Validation Loss 0.075 Time taken: 142.37 secs| Accuracy 91.919

Epoch: 9 Train Loss 0.029 Time taken: 349.99 secs

	Epoch: 9 Validation Loss 0.068 Time taken: 141.66 secs| Accuracy 92.625

Epoch: 10 Train Loss 0.024 Time taken: 350.12 secs

	Epoch: 10 Validation Loss 0.066 Time taken: 142.53 secs| Accuracy 92.399
T\P		 0		 1
0		3219		0371		
1		0204		3284		

Accuracy 91.8762
Time taken: 47.97 secs
4974.557 secs


On the best model
T\P		 0		 1
0		3231		0359		
1		0184		3304		

Accuracy 92.3283
Time taken: 43.24 secs



======================================================================
----------------------------------------------------------------------
				run_ID=26
Tue Jul  6 21:50:48 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 10617 | Test_set: 7078
Train_batches: 553 | Validation_batches: 553 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.134 GB
34 seconds taken

BertForSequenceClassification| 50% in training, 30% validation, 20% test| random negative-sentence|bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.169 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.042 Time taken: 327.48 secs

	Epoch: 1 Validation Loss 0.058 Time taken: 133.43 secs| Accuracy 94.725

Epoch: 2 Train Loss 0.046 Time taken: 330.18 secs

	Epoch: 2 Validation Loss 0.046 Time taken: 132.77 secs| Accuracy 96.327

Epoch: 3 Train Loss 0.027 Time taken: 329.93 secs

	Epoch: 3 Validation Loss 0.03 Time taken: 133.01 secs| Accuracy 97.014
T\P		 0		 1
0		3504		0086		
1		0104		3384		

Accuracy 97.3156
Time taken: 44.82 secs
1446.922 secs


On the best model
T\P		 0		 1
0		3504		0086		
1		0104		3384		

Accuracy 97.3156
Time taken: 44.83 secs


======================================================================
----------------------------------------------------------------------
				run_ID=27
Sat Jul 10 14:59:04 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 10617 | Test_set: 7078
Train_batches: 553 | Validation_batches: 553 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.131 GB
14 seconds taken

BertForSequenceClassification| train_ratio:0.5 val_ratio:0.3 | random negative-sentence: True| model:ai4bharat/indic-bert
Memory taken on GPU 0.125 GB
Memory taken on RAM 1.297 GB
4 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.121 Time taken: 294.52 secs

	Epoch: 1 Validation Loss 0.033 Time taken: 131.14 secs| Accuracy 96.072

Epoch: 2 Train Loss 0.031 Time taken: 294.9 secs

	Epoch: 2 Validation Loss 0.032 Time taken: 131.78 secs| Accuracy 95.969

Epoch: 3 Train Loss 0.029 Time taken: 293.93 secs

	Epoch: 3 Validation Loss 0.033 Time taken: 130.68 secs| Accuracy 95.912

Epoch: 4 Train Loss 0.021 Time taken: 293.55 secs

	Epoch: 4 Validation Loss 0.03 Time taken: 130.29 secs| Accuracy 96.336

Epoch: 5 Train Loss 0.017 Time taken: 294.55 secs

	Epoch: 5 Validation Loss 0.043 Time taken: 131.38 secs| Accuracy 94.867
T\P		 0		 1
0		3579		0011		
1		0392		3096		

Accuracy 94.3063
Time taken: 44.08 secs
2172.449 secs


On the best model
T\P		 0		 1
0		3540		0050		
1		0217		3271		

Accuracy 96.2277
Time taken: 44.28 secs


======================================================================
----------------------------------------------------------------------
				run_ID=28
Sat Jul 10 15:38:10 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 10617 | Test_set: 7078
Train_batches: 553 | Validation_batches: 553 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.146 GB
12 seconds taken

BertForSequenceClassification| train_ratio:0.5 val_ratio:0.3 | random negative-sentence: False| model:ai4bharat/indic-bert
Memory taken on GPU 0.125 GB
Memory taken on RAM 1.126 GB
4 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.253 Time taken: 294.79 secs

	Epoch: 1 Validation Loss 0.249 Time taken: 130.21 secs| Accuracy 55.873

Epoch: 2 Train Loss 0.25 Time taken: 292.42 secs

	Epoch: 2 Validation Loss 0.244 Time taken: 129.01 secs| Accuracy 54.469

Epoch: 3 Train Loss 0.21 Time taken: 291.54 secs

	Epoch: 3 Validation Loss 0.127 Time taken: 128.39 secs| Accuracy 82.274

Epoch: 4 Train Loss 0.111 Time taken: 291.66 secs

	Epoch: 4 Validation Loss 0.094 Time taken: 128.46 secs| Accuracy 87.049

Epoch: 5 Train Loss 0.08 Time taken: 291.91 secs

	Epoch: 5 Validation Loss 0.081 Time taken: 128.45 secs| Accuracy 88.914
T\P		 0		 1
0		3115		0475		
1		0281		3207		

Accuracy 89.319
Time taken: 43.02 secs
2152.969 secs


On the best model
T\P		 0		 1
0		3115		0475		
1		0281		3207		

Accuracy 89.319
Time taken: 43.64 secs


======================================================================
----------------------------------------------------------------------
				run_ID=29
Sat Jul 10 17:08:21 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 31851 | Validation_set: 1769 | Test_set: 1770
Train_batches: 996 | Validation_batches: 996 | Test_batches: 56
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.137 GB
10 seconds taken

BertForSequenceClassification| train_ratio:0.9 val_ratio:0.05 | random negative-sentence: False| model:ai4bharat/indic-bert
Memory taken on GPU 0.125 GB
Memory taken on RAM 1.109 GB
4 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.252 Time taken: 529.2 secs

	Epoch: 1 Validation Loss 0.25 Time taken: 21.89 secs| Accuracy 52.289

Epoch: 2 Train Loss 0.251 Time taken: 529.06 secs

	Epoch: 2 Validation Loss 0.249 Time taken: 21.92 secs| Accuracy 51.611

Epoch: 3 Train Loss 0.25 Time taken: 529.6 secs

	Epoch: 3 Validation Loss 0.261 Time taken: 21.63 secs| Accuracy 48.728

Epoch: 4 Train Loss 0.251 Time taken: 524.25 secs

	Epoch: 4 Validation Loss 0.251 Time taken: 21.47 secs| Accuracy 48.728

Epoch: 5 Train Loss 0.251 Time taken: 522.96 secs

	Epoch: 5 Validation Loss 0.25 Time taken: 21.47 secs| Accuracy 48.728
T\P		 0		 1
0		0000		0906		
1		0000		0864		

Accuracy 48.8136
Time taken: 10.81 secs
2754.965 secs


On the best model
T\P		 0		 1
0		0056		0850		
1		0003		0861		

Accuracy 51.8079
Time taken: 10.96 secs

======================================================================
----------------------------------------------------------------------

				run_ID=30
Sat Jul 10 20:58:28 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 10617 | Test_set: 7078
Train_batches: 553 | Validation_batches: 553 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.039 GB
11 seconds taken

SequenceClassification| train_ratio:0.5 val_ratio:0.3 | random negative-sentence: False| model:xlm-roberta-base
Memory taken on GPU 1.036 GB
Memory taken on RAM 2.552 GB
7 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.263 Time taken: 330.4 secs

	Epoch: 1 Validation Loss 0.254 Time taken: 116.88 secs| Accuracy 49.477

Epoch: 2 Train Loss 0.261 Time taken: 336.64 secs

	Epoch: 2 Validation Loss 0.258 Time taken: 116.87 secs| Accuracy 49.477

Epoch: 3 Train Loss 0.259 Time taken: 335.59 secs

	Epoch: 3 Validation Loss 0.256 Time taken: 116.81 secs| Accuracy 49.477

Epoch: 4 Train Loss 0.257 Time taken: 335.77 secs

	Epoch: 4 Validation Loss 0.251 Time taken: 117.09 secs| Accuracy 49.477

Epoch: 5 Train Loss 0.256 Time taken: 336.52 secs

	Epoch: 5 Validation Loss 0.259 Time taken: 116.88 secs| Accuracy 49.477
T\P		 0		 1
0		0000		3590		
1		0000		3488		

Accuracy 49.2795
Time taken: 39.58 secs
2305.624 secs


On the best model
T\P		 0		 1
0		0000		3590		
1		0000		3488		

Accuracy 49.2795
Time taken: 40.15 secs


======================================================================
----------------------------------------------------------------------
				run_ID=31
Sun Jul 18 23:06:56 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 14613 | Validation_set: 2087 | Test_set: 4176
Train_batches: 457 | Validation_batches: 457 | Test_batches: 131
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.013 GB
20 seconds taken

19 experiment repeat SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.975 GB
6 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.05 Time taken: 261.4 secs

	Epoch: 1 Validation Loss 0.092 Time taken: 26.54 secs| Accuracy 88.117

Epoch: 2 Train Loss 0.048 Time taken: 272.28 secs

	Epoch: 2 Validation Loss 0.037 Time taken: 26.56 secs| Accuracy 95.4

Epoch: 3 Train Loss 0.052 Time taken: 273.39 secs

	Epoch: 3 Validation Loss 0.034 Time taken: 26.61 secs| Accuracy 95.736
T\P		 0		 1
0		2007		0102		
1		0114		1953		

Accuracy 94.8276
Time taken: 26.69 secs
933.772 secs


On the best model
T\P		 0		 1
0		2007		0102		
1		0114		1953		

Accuracy 94.8276
Time taken: 26.62 secs


======================================================================
----------------------------------------------------------------------
				run_ID=32
Mon Jul 19 16:05:35 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.101 GB
31 seconds taken

SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:xlm-mlm-100-1280
Memory taken on GPU 2.16 GB
Memory taken on RAM 2.584 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)

X X X X X X X X X X X X X X X X X X X X 
CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.16 GiB already allocated; 19.75 MiB free; 13.68 GiB reserved in total by PyTo
X X X X X X X X X X X X X X X X X X X X 



======================================================================
----------------------------------------------------------------------
				run_ID=33
Mon Jul 19 16:07:32 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.1 GB
39 seconds taken

SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:xlm-mlm-tlm-xnli15-1024
Memory taken on GPU 0.928 GB
Memory taken on RAM 2.192 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 3.116 Time taken: 510.89 secs

	Epoch: 1 Validation Loss 0.296 Time taken: 132.38 secs| Accuracy 50.494

Epoch: 2 Train Loss 0.474 Time taken: 518.75 secs

	Epoch: 2 Validation Loss 0.262 Time taken: 131.7 secs| Accuracy 49.506

Epoch: 3 Train Loss 0.361 Time taken: 518.45 secs

	Epoch: 3 Validation Loss 0.221 Time taken: 132.08 secs| Accuracy 65.824

Epoch: 4 Train Loss 0.259 Time taken: 518.15 secs

	Epoch: 4 Validation Loss 0.192 Time taken: 131.85 secs| Accuracy 74.117

Epoch: 5 Train Loss 0.195 Time taken: 518.78 secs

	Epoch: 5 Validation Loss 0.177 Time taken: 131.41 secs| Accuracy 76.378
T\P		 0		 1
0		3808		1572		
1		0900		4337		

Accuracy 76.7166
Time taken: 99.44 secs
3362.084 secs


On the best model
T\P		 0		 1
0		3808		1572		
1		0900		4337		

Accuracy 76.7166
Time taken: 99.8 secs


======================================================================
----------------------------------------------------------------------
				run_ID=34
Mon Jul 19 16:16:13 2021
GPU is available Tesla K80 11 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.153 GB
59 seconds taken

SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:xlm-mlm-xnli15-1024
Memory taken on GPU 0.928 GB
Memory taken on RAM 2.154 GB
148 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 4.826 Time taken: 896.74 secs

	Epoch: 1 Validation Loss 0.275 Time taken: 247.16 secs| Accuracy 49.506

Epoch: 2 Train Loss 0.362 Time taken: 896.79 secs

	Epoch: 2 Validation Loss 0.275 Time taken: 246.76 secs| Accuracy 49.506

Epoch: 3 Train Loss 0.31 Time taken: 902.3 secs

	Epoch: 3 Validation Loss 0.254 Time taken: 248.87 secs| Accuracy 49.506

Epoch: 4 Train Loss 0.297 Time taken: 900.96 secs

	Epoch: 4 Validation Loss 0.267 Time taken: 248.4 secs| Accuracy 49.506

Epoch: 5 Train Loss 0.287 Time taken: 906.09 secs

	Epoch: 5 Validation Loss 0.25 Time taken: 250.24 secs| Accuracy 50.494
T\P		 0		 1
0		5380		0000		
1		5237		0000		

Accuracy 50.6734
Time taken: 187.05 secs
5941.851 secs


On the best model
T\P		 0		 1
0		5380		0000		
1		5237		0000		

Accuracy 50.6734
Time taken: 186.41 secs



======================================================================
----------------------------------------------------------------------
Resuming 33...
				run_ID=35
Mon Jul 19 17:14:47 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.106 GB
42 seconds taken

SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:xlm-mlm-tlm-xnli15-1024
Memory taken on GPU 0.928 GB
Memory taken on RAM 2.156 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.277 Time taken: 518.79 secs

	Epoch: 1 Validation Loss 0.148 Time taken: 131.24 secs| Accuracy 79.797

Epoch: 2 Train Loss 0.146 Time taken: 518.88 secs

	Epoch: 2 Validation Loss 0.137 Time taken: 131.37 secs| Accuracy 81.859

Epoch: 3 Train Loss 0.117 Time taken: 518.56 secs

	Epoch: 3 Validation Loss 0.14 Time taken: 130.82 secs| Accuracy 83.3
T\P		 0		 1
0		4241		1139		
1		0687		4550		

Accuracy 82.8012
Time taken: 98.52 secs
2060.716 secs


On the best model
T\P		 0		 1
0		4241		1139		
1		0687		4550		

Accuracy 82.8012
Time taken: 98.66 secs


======================================================================
----------------------------------------------------------------------
Resuming 35...
				run_ID=36
Mon Jul 19 17:54:49 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.1 GB
37 seconds taken

SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:xlm-mlm-tlm-xnli15-1024
Memory taken on GPU 0.928 GB
Memory taken on RAM 2.179 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 516.69 secs

	Epoch: 1 Validation Loss 0.199 Time taken: 129.2 secs| Accuracy 68.508

Epoch: 2 Train Loss 0.23 Time taken: 518.33 secs

	Epoch: 2 Validation Loss 0.252 Time taken: 128.23 secs| Accuracy 49.506

Epoch: 3 Train Loss 0.267 Time taken: 518.26 secs

	Epoch: 3 Validation Loss 0.254 Time taken: 128.34 secs| Accuracy 49.506
T\P		 0		 1
0		0000		5380		
1		0000		5237		

Accuracy 49.3266
Time taken: 96.47 secs
2040.134 secs


On the best model
T\P		 0		 1
0		2174		3206		
1		0137		5100		

Accuracy 68.5128
Time taken: 97.99 secs



======================================================================
----------------------------------------------------------------------
				run_ID=37
Mon Jul 19 18:19:39 2021
GPU is available Tesla K80 11 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.109 GB
51 seconds taken

SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:bert-base-multilingual-uncased
Memory taken on GPU 0.624 GB
Memory taken on RAM 1.443 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 0.0001
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.048 Time taken: 549.32 secs

	Epoch: 1 Validation Loss 0.037 Time taken: 154.31 secs| Accuracy 96.256

Epoch: 2 Train Loss 0.037 Time taken: 553.37 secs

	Epoch: 2 Validation Loss 0.048 Time taken: 155.22 secs| Accuracy 95.988

Epoch: 3 Train Loss 0.033 Time taken: 552.94 secs

	Epoch: 3 Validation Loss 0.024 Time taken: 155.01 secs| Accuracy 97.217

Epoch: 4 Train Loss 0.027 Time taken: 552.94 secs

	Epoch: 4 Validation Loss 0.035 Time taken: 155.05 secs| Accuracy 96.807

Epoch: 5 Train Loss 0.08 Time taken: 552.33 secs

	Epoch: 5 Validation Loss 0.253 Time taken: 154.72 secs| Accuracy 49.506
T\P		 0		 1
0		0000		5380		
1		0000		5237		

Accuracy 49.3266
Time taken: 117.43 secs
3659.196 secs


On the best model
T\P		 0		 1
0		5301		0079		
1		0193		5044		

Accuracy 97.4381
Time taken: 117.89 secs



======================================================================
----------------------------------------------------------------------
				run_ID=38
Mon Jul 19 18:33:39 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.107 GB
38 seconds taken

lower alpha| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.192 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 5e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.029 Time taken: 470.37 secs

	Epoch: 1 Validation Loss 0.028 Time taken: 46.66 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.02 Time taken: 476.9 secs

	Epoch: 2 Validation Loss 0.019 Time taken: 46.43 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.013 Time taken: 475.84 secs

	Epoch: 3 Validation Loss 0.013 Time taken: 46.4 secs| Accuracy 98.615

Epoch: 4 Train Loss 0.013 Time taken: 476.94 secs

	Epoch: 4 Validation Loss 0.021 Time taken: 46.32 secs| Accuracy 97.711

Epoch: 5 Train Loss 0.014 Time taken: 476.96 secs

	Epoch: 5 Validation Loss 0.017 Time taken: 46.45 secs| Accuracy 98.361
T\P		 0		 1
0		3538		0052		
1		0065		3423		

Accuracy 98.347
Time taken: 46.31 secs
2664.735 secs


On the best model
T\P		 0		 1
0		3538		0052		
1		0076		3412		

Accuracy 98.1916
Time taken: 47.06 secs


======================================================================
----------------------------------------------------------------------
				run_ID=39
Mon Jul 19 19:29:24 2021
GPU is available Tesla K80 11 GB

Batch_size: 32
Train set: 14613 | Validation_set: 2087 | Test_set: 4176
Train_batches: 457 | Validation_batches: 457 | Test_batches: 131
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.012 GB
33 seconds taken

19 repeated with lower alpha | SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.276 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 5e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.045 Time taken: 460.0 secs

	Epoch: 1 Validation Loss 0.033 Time taken: 46.06 secs| Accuracy 96.119

Epoch: 2 Train Loss 0.036 Time taken: 463.03 secs

	Epoch: 2 Validation Loss 0.032 Time taken: 45.9 secs| Accuracy 96.358

Epoch: 3 Train Loss 0.031 Time taken: 462.94 secs

	Epoch: 3 Validation Loss 0.03 Time taken: 46.09 secs| Accuracy 96.263

Epoch: 4 Train Loss 0.026 Time taken: 462.98 secs

	Epoch: 4 Validation Loss 0.03 Time taken: 45.78 secs| Accuracy 96.838

Epoch: 5 Train Loss 0.026 Time taken: 461.65 secs

	Epoch: 5 Validation Loss 0.029 Time taken: 45.51 secs| Accuracy 96.31
T\P		 0		 1
0		2022		0087		
1		0076		1991		

Accuracy 96.0967
Time taken: 45.94 secs
2595.957 secs


On the best model
T\P		 0		 1
0		2072		0037		
1		0099		1968		

Accuracy 96.7433
Time taken: 45.96 secs



======================================================================
----------------------------------------------------------------------
				run_ID=40
Mon Jul 19 19:34:54 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.108 GB
35 seconds taken

lower alpha| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.163 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 5e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.079 Time taken: 468.13 secs

	Epoch: 1 Validation Loss 0.054 Time taken: 46.44 secs| Accuracy 93.473

Epoch: 2 Train Loss 0.037 Time taken: 476.63 secs

	Epoch: 2 Validation Loss 0.047 Time taken: 46.44 secs| Accuracy 94.603

Epoch: 3 Train Loss 0.028 Time taken: 476.92 secs

	Epoch: 3 Validation Loss 0.039 Time taken: 46.59 secs| Accuracy 95.507

Epoch: 4 Train Loss 0.02 Time taken: 476.68 secs

	Epoch: 4 Validation Loss 0.046 Time taken: 46.62 secs| Accuracy 94.688

Epoch: 5 Train Loss 0.019 Time taken: 477.68 secs

	Epoch: 5 Validation Loss 0.044 Time taken: 46.3 secs| Accuracy 94.744
T\P		 0		 1
0		3392		0198		
1		0155		3333		

Accuracy 95.0127
Time taken: 46.19 secs
2664.237 secs


On the best model
T\P		 0		 1
0		3392		0198		
1		0141		3347		

Accuracy 95.2105
Time taken: 47.05 secs


======================================================================
----------------------------------------------------------------------
				run_ID=41
Mon Jul 19 21:29:10 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.146 GB
36 seconds taken

even lower alpha| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.182 GB
34 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.027 Time taken: 471.75 secs

	Epoch: 1 Validation Loss 0.016 Time taken: 46.15 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.013 Time taken: 473.64 secs

	Epoch: 2 Validation Loss 0.014 Time taken: 45.37 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.008 Time taken: 467.3 secs

	Epoch: 3 Validation Loss 0.013 Time taken: 45.34 secs| Accuracy 98.615

Epoch: 4 Train Loss 0.006 Time taken: 467.93 secs

	Epoch: 4 Validation Loss 0.017 Time taken: 45.54 secs| Accuracy 98.305

Epoch: 5 Train Loss 0.005 Time taken: 468.2 secs

	Epoch: 5 Validation Loss 0.013 Time taken: 45.73 secs| Accuracy 98.785
T\P		 0		 1
0		3570		0020		
1		0092		3396		

Accuracy 98.4176
Time taken: 45.17 secs
2635.259 secs


On the best model
T\P		 0		 1
0		3570		0020		
1		0092		3396		

Accuracy 98.4176
Time taken: 45.09 secs



======================================================================
----------------------------------------------------------------------
				run_ID=42
Mon Jul 19 21:33:30 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.129 GB
34 seconds taken

even lower alpha | SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.23 GB
31 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.086 Time taken: 484.63 secs

	Epoch: 1 Validation Loss 0.051 Time taken: 47.5 secs| Accuracy 93.897

Epoch: 2 Train Loss 0.035 Time taken: 488.6 secs

	Epoch: 2 Validation Loss 0.039 Time taken: 47.89 secs| Accuracy 95.535

Epoch: 3 Train Loss 0.021 Time taken: 488.48 secs

	Epoch: 3 Validation Loss 0.032 Time taken: 47.29 secs| Accuracy 95.733

Epoch: 4 Train Loss 0.015 Time taken: 488.62 secs

	Epoch: 4 Validation Loss 0.034 Time taken: 47.85 secs| Accuracy 96.016

Epoch: 5 Train Loss 0.012 Time taken: 489.58 secs

	Epoch: 5 Validation Loss 0.033 Time taken: 47.49 secs| Accuracy 96.101
T\P		 0		 1
0		3493		0097		
1		0202		3286		

Accuracy 95.7756
Time taken: 48.28 secs
2742.07 secs


On the best model
T\P		 0		 1
0		3493		0097		
1		0202		3286		

Accuracy 95.7756
Time taken: 48.19 secs

======================================================================
----------------------------------------------------------------------
				run_ID=43
Mon Jul 19 22:41:16 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.067 GB
13 seconds taken

even lower alpha| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:xlm-roberta-base
Memory taken on GPU 1.036 GB
Memory taken on RAM 2.576 GB
8 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.053 Time taken: 476.18 secs

	Epoch: 1 Validation Loss 0.015 Time taken: 42.82 secs| Accuracy 98.474

Epoch: 2 Train Loss 0.022 Time taken: 486.32 secs

	Epoch: 2 Validation Loss 0.008 Time taken: 42.83 secs| Accuracy 99.011

Epoch: 3 Train Loss 0.016 Time taken: 486.54 secs

	Epoch: 3 Validation Loss 0.012 Time taken: 42.79 secs| Accuracy 98.926

Epoch: 4 Train Loss 0.012 Time taken: 487.14 secs

	Epoch: 4 Validation Loss 0.01 Time taken: 42.88 secs| Accuracy 99.124

Epoch: 5 Train Loss 0.01 Time taken: 491.13 secs

	Epoch: 5 Validation Loss 0.009 Time taken: 43.26 secs| Accuracy 99.011
T\P		 0		 1
0		3529		0061		
1		0027		3461		

Accuracy 98.7567
Time taken: 43.26 secs
2703.142 secs


On the best model
T\P		 0		 1
0		3557		0033		
1		0057		3431		

Accuracy 98.7285
Time taken: 41.99 secs



======================================================================
----------------------------------------------------------------------
				run_ID=44
Mon Jul 19 22:40:33 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.142 GB
11 seconds taken

even lower alpha | SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:ai4bharat/indic-bert
Memory taken on GPU 0.125 GB
Memory taken on RAM 1.308 GB
8 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.059 Time taken: 441.92 secs

	Epoch: 1 Validation Loss 0.026 Time taken: 47.34 secs| Accuracy 96.75

Epoch: 2 Train Loss 0.025 Time taken: 440.39 secs

	Epoch: 2 Validation Loss 0.026 Time taken: 47.19 secs| Accuracy 97.118

Epoch: 3 Train Loss 0.019 Time taken: 439.51 secs

	Epoch: 3 Validation Loss 0.019 Time taken: 47.64 secs| Accuracy 97.711

Epoch: 4 Train Loss 0.015 Time taken: 441.26 secs

	Epoch: 4 Validation Loss 0.023 Time taken: 47.35 secs| Accuracy 97.203

Epoch: 5 Train Loss 0.011 Time taken: 441.79 secs

	Epoch: 5 Validation Loss 0.02 Time taken: 47.55 secs| Accuracy 97.57
T\P		 0		 1
0		3523		0067		
1		0118		3370		

Accuracy 97.3863
Time taken: 47.91 secs
2491.587 secs


On the best model
T\P		 0		 1
0		3541		0049		
1		0143		3345		

Accuracy 97.2874
Time taken: 47.67 secs


======================================================================
----------------------------------------------------------------------
				run_ID=45
Tue Jul 20 01:04:36 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 14613 | Validation_set: 2087 | Test_set: 4176
Train_batches: 457 | Validation_batches: 457 | Test_batches: 131
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.077 GB
27 seconds taken

19 repeat with even lower alpha| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.996 GB
33 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.047 Time taken: 259.67 secs

	Epoch: 1 Validation Loss 0.026 Time taken: 25.31 secs| Accuracy 96.79

Epoch: 2 Train Loss 0.026 Time taken: 262.74 secs

	Epoch: 2 Validation Loss 0.024 Time taken: 25.29 secs| Accuracy 96.79

Epoch: 3 Train Loss 0.021 Time taken: 262.48 secs

	Epoch: 3 Validation Loss 0.022 Time taken: 25.28 secs| Accuracy 97.269

Epoch: 4 Train Loss 0.019 Time taken: 262.8 secs

	Epoch: 4 Validation Loss 0.022 Time taken: 25.28 secs| Accuracy 97.221

Epoch: 5 Train Loss 0.017 Time taken: 262.51 secs

	Epoch: 5 Validation Loss 0.021 Time taken: 25.29 secs| Accuracy 97.508
T\P		 0		 1
0		2056		0053		
1		0071		1996		

Accuracy 97.0307
Time taken: 25.45 secs
1473.419 secs


On the best model
T\P		 0		 1
0		2056		0053		
1		0071		1996		

Accuracy 97.0307
Time taken: 25.64 secs


======================================================================
----------------------------------------------------------------------
				run_ID=46
Tue Jul 20 13:14:46 2021
GPU is available Tesla K80 11 GB

Batch_size: 32
Train set: 24773 | Validation_set: 3539 | Test_set: 7078
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.138 GB
41 seconds taken

41 repeat with per epoch saves| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.497 GB
35 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.028 Time taken: 791.17 secs

	Epoch: 1 Validation Loss 0.015 Time taken: 79.63 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.014 Time taken: 791.11 secs

	Epoch: 2 Validation Loss 0.012 Time taken: 79.73 secs| Accuracy 98.785

Epoch: 3 Train Loss 0.008 Time taken: 792.27 secs

	Epoch: 3 Validation Loss 0.01 Time taken: 79.61 secs| Accuracy 98.955

Epoch: 4 Train Loss 0.007 Time taken: 792.54 secs

	Epoch: 4 Validation Loss 0.011 Time taken: 79.73 secs| Accuracy 98.615

Epoch: 5 Train Loss 0.005 Time taken: 791.65 secs

	Epoch: 5 Validation Loss 0.014 Time taken: 79.76 secs| Accuracy 98.785
T\P		 0		 1
0		3519		0071		
1		0047		3441		

Accuracy 98.3329
Time taken: 80.73 secs
4455.532 secs



======================================================================
----------------------------------------------------------------------
				run_ID=47
Tue Jul 20 15:59:00 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 24772 | Validation_set: 3538 | Test_set: 7080
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.052 GB
32 seconds taken

41 repeat with separate sent pairs and per epoch saves| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.154 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.033 Time taken: 459.3 secs

	Epoch: 1 Validation Loss 0.012 Time taken: 45.28 secs| Accuracy 98.445

Epoch: 2 Train Loss 0.015 Time taken: 470.2 secs

	Epoch: 2 Validation Loss 0.013 Time taken: 45.44 secs| Accuracy 98.53

Epoch: 3 Train Loss 0.009 Time taken: 469.9 secs

	Epoch: 3 Validation Loss 0.012 Time taken: 45.33 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.007 Time taken: 469.52 secs

	Epoch: 4 Validation Loss 0.011 Time taken: 45.14 secs| Accuracy 98.954

Epoch: 5 Train Loss 0.006 Time taken: 472.95 secs

	Epoch: 5 Validation Loss 0.012 Time taken: 45.44 secs| Accuracy 98.841
T\P		 0		 1
0		3500		0040		
1		0053		3487		

Accuracy 98.6864
Time taken: 45.38 secs
2645.669 secs


On the best model
T\P		 0		 1
0		3510		0030		
1		0060		3480		

Accuracy 98.7288
Time taken: 47.17 secs


======================================================================
----------------------------------------------------------------------
				run_ID=48
Tue Jul 20 15:59:00 2021


Batch_size: 32
Train set: 24772 | Validation_set: 3538 | Test_set: 7080
Train_batches: 775 | Validation_batches: 775 | Test_batches: 222
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.073 GB
32 seconds taken

41 repeat with separate sent pairs and per epoch saves| SequenceClassification| train_ratio:0.7 val_ratio:0.1 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU -2.001 GB
Memory taken on RAM 0.596 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.088 Time taken: 470.53 secs

	Epoch: 1 Validation Loss 0.048 Time taken: 45.61 secs| Accuracy 93.697

Epoch: 2 Train Loss 0.036 Time taken: 470.67 secs

	Epoch: 2 Validation Loss 0.048 Time taken: 45.23 secs| Accuracy 93.443

Epoch: 3 Train Loss 0.024 Time taken: 469.49 secs

	Epoch: 3 Validation Loss 0.035 Time taken: 45.36 secs| Accuracy 95.789

Epoch: 4 Train Loss 0.016 Time taken: 470.15 secs

	Epoch: 4 Validation Loss 0.035 Time taken: 45.34 secs| Accuracy 95.506


======================================================================
----------------------------------------------------------------------
				run_ID=49
Wed Jul 21 16:48:47 2021
GPU is available Tesla P4 7 GB

Batch_size: 32
Train set: 17694 | Validation_set: 7078 | Test_set: 10618
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.054 GB
31 seconds taken

26 repeat with lower alpha| SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.801 GB
5 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.037 Time taken: 356.99 secs

	Epoch: 1 Validation Loss 0.015 Time taken: 44.74 secs| Accuracy 98.135

Epoch: 2 Train Loss 0.016 Time taken: 359.48 secs

	Epoch: 2 Validation Loss 0.018 Time taken: 44.61 secs| Accuracy 97.895

Epoch: 3 Train Loss 0.01 Time taken: 360.39 secs

	Epoch: 3 Validation Loss 0.018 Time taken: 44.59 secs| Accuracy 98.135

Epoch: 4 Train Loss 0.008 Time taken: 360.46 secs

	Epoch: 4 Validation Loss 0.021 Time taken: 44.69 secs| Accuracy 97.683

Epoch: 5 Train Loss 0.005 Time taken: 360.79 secs

	Epoch: 5 Validation Loss 0.014 Time taken: 44.74 secs| Accuracy 98.587
T\P		 0		 1
0		5241		0068		
1		0056		5253		

Accuracy 98.8322
Time taken: 67.14 secs
2111.283 secs


On the best model
T\P		 0		 1
0		5241		0068		
1		0056		5253		

Accuracy 98.8322
Time taken: 65.3 secs


======================================================================
----------------------------------------------------------------------
				run_ID=50
Resuming 49 ...
Wed Jul 21 16:48:47 2021
GPU is available Tesla P4 7 GB

Batch_size: 32
Train set: 17694 | Validation_set: 7078 | Test_set: 10618
Train_batches: 553 | Validation_batches: 553 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.054 GB
31 seconds taken

26 repeat with lower alpha| SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.801 GB
5 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.005 Time taken: 358.79 secs

	Epoch: 1 Validation Loss 0.015 Time taken: 44.68 secs| Accuracy 98.46

Epoch: 2 Train Loss 0.004 Time taken: 359.69 secs

	Epoch: 2 Validation Loss 0.015 Time taken: 44.45 secs| Accuracy 98.545

Epoch: 3 Train Loss 0.004 Time taken: 359.66 secs

	Epoch: 3 Validation Loss 0.015 Time taken: 44.45 secs| Accuracy 98.234

Epoch: 4 Train Loss 0.004 Time taken: 359.67 secs

	Epoch: 4 Validation Loss 0.015 Time taken: 44.38 secs| Accuracy 98.234

Epoch: 5 Train Loss 0.003 Time taken: 360.27 secs

	Epoch: 5 Validation Loss 0.012 Time taken: 44.66 secs| Accuracy 98.728
T\P		 0		 1
0		5252		0057		
1		0061		5248		

Accuracy 98.8887
Time taken: 67.37 secs
2113.675 secs


On the best model
T\P		 0		 1
0		5252		0057		
1		0061		5248		

Accuracy 98.8887
Time taken: 66.96 secs


======================================================================
----------------------------------------------------------------------
				run_ID=52
Resuming 49 ...
Wed Jul 21 18:35:29 2021
GPU is available Tesla P4 7 GB

Batch_size: 32
Train set: 256 | Validation_set: 256 | Test_set: 256
Train_batches: 8 | Validation_batches: 8 | Test_batches: 8
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.011 GB
6 seconds taken

SequenceClassification| 128 sentence pairs in train,val,test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.806 GB
5 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 5.24 secs

	Epoch: 1 Validation Loss 0.126 Time taken: 1.68 secs| Accuracy 94.531

Epoch: 2 Train Loss 0.095 Time taken: 5.25 secs

	Epoch: 2 Validation Loss 0.037 Time taken: 1.75 secs| Accuracy 96.484

Epoch: 3 Train Loss 0.046 Time taken: 5.22 secs

	Epoch: 3 Validation Loss 0.03 Time taken: 1.75 secs| Accuracy 96.094

Epoch: 4 Train Loss 0.031 Time taken: 5.25 secs

	Epoch: 4 Validation Loss 0.028 Time taken: 1.67 secs| Accuracy 96.094

Epoch: 5 Train Loss 0.027 Time taken: 5.28 secs

	Epoch: 5 Validation Loss 0.023 Time taken: 1.74 secs| Accuracy 97.266
T\P		 0		 1
0		0128		0000		
1		0004		0124		

Accuracy 98.4375
Time taken: 4.1 secs
125.805 secs


On the best model
T\P		 0		 1
0		0128		0000		
1		0004		0124		

Accuracy 98.4375
Time taken: 1.73 secs


======================================================================
----------------------------------------------------------------------
				run_ID=53
Wed Jul 21 19:34:43 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 24 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.011 GB
11 seconds taken

asha_sentsim | SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.182 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.234 Time taken: 12.99 secs

	Epoch: 1 Validation Loss 0.135 Time taken: 1.82 secs| Accuracy 85.43

Epoch: 2 Train Loss 0.108 Time taken: 13.13 secs

	Epoch: 2 Validation Loss 0.091 Time taken: 1.92 secs| Accuracy 89.735

Epoch: 3 Train Loss 0.07 Time taken: 13.57 secs

	Epoch: 3 Validation Loss 0.084 Time taken: 1.96 secs| Accuracy 90.066

Epoch: 4 Train Loss 0.051 Time taken: 14.03 secs

	Epoch: 4 Validation Loss 0.089 Time taken: 2.03 secs| Accuracy 90.066

Epoch: 5 Train Loss 0.029 Time taken: 14.6 secs

	Epoch: 5 Validation Loss 0.091 Time taken: 2.18 secs| Accuracy 91.391
T\P		 0		 1
0		0172		0026		
1		0023		0233		

Accuracy 89.207
Time taken: 4.8 secs
151.985 secs


On the best model
T\P		 0		 1
0		0172		0026		
1		0023		0233		

Accuracy 89.207
Time taken: 3.36 secs


======================================================================
----------------------------------------------------------------------
				run_ID=54
Wed Jul 21 20:10:07 2021
GPU is available Tesla P4 7 GB

Batch_size: 32
Train set: 128 | Validation_set: 128 | Test_set: 128
Train_batches: 4 | Validation_batches: 4 | Test_batches: 4
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.019 GB
6 seconds taken

 SequenceClassification| 64 sentence pairs in train,val,test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.006 GB
2 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.509 Time taken: 2.82 secs

	Epoch: 1 Validation Loss 0.383 Time taken: 0.95 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.293 Time taken: 2.81 secs

	Epoch: 2 Validation Loss 0.22 Time taken: 0.95 secs| Accuracy 71.875

Epoch: 3 Train Loss 0.232 Time taken: 2.71 secs

	Epoch: 3 Validation Loss 0.187 Time taken: 0.92 secs| Accuracy 71.094

Epoch: 4 Train Loss 0.162 Time taken: 5.6 secs

	Epoch: 4 Validation Loss 0.143 Time taken: 0.98 secs| Accuracy 80.469

Epoch: 5 Train Loss 0.129 Time taken: 5.79 secs

	Epoch: 5 Validation Loss 0.091 Time taken: 0.96 secs| Accuracy 90.625
T\P		 0		 1
0		0060		0004		
1		0009		0055		

Accuracy 89.8438
Time taken: 0.97 secs
139.375 secs


On the best model
T\P		 0		 1
0		0060		0004		
1		0009		0055		

Accuracy 89.8438
Time taken: 1.29 secs


======================================================================
----------------------------------------------------------------------
				run_ID=55
Wed Jul 21 20:38:33 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 64 | Validation_set: 64 | Test_set: 68
Train_batches: 2 | Validation_batches: 2 | Test_batches: 3
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.028 GB
8 seconds taken

 SequenceClassification| 32 sentence pairs in train,val,test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.215 GB
31 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.268 Time taken: 1.42 secs

	Epoch: 1 Validation Loss 0.264 Time taken: 0.5 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.19 Time taken: 1.27 secs

	Epoch: 2 Validation Loss 0.218 Time taken: 0.49 secs| Accuracy 65.625

Epoch: 3 Train Loss 0.154 Time taken: 1.32 secs

	Epoch: 3 Validation Loss 0.188 Time taken: 0.51 secs| Accuracy 82.812

Epoch: 4 Train Loss 0.12 Time taken: 1.34 secs

	Epoch: 4 Validation Loss 0.159 Time taken: 0.52 secs| Accuracy 85.938

Epoch: 5 Train Loss 0.084 Time taken: 1.39 secs

	Epoch: 5 Validation Loss 0.14 Time taken: 0.54 secs| Accuracy 82.812
T\P		 0		 1
0		0033		0001		
1		0001		0033		

Accuracy 97.0588
Time taken: 2.34 secs
139.713 secs


On the best model
T\P		 0		 1
0		0032		0002		
1		0001		0033		

Accuracy 95.5882
Time taken: 0.64 secs


======================================================================
----------------------------------------------------------------------
				run_ID=56
Wed Jul 21 20:58:00 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 400 | Validation_set: 400 | Test_set: 400
Train_batches: 13 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.031 GB
10 seconds taken

 SequenceClassification| 200 sentence pairs in train,val,test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.215 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 7.2 secs

	Epoch: 1 Validation Loss 0.054 Time taken: 2.38 secs| Accuracy 96.0

Epoch: 2 Train Loss 0.053 Time taken: 7.17 secs

	Epoch: 2 Validation Loss 0.034 Time taken: 2.49 secs| Accuracy 96.25

Epoch: 3 Train Loss 0.029 Time taken: 7.31 secs

	Epoch: 3 Validation Loss 0.028 Time taken: 2.49 secs| Accuracy 96.25

Epoch: 4 Train Loss 0.018 Time taken: 7.3 secs

	Epoch: 4 Validation Loss 0.027 Time taken: 2.54 secs| Accuracy 96.75

Epoch: 5 Train Loss 0.018 Time taken: 12.09 secs

	Epoch: 5 Validation Loss 0.026 Time taken: 2.63 secs| Accuracy 97.0
T\P		 0		 1
0		0191		0009		
1		0012		0188		

Accuracy 94.75
Time taken: 2.64 secs
144.043 secs


On the best model
T\P		 0		 1
0		0191		0009		
1		0012		0188		

Accuracy 94.75
Time taken: 2.62 secs


======================================================================
----------------------------------------------------------------------
				run_ID=57
Wed Jul 21 21:29:45 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 500 | Validation_set: 500 | Test_set: 500
Train_batches: 16 | Validation_batches: 16 | Test_batches: 16
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.021 GB
5 seconds taken

SequenceClassification| 250 sentence pairs for train,valid, and test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.205 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.147 Time taken: 8.8 secs

	Epoch: 1 Validation Loss 0.029 Time taken: 2.92 secs| Accuracy 98.0

Epoch: 2 Train Loss 0.036 Time taken: 8.8 secs

	Epoch: 2 Validation Loss 0.018 Time taken: 2.98 secs| Accuracy 98.2

Epoch: 3 Train Loss 0.017 Time taken: 8.89 secs

	Epoch: 3 Validation Loss 0.02 Time taken: 3.05 secs| Accuracy 98.0

Epoch: 4 Train Loss 0.018 Time taken: 8.91 secs

	Epoch: 4 Validation Loss 0.019 Time taken: 3.09 secs| Accuracy 98.0

Epoch: 5 Train Loss 0.013 Time taken: 9.09 secs

	Epoch: 5 Validation Loss 0.02 Time taken: 3.16 secs| Accuracy 97.8
T\P		 0		 1
0		0244		0006		
1		0016		0234		

Accuracy 95.6
Time taken: 4.63 secs
112.674 secs


On the best model
T\P		 0		 1
0		0248		0002		
1		0020		0230		

Accuracy 95.6
Time taken: 3.3 secs


======================================================================
----------------------------------------------------------------------
				run_ID=58
Wed Jul 21 21:46:01 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 800 | Test_set: 800
Train_batches: 25 | Validation_batches: 25 | Test_batches: 25
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.016 GB
6 seconds taken

SequenceClassification| 400 sentence pairs for train,valid, and test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.177 GB
32 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.115 Time taken: 13.61 secs

	Epoch: 1 Validation Loss 0.041 Time taken: 4.56 secs| Accuracy 95.0

Epoch: 2 Train Loss 0.034 Time taken: 13.77 secs

	Epoch: 2 Validation Loss 0.037 Time taken: 4.71 secs| Accuracy 96.125

Epoch: 3 Train Loss 0.024 Time taken: 14.3 secs

	Epoch: 3 Validation Loss 0.037 Time taken: 4.78 secs| Accuracy 95.5

Epoch: 4 Train Loss 0.017 Time taken: 14.41 secs

	Epoch: 4 Validation Loss 0.035 Time taken: 4.95 secs| Accuracy 95.375

Epoch: 5 Train Loss 0.013 Time taken: 14.93 secs

	Epoch: 5 Validation Loss 0.038 Time taken: 4.94 secs| Accuracy 95.125
T\P		 0		 1
0		0375		0025		
1		0029		0371		

Accuracy 93.25
Time taken: 5.05 secs
134.634 secs


On the best model
T\P		 0		 1
0		0380		0020		
1		0027		0373		

Accuracy 94.125
Time taken: 5.03 secs


======================================================================
----------------------------------------------------------------------
				run_ID=59
Wed Jul 21 22:20:43 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 2000 | Validation_set: 1000 | Test_set: 1000
Train_batches: 63 | Validation_batches: 32 | Test_batches: 32
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.033 GB
12 seconds taken

SequenceClassification| 1k sentence pairs for train, 500 valid, and test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.211 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.079 Time taken: 34.37 secs

	Epoch: 1 Validation Loss 0.057 Time taken: 5.99 secs| Accuracy 92.9

Epoch: 2 Train Loss 0.031 Time taken: 36.88 secs

	Epoch: 2 Validation Loss 0.06 Time taken: 6.65 secs| Accuracy 93.0

Epoch: 3 Train Loss 0.02 Time taken: 39.05 secs

	Epoch: 3 Validation Loss 0.061 Time taken: 6.49 secs| Accuracy 93.2

Epoch: 4 Train Loss 0.013 Time taken: 38.29 secs

	Epoch: 4 Validation Loss 0.052 Time taken: 6.69 secs| Accuracy 93.3

Epoch: 5 Train Loss 0.011 Time taken: 38.38 secs

	Epoch: 5 Validation Loss 0.051 Time taken: 6.65 secs| Accuracy 93.8
T\P		 0		 1
0		0438		0062		
1		0019		0481		

Accuracy 91.9
Time taken: 9.69 secs
262.047 secs


On the best model
T\P		 0		 1
0		0438		0062		
1		0019		0481		

Accuracy 91.9
Time taken: 6.53 secs


======================================================================
----------------------------------------------------------------------
				run_ID=60
Wed Jul 21 22:41:35 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.029 GB
10 seconds taken

repeating 58 | SequenceClassification| 400 sentence pairs for train, 200 for valid, and test | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.211 GB
31 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.115 Time taken: 13.47 secs

	Epoch: 1 Validation Loss 0.042 Time taken: 2.32 secs| Accuracy 95.0

Epoch: 2 Train Loss 0.034 Time taken: 13.67 secs

	Epoch: 2 Validation Loss 0.04 Time taken: 2.43 secs| Accuracy 95.5

Epoch: 3 Train Loss 0.024 Time taken: 13.84 secs

	Epoch: 3 Validation Loss 0.039 Time taken: 2.43 secs| Accuracy 94.75

Epoch: 4 Train Loss 0.017 Time taken: 13.96 secs

	Epoch: 4 Validation Loss 0.038 Time taken: 2.46 secs| Accuracy 94.5

Epoch: 5 Train Loss 0.013 Time taken: 14.24 secs

	Epoch: 5 Validation Loss 0.039 Time taken: 2.48 secs| Accuracy 95.25
T\P		 0		 1
0		0196		0004		
1		0009		0191		

Accuracy 96.75
Time taken: 2.52 secs
121.09 secs


On the best model
T\P		 0		 1
0		0196		0004		
1		0008		0192		

Accuracy 97.0
Time taken: 2.54 secs


======================================================================
----------------------------------------------------------------------
				run_ID=61
Wed Jul 21 22:48:26 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.002 GB
6 seconds taken

REPEAT | asha_sentsim | SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: True| model:bert-base-multilingual-cased
Memory taken on GPU -2.004 GB
Memory taken on RAM 0.663 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.234 Time taken: 12.92 secs

	Epoch: 1 Validation Loss 0.135 Time taken: 1.85 secs| Accuracy 85.43

Epoch: 2 Train Loss 0.108 Time taken: 13.21 secs

	Epoch: 2 Validation Loss 0.091 Time taken: 1.94 secs| Accuracy 89.735

Epoch: 3 Train Loss 0.07 Time taken: 13.38 secs

	Epoch: 3 Validation Loss 0.084 Time taken: 1.94 secs| Accuracy 90.066

Epoch: 4 Train Loss 0.051 Time taken: 13.33 secs

	Epoch: 4 Validation Loss 0.089 Time taken: 1.9 secs| Accuracy 90.066

Epoch: 5 Train Loss 0.029 Time taken: 13.54 secs

	Epoch: 5 Validation Loss 0.091 Time taken: 1.94 secs| Accuracy 91.391
T\P		 0		 1
0		0172		0026		
1		0023		0233		

Accuracy 89.207
Time taken: 2.91 secs
150.246 secs


On the best model
T\P		 0		 1
0		0172		0026		
1		0023		0233		

Accuracy 89.207
Time taken: 4.23 secs


======================================================================
----------------------------------------------------------------------
				run_ID=62
Wed Jul 21 23:00:50 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.028 GB
12 seconds taken

58 repeat but | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.215 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.24 Time taken: 14.21 secs

	Epoch: 1 Validation Loss 0.204 Time taken: 2.48 secs| Accuracy 69.0

Epoch: 2 Train Loss 0.161 Time taken: 14.67 secs

	Epoch: 2 Validation Loss 0.163 Time taken: 2.67 secs| Accuracy 77.75

Epoch: 3 Train Loss 0.098 Time taken: 15.29 secs

	Epoch: 3 Validation Loss 0.164 Time taken: 2.75 secs| Accuracy 78.0

Epoch: 4 Train Loss 0.058 Time taken: 16.06 secs

	Epoch: 4 Validation Loss 0.187 Time taken: 2.91 secs| Accuracy 75.0

Epoch: 5 Train Loss 0.034 Time taken: 16.91 secs

	Epoch: 5 Validation Loss 0.165 Time taken: 2.91 secs| Accuracy 78.25
T\P		 0		 1
0		0159		0041		
1		0041		0159		

Accuracy 79.5
Time taken: 5.07 secs
154.076 secs


On the best model
T\P		 0		 1
0		0159		0041		
1		0041		0159		

Accuracy 79.5
Time taken: 4.48 secs


======================================================================
----------------------------------------------------------------------
				run_ID=63
Wed Jul 21 23:00:50 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.014 GB
14 seconds taken

SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: True| model:ai4bharat/indic-bert
Memory taken on GPU -2.542 GB
Memory taken on RAM 0.003 GB
6 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.408 Time taken: 15.39 secs

	Epoch: 1 Validation Loss 0.319 Time taken: 3.09 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.26 Time taken: 15.3 secs

	Epoch: 2 Validation Loss 0.205 Time taken: 2.86 secs| Accuracy 86.5

Epoch: 3 Train Loss 0.156 Time taken: 14.41 secs

	Epoch: 3 Validation Loss 0.076 Time taken: 2.77 secs| Accuracy 90.25

Epoch: 4 Train Loss 0.063 Time taken: 14.28 secs

	Epoch: 4 Validation Loss 0.068 Time taken: 2.79 secs| Accuracy 91.75

Epoch: 5 Train Loss 0.037 Time taken: 14.65 secs

	Epoch: 5 Validation Loss 0.087 Time taken: 2.88 secs| Accuracy 89.0
T\P		 0		 1
0		0168		0032		
1		0011		0189		

Accuracy 89.25
Time taken: 2.87 secs
96.664 secs


On the best model
T\P		 0		 1
0		0181		0019		
1		0018		0182		

Accuracy 90.75
Time taken: 2.87 secs


======================================================================
----------------------------------------------------------------------
				run_ID=64
Wed Jul 21 23:24:27 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.012 GB
9 seconds taken

SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: True| model:xlm-roberta-base
Memory taken on GPU 1.036 GB
Memory taken on RAM 2.556 GB
7 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.444 Time taken: 16.55 secs

	Epoch: 1 Validation Loss 0.26 Time taken: 2.82 secs| Accuracy 55.0

Epoch: 2 Train Loss 0.195 Time taken: 17.46 secs

	Epoch: 2 Validation Loss 0.111 Time taken: 3.01 secs| Accuracy 84.5

Epoch: 3 Train Loss 0.116 Time taken: 16.94 secs

	Epoch: 3 Validation Loss 0.05 Time taken: 2.85 secs| Accuracy 94.0

Epoch: 4 Train Loss 0.063 Time taken: 17.26 secs

	Epoch: 4 Validation Loss 0.051 Time taken: 2.93 secs| Accuracy 94.0

Epoch: 5 Train Loss 0.052 Time taken: 17.42 secs

	Epoch: 5 Validation Loss 0.046 Time taken: 2.78 secs| Accuracy 95.25
T\P		 0		 1
0		0199		0001		
1		0012		0188		

Accuracy 96.75
Time taken: 2.77 secs
248.547 secs


On the best model
T\P		 0		 1
0		0199		0001		
1		0012		0188		

Accuracy 96.75
Time taken: 2.9 secs

======================================================================
----------------------------------------------------------------------
				run_ID=65
Wed Jul 21 23:40:57 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.006 GB
13 seconds taken

SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: True| model:xlm-mlm-xnli15-1024
Memory taken on GPU 0.928 GB
Memory taken on RAM 2.835 GB
36 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 22.61 Time taken: 24.24 secs

	Epoch: 1 Validation Loss 0.365 Time taken: 4.33 secs| Accuracy 50.0

Epoch: 2 Train Loss 3.306 Time taken: 25.94 secs

	Epoch: 2 Validation Loss 0.739 Time taken: 4.26 secs| Accuracy 50.0

Epoch: 3 Train Loss 2.637 Time taken: 24.65 secs

	Epoch: 3 Validation Loss 0.348 Time taken: 4.37 secs| Accuracy 50.0

Epoch: 4 Train Loss 2.176 Time taken: 25.54 secs

	Epoch: 4 Validation Loss 0.397 Time taken: 4.29 secs| Accuracy 50.0

Epoch: 5 Train Loss 1.779 Time taken: 24.92 secs

	Epoch: 5 Validation Loss 0.327 Time taken: 4.34 secs| Accuracy 50.0
T\P		 0		 1
0		0200		0000		
1		0200		0000		

Accuracy 50.0
Time taken: 4.06 secs
283.901 secs


On the best model
T\P		 0		 1
0		0000		0200		
1		0000		0200		

Accuracy 50.0
Time taken: 4.43 secs


======================================================================
----------------------------------------------------------------------
				run_ID=66
Wed Jul 21 23:49:37 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.019 GB
11 seconds taken

SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: True| model:xlm-mlm-xnli15-1024
Memory taken on GPU 0.928 GB
Memory taken on RAM 2.803 GB
45 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 22.61 Time taken: 24.09 secs

	Epoch: 1 Validation Loss 0.365 Time taken: 4.28 secs| Accuracy 50.0

Epoch: 2 Train Loss 3.306 Time taken: 25.86 secs

	Epoch: 2 Validation Loss 0.739 Time taken: 4.26 secs| Accuracy 50.0

Epoch: 3 Train Loss 2.637 Time taken: 24.44 secs

	Epoch: 3 Validation Loss 0.348 Time taken: 4.01 secs| Accuracy 50.0

Epoch: 4 Train Loss 2.176 Time taken: 24.64 secs

	Epoch: 4 Validation Loss 0.397 Time taken: 4.2 secs| Accuracy 50.0

Epoch: 5 Train Loss 1.779 Time taken: 25.06 secs

	Epoch: 5 Validation Loss 0.327 Time taken: 4.09 secs| Accuracy 50.0
T\P		 0		 1
0		0200		0000		
1		0200		0000		

Accuracy 50.0
Time taken: 4.25 secs
187.712 secs


On the best model
T\P		 0		 1
0		0000		0200		
1		0000		0200		

Accuracy 50.0
Time taken: 4.12 secs


======================================================================
----------------------------------------------------------------------
				run_ID=67
Thu Jul 22 00:01:08 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.015 GB
11 seconds taken

SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-uncased
Memory taken on GPU -3.079 GB
Memory taken on RAM 0.599 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.267 Time taken: 16.89 secs

	Epoch: 1 Validation Loss 0.207 Time taken: 3.13 secs| Accuracy 77.0

Epoch: 2 Train Loss 0.175 Time taken: 16.1 secs

	Epoch: 2 Validation Loss 0.172 Time taken: 2.94 secs| Accuracy 77.0

Epoch: 3 Train Loss 0.114 Time taken: 15.81 secs

	Epoch: 3 Validation Loss 0.162 Time taken: 2.92 secs| Accuracy 77.5

Epoch: 4 Train Loss 0.068 Time taken: 15.99 secs

	Epoch: 4 Validation Loss 0.15 Time taken: 3.02 secs| Accuracy 80.25

Epoch: 5 Train Loss 0.043 Time taken: 16.17 secs

	Epoch: 5 Validation Loss 0.152 Time taken: 3.06 secs| Accuracy 80.0
T\P		 0		 1
0		0165		0035		
1		0041		0159		

Accuracy 81.0
Time taken: 3.07 secs
133.107 secs


On the best model
T\P		 0		 1
0		0162		0038		
1		0044		0156		

Accuracy 79.5
Time taken: 2.93 secs


======================================================================
----------------------------------------------------------------------
				run_ID=68
Fri Jul 23 13:50:00 2021


Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.007 GB
9 seconds taken

NEW CODE: 62 repeat | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-uncased
Memory taken on GPU 0.627 GB
Memory taken on RAM 0.0 GB
3 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.261 Time taken: 13.26 secs

	Epoch: 1 Validation Loss 0.197 Time taken: 2.34 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.162 Time taken: 13.46 secs

	Epoch: 2 Validation Loss 0.161 Time taken: 2.48 secs| Accuracy 50.0

Epoch: 3 Train Loss 0.109 Time taken: 13.72 secs

	Epoch: 3 Validation Loss 0.172 Time taken: 2.44 secs| Accuracy 50.0

Epoch: 4 Train Loss 0.075 Time taken: 13.81 secs

	Epoch: 4 Validation Loss 0.175 Time taken: 2.47 secs| Accuracy 50.5

Epoch: 5 Train Loss 0.051 Time taken: 13.97 secs

	Epoch: 5 Validation Loss 0.172 Time taken: 2.58 secs| Accuracy 55.75

On the best model

======================================================================
----------------------------------------------------------------------
				run_ID=69
Fri Jul 23 15:15:16 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.014 GB
10 seconds taken

NEW CODE with activation = identity; fc_input = pooler_output : 62 repeat | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.129 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.293 Time taken: 13.48 secs

	Epoch: 1 Validation Loss 0.197 Time taken: 2.4 secs| Accuracy 71.25

Epoch: 2 Train Loss 0.154 Time taken: 13.66 secs

	Epoch: 2 Validation Loss 0.161 Time taken: 2.47 secs| Accuracy 75.25

Epoch: 3 Train Loss 0.096 Time taken: 13.75 secs

	Epoch: 3 Validation Loss 0.152 Time taken: 2.46 secs| Accuracy 79.5

Epoch: 4 Train Loss 0.056 Time taken: 13.81 secs

	Epoch: 4 Validation Loss 0.143 Time taken: 2.42 secs| Accuracy 79.75

Epoch: 5 Train Loss 0.04 Time taken: 13.96 secs

	Epoch: 5 Validation Loss 0.166 Time taken: 2.49 secs| Accuracy 80.75
T\P		 0		 1
0		0145		0055		
1		0022		0178		

Accuracy 80.75
Time taken: 4.46 secs
259.452 secs


On the best model
T\P		 0		 1
0		0145		0055		
1		0022		0178		

Accuracy 80.75
Time taken: 2.51 secs


======================================================================
----------------------------------------------------------------------
				run_ID=70
Fri Jul 23 15:24:19 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.004 GB
8 seconds taken

NEW CODE with activation = identity; fc_input = average of first 2 layers of BERT : 62 repeat | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.117 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.356 Time taken: 5.89 secs

	Epoch: 1 Validation Loss 0.293 Time taken: 2.37 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.274 Time taken: 5.97 secs

	Epoch: 2 Validation Loss 0.253 Time taken: 2.49 secs| Accuracy 50.0

Epoch: 3 Train Loss 0.265 Time taken: 5.9 secs

	Epoch: 3 Validation Loss 0.251 Time taken: 2.4 secs| Accuracy 49.75

Epoch: 4 Train Loss 0.262 Time taken: 5.97 secs

	Epoch: 4 Validation Loss 0.25 Time taken: 2.42 secs| Accuracy 51.0

Epoch: 5 Train Loss 0.24 Time taken: 6.02 secs

	Epoch: 5 Validation Loss 0.246 Time taken: 2.42 secs| Accuracy 53.75
T\P		 0		 1
0		0140		0060		
1		0129		0071		

Accuracy 52.75
Time taken: 2.51 secs
128.564 secs


On the best model
T\P		 0		 1
0		0140		0060		
1		0129		0071		

Accuracy 52.75
Time taken: 2.49 secs


======================================================================
----------------------------------------------------------------------
				run_ID=71
Fri Jul 23 15:29:33 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.015 GB
9 seconds taken

NEW CODE with activation = identity; fc_input = average of last 2 layers of BERT (excluding the pooling_output): 62 repeat | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.136 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.489 Time taken: 12.72 secs

	Epoch: 1 Validation Loss 0.254 Time taken: 2.42 secs| Accuracy 59.25

Epoch: 2 Train Loss 0.179 Time taken: 13.12 secs

	Epoch: 2 Validation Loss 0.193 Time taken: 2.53 secs| Accuracy 73.0

Epoch: 3 Train Loss 0.126 Time taken: 13.05 secs

	Epoch: 3 Validation Loss 0.178 Time taken: 2.5 secs| Accuracy 76.25

Epoch: 4 Train Loss 0.097 Time taken: 13.27 secs

	Epoch: 4 Validation Loss 0.17 Time taken: 2.52 secs| Accuracy 79.5

Epoch: 5 Train Loss 0.08 Time taken: 13.36 secs

	Epoch: 5 Validation Loss 0.164 Time taken: 2.53 secs| Accuracy 79.5
T\P		 0		 1
0		0163		0037		
1		0043		0157		

Accuracy 80.0
Time taken: 3.07 secs
152.522 secs


On the best model
T\P		 0		 1
0		0153		0047		
1		0041		0159		

Accuracy 78.0
Time taken: 2.58 secs


======================================================================
----------------------------------------------------------------------
				run_ID=72
Fri Jul 23 17:17:38 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.008 GB
3 seconds taken

asha_sentsim | NEW CODE with activation = identity; fc_input = average of last 2 layers of BERT (excluding the pooling_output): 62 repeat | SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.132 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.46 Time taken: 12.08 secs

	Epoch: 1 Validation Loss 0.202 Time taken: 1.79 secs| Accuracy 73.51

Epoch: 2 Train Loss 0.121 Time taken: 12.23 secs

	Epoch: 2 Validation Loss 0.112 Time taken: 1.87 secs| Accuracy 86.755

Epoch: 3 Train Loss 0.093 Time taken: 12.4 secs

	Epoch: 3 Validation Loss 0.104 Time taken: 1.86 secs| Accuracy 87.086

Epoch: 4 Train Loss 0.076 Time taken: 15.15 secs

	Epoch: 4 Validation Loss 0.104 Time taken: 1.89 secs| Accuracy 87.086

Epoch: 5 Train Loss 0.065 Time taken: 12.88 secs

	Epoch: 5 Validation Loss 0.099 Time taken: 1.93 secs| Accuracy 87.417
T\P		 0		 1
0		0158		0040		
1		0023		0233		

Accuracy 86.1233
Time taken: 5.48 secs
150.629 secs


On the best model
T\P		 0		 1
0		0158		0040		
1		0023		0233		

Accuracy 86.1233
Time taken: 2.96 secs


======================================================================
----------------------------------------------------------------------
				run_ID=73
Fri Jul 23 17:17:38 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.008 GB
3 seconds taken

asha_sentsim | NEW CODE with activation = identity; fc_input = average of first 2 layers of BERT: 62 repeat | SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU -1.917 GB
Memory taken on RAM 0.281 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.35 Time taken: 5.93 secs

	Epoch: 1 Validation Loss 0.279 Time taken: 1.93 secs| Accuracy 40.728

Epoch: 2 Train Loss 0.264 Time taken: 6.1 secs

	Epoch: 2 Validation Loss 0.238 Time taken: 2.08 secs| Accuracy 59.934

Epoch: 3 Train Loss 0.245 Time taken: 6.06 secs

	Epoch: 3 Validation Loss 0.236 Time taken: 1.92 secs| Accuracy 59.934

Epoch: 4 Train Loss 0.24 Time taken: 6.18 secs

	Epoch: 4 Validation Loss 0.228 Time taken: 2.01 secs| Accuracy 60.265

Epoch: 5 Train Loss 0.221 Time taken: 6.02 secs

	Epoch: 5 Validation Loss 0.206 Time taken: 1.94 secs| Accuracy 79.139
T\P		 0		 1
0		0086		0112		
1		0000		0256		

Accuracy 75.3304
Time taken: 2.96 secs
142.25 secs


On the best model
T\P		 0		 1
0		0086		0112		
1		0000		0256		

Accuracy 75.3304
Time taken: 2.99 secs


======================================================================
----------------------------------------------------------------------
				run_ID=74
Fri Jul 23 17:39:18 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.071 GB
8 seconds taken

full_inshorts | NEW CODE with activation = identity; fc_input = average of first 2 layers of BERT: 62 repeat | SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.011 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.245 Time taken: 134.05 secs

	Epoch: 1 Validation Loss 0.195 Time taken: 41.3 secs| Accuracy 70.062

Epoch: 2 Train Loss 0.169 Time taken: 135.38 secs

	Epoch: 2 Validation Loss 0.136 Time taken: 40.7 secs| Accuracy 81.69

Epoch: 3 Train Loss 0.13 Time taken: 135.51 secs

	Epoch: 3 Validation Loss 0.119 Time taken: 40.91 secs| Accuracy 83.964

Epoch: 4 Train Loss 0.111 Time taken: 135.38 secs

	Epoch: 4 Validation Loss 0.106 Time taken: 40.75 secs| Accuracy 85.716

Epoch: 5 Train Loss 0.101 Time taken: 135.46 secs

	Epoch: 5 Validation Loss 0.101 Time taken: 40.75 secs| Accuracy 86.253
T\P		 0		 1
0		4528		0781		
1		0719		4589		

Accuracy 85.8717
Time taken: 62.18 secs
975.771 secs


On the best model
T\P		 0		 1
0		4528		0781		
1		0719		4589		

Accuracy 85.8717
Time taken: 61.93 secs


======================================================================
----------------------------------------------------------------------
				run_ID=75
Fri Jul 23 18:00:01 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.074 GB
7 seconds taken

full_inshorts | NEW CODE with activation = identity; fc_input = average of last 2 layers of BERT (excluding pooling_output): 62 repeat | SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.022 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.135 Time taken: 292.94 secs

	Epoch: 1 Validation Loss 0.07 Time taken: 41.2 secs| Accuracy 92.978

Epoch: 2 Train Loss 0.058 Time taken: 293.87 secs

	Epoch: 2 Validation Loss 0.05 Time taken: 40.99 secs| Accuracy 94.999

Epoch: 3 Train Loss 0.035 Time taken: 293.65 secs

	Epoch: 3 Validation Loss 0.039 Time taken: 40.94 secs| Accuracy 95.479

Epoch: 4 Train Loss 0.023 Time taken: 293.55 secs

	Epoch: 4 Validation Loss 0.045 Time taken: 40.94 secs| Accuracy 94.688

Epoch: 5 Train Loss 0.015 Time taken: 293.32 secs

	Epoch: 5 Validation Loss 0.035 Time taken: 40.74 secs| Accuracy 95.592
T\P		 0		 1
0		5057		0252		
1		0225		5083		

Accuracy 95.5072
Time taken: 62.14 secs
1765.6 secs


On the best model
T\P		 0		 1
0		5057		0252		
1		0225		5083		

Accuracy 95.5072
Time taken: 61.8 secs




======================================================================
----------------------------------------------------------------------
				run_ID=76
Fri Jul 23 21:40:21 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.001 GB
9 seconds taken

62 grad plots | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.665 GB
Memory taken on RAM 0.001 GB
3 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.24 Time taken: 122.42 secs

	Epoch: 1 Validation Loss 0.204 Time taken: 2.54 secs| Accuracy 69.0

Epoch: 2 Train Loss 0.161 Time taken: 125.61 secs

	Epoch: 2 Validation Loss 0.163 Time taken: 2.54 secs| Accuracy 78.0

Epoch: 3 Train Loss 0.098 Time taken: 123.6 secs

	Epoch: 3 Validation Loss 0.163 Time taken: 2.55 secs| Accuracy 78.0

Epoch: 4 Train Loss 0.058 Time taken: 125.74 secs

	Epoch: 4 Validation Loss 0.186 Time taken: 2.59 secs| Accuracy 75.0

Epoch: 5 Train Loss 0.034 Time taken: 123.36 secs

	Epoch: 5 Validation Loss 0.165 Time taken: 2.61 secs| Accuracy 78.0
T\P		 0		 1
0		0159		0041		
1		0040		0160		

Accuracy 79.75
Time taken: 2.64 secs
723.332 secs



======================================================================
----------------------------------------------------------------------
				run_ID=77
Fri Jul 23 21:54:40 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.001 GB
8 seconds taken

61 grad plots | asha_sent_sim | SequenceClassification| train_ratio:0.5 val_ratio:0.2 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.001 GB
3 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.227 Time taken: 116.81 secs

	Epoch: 1 Validation Loss 0.118 Time taken: 1.93 secs| Accuracy 87.417

Epoch: 2 Train Loss 0.1 Time taken: 121.31 secs

	Epoch: 2 Validation Loss 0.092 Time taken: 1.98 secs| Accuracy 87.748

Epoch: 3 Train Loss 0.067 Time taken: 120.15 secs

	Epoch: 3 Validation Loss 0.087 Time taken: 1.96 secs| Accuracy 90.397

Epoch: 4 Train Loss 0.041 Time taken: 120.27 secs

	Epoch: 4 Validation Loss 0.099 Time taken: 1.97 secs| Accuracy 90.066

Epoch: 5 Train Loss 0.031 Time taken: 119.74 secs

	Epoch: 5 Validation Loss 0.09 Time taken: 1.99 secs| Accuracy 89.735
T\P		 0		 1
0		0161		0037		
1		0017		0239		

Accuracy 88.1057
Time taken: 2.9 secs
687.644 secs



======================================================================
----------------------------------------------------------------------
				run_ID=78
Sat Jul 24 18:02:57 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.027 GB
14 seconds taken

NEW CODE; fc_input = average of first 2 layers of BERT | optim separated | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 0.006 GB
3 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.368 Time taken: 5.88 secs

	Epoch: 1 Validation Loss 0.291 Time taken: 2.27 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.273 Time taken: 5.79 secs

	Epoch: 2 Validation Loss 0.253 Time taken: 2.37 secs| Accuracy 50.0

Epoch: 3 Train Loss 0.262 Time taken: 5.81 secs

	Epoch: 3 Validation Loss 0.252 Time taken: 2.29 secs| Accuracy 50.0

Epoch: 4 Train Loss 0.26 Time taken: 5.8 secs

	Epoch: 4 Validation Loss 0.252 Time taken: 2.33 secs| Accuracy 50.0

Epoch: 5 Train Loss 0.253 Time taken: 5.83 secs

	Epoch: 5 Validation Loss 0.25 Time taken: 2.29 secs| Accuracy 51.75
T\P		 0		 1
0		0194		0006		
1		0188		0012		

Accuracy 51.5
Time taken: 4.61 secs
108.975 secs


On the best model
T\P		 0		 1
0		0194		0006		
1		0188		0012		

Accuracy 51.5
Time taken: 2.37 secs


======================================================================
----------------------------------------------------------------------
				run_ID=79
Sat Jul 24 18:02:57 2021
GPU is available Tesla T4 15 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.002 GB
9 seconds taken

optim separated | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.666 GB
Memory taken on RAM 0.298 GB
3 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.24 Time taken: 13.38 secs

	Epoch: 1 Validation Loss 0.204 Time taken: 2.32 secs| Accuracy 69.0

Epoch: 2 Train Loss 0.161 Time taken: 13.66 secs

	Epoch: 2 Validation Loss 0.163 Time taken: 2.4 secs| Accuracy 78.0

Epoch: 3 Train Loss 0.098 Time taken: 13.69 secs

	Epoch: 3 Validation Loss 0.163 Time taken: 2.39 secs| Accuracy 78.0

Epoch: 4 Train Loss 0.058 Time taken: 13.76 secs

	Epoch: 4 Validation Loss 0.186 Time taken: 2.42 secs| Accuracy 75.0

Epoch: 5 Train Loss 0.034 Time taken: 13.9 secs

	Epoch: 5 Validation Loss 0.165 Time taken: 2.42 secs| Accuracy 78.0



======================================================================
----------------------------------------------------------------------
				run_ID=80
Tue Jul 27 12:00:19 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.411 GB
27 seconds taken

bigger max_len 100 ---> 200 | SequenceClassification | 400 sentence pairs for train, 200 for valid and test | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.561 GB
11 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.243 Time taken: 5.98 secs

	Epoch: 1 Validation Loss 0.201 Time taken: 0.99 secs| Accuracy 71.25

Epoch: 2 Train Loss 0.153 Time taken: 5.11 secs

	Epoch: 2 Validation Loss 0.163 Time taken: 0.99 secs| Accuracy 78.5

Epoch: 3 Train Loss 0.089 Time taken: 5.15 secs

	Epoch: 3 Validation Loss 0.157 Time taken: 1.04 secs| Accuracy 80.0

Epoch: 4 Train Loss 0.056 Time taken: 5.15 secs

	Epoch: 4 Validation Loss 0.155 Time taken: 1.04 secs| Accuracy 77.0

Epoch: 5 Train Loss 0.036 Time taken: 5.09 secs

	Epoch: 5 Validation Loss 0.209 Time taken: 1.02 secs| Accuracy 71.5
T\P		 0		 1
0		0104		0096		
1		0021		0179		

Accuracy 70.75
Time taken: 1.05 secs
55.508 secs


On the best model
T\P		 0		 1
0		0164		0036		
1		0045		0155		

Accuracy 79.75
Time taken: 1.01 secs



======================================================================
----------------------------------------------------------------------
				run_ID=81
Tue Jul 27 12:59:37 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM -1.054 GB
26 seconds taken

asha_sentsim | bigger max_len 100 ---> 200 | SequenceClassification | train_ratio:0.5 val_ratio:0.2 | random negative-sentence: False| model:bert-base-multilingual-cased
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.608 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.23 Time taken: 5.63 secs

	Epoch: 1 Validation Loss 0.132 Time taken: 0.89 secs| Accuracy 87.748

Epoch: 2 Train Loss 0.1 Time taken: 4.87 secs

	Epoch: 2 Validation Loss 0.098 Time taken: 0.92 secs| Accuracy 87.417

Epoch: 3 Train Loss 0.07 Time taken: 4.98 secs

	Epoch: 3 Validation Loss 0.091 Time taken: 0.96 secs| Accuracy 89.073

Epoch: 4 Train Loss 0.048 Time taken: 4.94 secs

	Epoch: 4 Validation Loss 0.1 Time taken: 0.93 secs| Accuracy 88.079

Epoch: 5 Train Loss 0.038 Time taken: 4.96 secs

	Epoch: 5 Validation Loss 0.093 Time taken: 0.91 secs| Accuracy 89.735
T\P		 0		 1
0		0154		0044		
1		0018		0238		

Accuracy 86.3436
Time taken: 1.22 secs
47.748 secs


On the best model
T\P		 0		 1
0		0154		0044		
1		0018		0238		

Accuracy 86.3436
Time taken: 1.23 secs


======================================================================
----------------------------------------------------------------------
				run_ID=82
Tue Jul 27 13:34:23 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.331 GB
27 seconds taken

inshorts_400pairs | optim separated | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.574 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.241 Time taken: 8.43 secs

	Epoch: 1 Validation Loss 0.202 Time taken: 1.51 secs| Accuracy 73.25

Epoch: 2 Train Loss 0.156 Time taken: 7.48 secs

	Epoch: 2 Validation Loss 0.157 Time taken: 1.5 secs| Accuracy 78.75

Epoch: 3 Train Loss 0.091 Time taken: 7.54 secs

	Epoch: 3 Validation Loss 0.152 Time taken: 1.5 secs| Accuracy 79.0

Epoch: 4 Train Loss 0.055 Time taken: 7.59 secs

	Epoch: 4 Validation Loss 0.165 Time taken: 1.52 secs| Accuracy 78.75

Epoch: 5 Train Loss 0.042 Time taken: 7.54 secs

	Epoch: 5 Validation Loss 0.174 Time taken: 1.56 secs| Accuracy 78.25
T\P		 0		 1
0		0133		0067		
1		0028		0172		

Accuracy 76.25
Time taken: 1.58 secs
64.21 secs


On the best model
T\P		 0		 1
0		0171		0029		
1		0052		0148		

Accuracy 79.75
Time taken: 1.58 secs


======================================================================
----------------------------------------------------------------------
				run_ID=83
Tue Jul 27 14:07:11 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.124 GB
27 seconds taken

inshorts_400pairs | optim separated | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 400
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.754 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.237 Time taken: 10.94 secs

	Epoch: 1 Validation Loss 0.203 Time taken: 2.0 secs| Accuracy 68.0

Epoch: 2 Train Loss 0.148 Time taken: 10.33 secs

	Epoch: 2 Validation Loss 0.168 Time taken: 2.0 secs| Accuracy 78.0

Epoch: 3 Train Loss 0.093 Time taken: 10.25 secs

	Epoch: 3 Validation Loss 0.157 Time taken: 1.99 secs| Accuracy 78.25

Epoch: 4 Train Loss 0.051 Time taken: 10.4 secs

	Epoch: 4 Validation Loss 0.165 Time taken: 2.03 secs| Accuracy 78.75

Epoch: 5 Train Loss 0.039 Time taken: 10.3 secs

	Epoch: 5 Validation Loss 0.193 Time taken: 1.99 secs| Accuracy 72.25
T\P		 0		 1
0		0120		0080		
1		0025		0175		

Accuracy 73.75
Time taken: 2.01 secs
86.436 secs


On the best model
T\P		 0		 1
0		0151		0049		
1		0037		0163		

Accuracy 78.5
Time taken: 2.01 secs


======================================================================
----------------------------------------------------------------------
				run_ID=84
Tue Jul 27 14:21:57 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.073 GB
29 seconds taken

asha_sentsim | optim separated | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.535 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 8.01 secs

	Epoch: 1 Validation Loss 0.111 Time taken: 1.24 secs| Accuracy 87.417

Epoch: 2 Train Loss 0.099 Time taken: 7.22 secs

	Epoch: 2 Validation Loss 0.095 Time taken: 1.22 secs| Accuracy 88.079

Epoch: 3 Train Loss 0.069 Time taken: 7.18 secs

	Epoch: 3 Validation Loss 0.097 Time taken: 1.22 secs| Accuracy 88.411

Epoch: 4 Train Loss 0.048 Time taken: 7.22 secs

	Epoch: 4 Validation Loss 0.102 Time taken: 1.21 secs| Accuracy 88.079

Epoch: 5 Train Loss 0.036 Time taken: 7.23 secs

	Epoch: 5 Validation Loss 0.113 Time taken: 1.2 secs| Accuracy 87.748
T\P		 0		 1
0		0152		0046		
1		0017		0239		

Accuracy 86.1233
Time taken: 1.71 secs
61.642 secs


On the best model
T\P		 0		 1
0		0153		0045		
1		0015		0241		

Accuracy 86.7841
Time taken: 1.7 secs


======================================================================
----------------------------------------------------------------------
				run_ID=85
Tue Jul 27 15:10:28 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 4
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 189 | Validation_batches: 76 | Test_batches: 114
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.177 GB
27 seconds taken

asha_sentsim | optim separated | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 400 <--------------------------------Difference
Memory taken on GPU 0.663 GB
Memory taken on RAM 2.374 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)

X X X X X X X X X X X X X X X X X X X X 
CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 2.35 GiB already allocated; 11.56 MiB free; 2.46 GiB reserved in total by PyTorc
X X X X X X X X X X X X X X X X X X X X 



======================================================================
----------------------------------------------------------------------
				run_ID=85
Tue Jul 27 15:39:20 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.472 GB
28 seconds taken

different 400: 0-400 ---> 400-800inshorts_400pairs | optim separated | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.816 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.261 Time taken: 8.26 secs

	Epoch: 1 Validation Loss 0.235 Time taken: 1.49 secs| Accuracy 62.5

Epoch: 2 Train Loss 0.182 Time taken: 7.58 secs

	Epoch: 2 Validation Loss 0.18 Time taken: 1.53 secs| Accuracy 75.25

Epoch: 3 Train Loss 0.116 Time taken: 7.55 secs

	Epoch: 3 Validation Loss 0.152 Time taken: 1.49 secs| Accuracy 80.5

Epoch: 4 Train Loss 0.078 Time taken: 7.54 secs

	Epoch: 4 Validation Loss 0.13 Time taken: 1.54 secs| Accuracy 83.5

Epoch: 5 Train Loss 0.051 Time taken: 7.55 secs

	Epoch: 5 Validation Loss 0.123 Time taken: 1.51 secs| Accuracy 85.75
T\P		 0		 1
0		0150		0050		
1		0037		0163		

Accuracy 78.25
Time taken: 1.49 secs
69.83 secs


On the best model
T\P		 0		 1
0		0150		0050		
1		0037		0163		

Accuracy 78.25
Time taken: 1.54 secs


======================================================================
----------------------------------------------------------------------
				run_ID=86
Tue Jul 27 16:18:35 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.458 GB
27 seconds taken

inshorts_400pairs | optim separated | arch:last_two | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.82 GB
9 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.481 Time taken: 8.22 secs

	Epoch: 1 Validation Loss 0.243 Time taken: 1.53 secs| Accuracy 60.75

Epoch: 2 Train Loss 0.17 Time taken: 7.12 secs

	Epoch: 2 Validation Loss 0.193 Time taken: 1.51 secs| Accuracy 72.25

Epoch: 3 Train Loss 0.128 Time taken: 7.32 secs

	Epoch: 3 Validation Loss 0.179 Time taken: 1.5 secs| Accuracy 76.75

Epoch: 4 Train Loss 0.095 Time taken: 7.33 secs

	Epoch: 4 Validation Loss 0.169 Time taken: 1.51 secs| Accuracy 77.75

Epoch: 5 Train Loss 0.075 Time taken: 7.16 secs

	Epoch: 5 Validation Loss 0.156 Time taken: 1.53 secs| Accuracy 80.75
T\P		 0		 1
0		0164		0036		
1		0039		0161		

Accuracy 81.25
Time taken: 1.46 secs
69.963 secs


On the best model
T\P		 0		 1
0		0164		0036		
1		0039		0161		

Accuracy 81.25
Time taken: 1.51 secs


======================================================================
----------------------------------------------------------------------
				run_ID=87
Tue Jul 27 16:30:12 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.675 GB
27 seconds taken

inshorts_400pairs | optim separated | arch:first_two | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.817 GB
9 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.395 Time taken: 4.09 secs

	Epoch: 1 Validation Loss 0.335 Time taken: 1.49 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.309 Time taken: 3.06 secs

	Epoch: 2 Validation Loss 0.28 Time taken: 1.5 secs| Accuracy 50.0

Epoch: 3 Train Loss 0.277 Time taken: 3.09 secs

	Epoch: 3 Validation Loss 0.258 Time taken: 1.55 secs| Accuracy 50.0

Epoch: 4 Train Loss 0.263 Time taken: 3.05 secs

	Epoch: 4 Validation Loss 0.252 Time taken: 1.51 secs| Accuracy 50.0

Epoch: 5 Train Loss 0.256 Time taken: 3.05 secs

	Epoch: 5 Validation Loss 0.25 Time taken: 1.51 secs| Accuracy 50.0
T\P		 0		 1
0		0200		0000		
1		0200		0000		

Accuracy 50.0
Time taken: 1.54 secs
36.487 secs


On the best model
T\P		 0		 1
0		0200		0000		
1		0200		0000		

Accuracy 50.0
Time taken: 1.58 secs



======================================================================
----------------------------------------------------------------------
				run_ID=88
Thu Jul 29 17:15:51 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.089 GB
26 seconds taken

inshorts_400pairs | different random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.746 GB
13 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.299 Time taken: 8.73 secs

	Epoch: 1 Validation Loss 0.217 Time taken: 1.53 secs| Accuracy 65.75

Epoch: 2 Train Loss 0.156 Time taken: 7.69 secs

	Epoch: 2 Validation Loss 0.166 Time taken: 1.55 secs| Accuracy 75.0

Epoch: 3 Train Loss 0.105 Time taken: 7.51 secs

	Epoch: 3 Validation Loss 0.192 Time taken: 1.55 secs| Accuracy 72.0

Epoch: 4 Train Loss 0.07 Time taken: 7.54 secs

	Epoch: 4 Validation Loss 0.148 Time taken: 1.53 secs| Accuracy 79.75

Epoch: 5 Train Loss 0.043 Time taken: 7.6 secs

	Epoch: 5 Validation Loss 0.17 Time taken: 1.58 secs| Accuracy 76.75
T\P		 0		 1
0		0186		0014		
1		0081		0119		

Accuracy 76.25
Time taken: 1.61 secs
63.66 secs


On the best model
T\P		 0		 1
0		0179		0021		
1		0066		0134		

Accuracy 78.25
Time taken: 1.66 secs


======================================================================
----------------------------------------------------------------------
				run_ID=89
Thu Jul 29 17:39:41 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.144 GB
31 seconds taken

inshorts_full | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.192 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.103 Time taken: 159.98 secs

	Epoch: 1 Validation Loss 0.057 Time taken: 21.25 secs| Accuracy 91.975

Epoch: 2 Train Loss 0.041 Time taken: 161.55 secs

	Epoch: 2 Validation Loss 0.038 Time taken: 21.47 secs| Accuracy 95.154

Epoch: 3 Train Loss 0.025 Time taken: 162.24 secs

	Epoch: 3 Validation Loss 0.043 Time taken: 21.49 secs| Accuracy 94.448

Epoch: 4 Train Loss 0.017 Time taken: 161.49 secs

	Epoch: 4 Validation Loss 0.04 Time taken: 21.54 secs| Accuracy 95.211

Epoch: 5 Train Loss 0.013 Time taken: 162.75 secs

	Epoch: 5 Validation Loss 0.041 Time taken: 21.57 secs| Accuracy 95.041
T\P		 0		 1
0		4865		0444		
1		0117		5191		

Accuracy 94.716
Time taken: 32.42 secs
963.229 secs


On the best model
T\P		 0		 1
0		4871		0438		
1		0123		5185		

Accuracy 94.716
Time taken: 37.62 secs


======================================================================
----------------------------------------------------------------------
				run_ID=90
Thu Jul 29 17:57:20 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.897 GB
31 seconds taken

inshorts_full | 1230 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.148 GB
9 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.103 Time taken: 163.8 secs

	Epoch: 1 Validation Loss 0.058 Time taken: 21.81 secs| Accuracy 92.116

Epoch: 2 Train Loss 0.041 Time taken: 163.45 secs

	Epoch: 2 Validation Loss 0.067 Time taken: 21.66 secs| Accuracy 91.339

Epoch: 3 Train Loss 0.027 Time taken: 169.67 secs

	Epoch: 3 Validation Loss 0.056 Time taken: 21.69 secs| Accuracy 92.851

Epoch: 4 Train Loss 0.022 Time taken: 163.08 secs

	Epoch: 4 Validation Loss 0.048 Time taken: 21.77 secs| Accuracy 94.052

Epoch: 5 Train Loss 0.016 Time taken: 163.5 secs

	Epoch: 5 Validation Loss 0.038 Time taken: 21.57 secs| Accuracy 95.366
T\P		 0		 1
0		5077		0232		
1		0248		5060		

Accuracy 95.4789
Time taken: 32.14 secs
985.579 secs


On the best model
T\P		 0		 1
0		5077		0232		
1		0248		5060		

Accuracy 95.4789
Time taken: 32.11 secs


======================================================================
----------------------------------------------------------------------
				run_ID=91
Thu Jul 29 19:06:22 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -1.324 GB
33 seconds taken

inshorts_full | 123 random seed | arch:first_two | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 5.693 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.268 Time taken: 61.54 secs

	Epoch: 1 Validation Loss 0.25 Time taken: 21.31 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.257 Time taken: 61.41 secs

	Epoch: 2 Validation Loss 0.251 Time taken: 21.27 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.257 Time taken: 61.53 secs

	Epoch: 3 Validation Loss 0.25 Time taken: 21.37 secs| Accuracy 49.986

Epoch: 4 Train Loss 0.257 Time taken: 61.53 secs

	Epoch: 4 Validation Loss 0.25 Time taken: 21.35 secs| Accuracy 49.986

Epoch: 5 Train Loss 0.256 Time taken: 61.5 secs

	Epoch: 5 Validation Loss 0.25 Time taken: 21.35 secs| Accuracy 49.986
T\P		 0		 1
0		5309		0000		
1		5308		0000		

Accuracy 50.0047
Time taken: 31.95 secs
455.184 secs


On the best model
T\P		 0		 1
0		5309		0000		
1		5308		0000		

Accuracy 50.0047
Time taken: 31.85 secs


======================================================================
----------------------------------------------------------------------
				run_ID=92
Thu Jul 29 19:19:13 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.787 GB
32 seconds taken

inshorts_full | 123 random seed | arch:last_two | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.74 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.134 Time taken: 153.15 secs

	Epoch: 1 Validation Loss 0.069 Time taken: 21.41 secs| Accuracy 92.498

Epoch: 2 Train Loss 0.057 Time taken: 153.07 secs

	Epoch: 2 Validation Loss 0.047 Time taken: 21.55 secs| Accuracy 94.843

Epoch: 3 Train Loss 0.035 Time taken: 154.39 secs

	Epoch: 3 Validation Loss 0.045 Time taken: 21.53 secs| Accuracy 94.504

Epoch: 4 Train Loss 0.022 Time taken: 153.11 secs

	Epoch: 4 Validation Loss 0.041 Time taken: 21.44 secs| Accuracy 95.394

Epoch: 5 Train Loss 0.016 Time taken: 152.75 secs

	Epoch: 5 Validation Loss 0.036 Time taken: 21.46 secs| Accuracy 95.366
T\P		 0		 1
0		5049		0260		
1		0203		5105		

Accuracy 95.6391
Time taken: 32.14 secs
924.09 secs


On the best model
T\P		 0		 1
0		5070		0239		
1		0264		5044		

Accuracy 95.2623
Time taken: 32.1 secs


======================================================================
----------------------------------------------------------------------
				run_ID=93
Thu Jul 29 20:31:22 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.295 GB
27 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: True| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.832 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.117 Time taken: 8.25 secs

	Epoch: 1 Validation Loss 0.043 Time taken: 1.46 secs| Accuracy 94.5

Epoch: 2 Train Loss 0.035 Time taken: 7.46 secs

	Epoch: 2 Validation Loss 0.039 Time taken: 1.41 secs| Accuracy 96.0

Epoch: 3 Train Loss 0.024 Time taken: 7.37 secs

	Epoch: 3 Validation Loss 0.041 Time taken: 1.38 secs| Accuracy 95.0

Epoch: 4 Train Loss 0.019 Time taken: 7.43 secs

	Epoch: 4 Validation Loss 0.037 Time taken: 1.45 secs| Accuracy 95.0

Epoch: 5 Train Loss 0.013 Time taken: 7.49 secs

	Epoch: 5 Validation Loss 0.039 Time taken: 1.45 secs| Accuracy 94.0
T\P		 0		 1
0		0197		0003		
1		0009		0191		

Accuracy 97.0
Time taken: 1.44 secs
57.909 secs


On the best model
T\P		 0		 1
0		0198		0002		
1		0012		0188		

Accuracy 96.5
Time taken: 1.46 secs




======================================================================
----------------------------------------------------------------------
				run_ID=94
Fri Jul 30 14:27:38 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.089 GB
28 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.753 GB
10 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.241 Time taken: 9.34 secs

	Epoch: 1 Validation Loss 0.202 Time taken: 1.49 secs| Accuracy 73.25

Epoch: 2 Train Loss 0.156 Time taken: 9.13 secs

	Epoch: 2 Validation Loss 0.157 Time taken: 1.51 secs| Accuracy 78.75

Epoch: 3 Train Loss 0.091 Time taken: 9.54 secs

	Epoch: 3 Validation Loss 0.152 Time taken: 1.52 secs| Accuracy 79.0

Epoch: 4 Train Loss 0.055 Time taken: 9.7 secs

	Epoch: 4 Validation Loss 0.165 Time taken: 1.53 secs| Accuracy 78.75

Epoch: 5 Train Loss 0.042 Time taken: 10.07 secs

	Epoch: 5 Validation Loss 0.174 Time taken: 1.56 secs| Accuracy 78.25

Epoch: 6 Train Loss 0.033 Time taken: 10.43 secs

	Epoch: 6 Validation Loss 0.156 Time taken: 1.57 secs| Accuracy 80.5

Epoch: 7 Train Loss 0.025 Time taken: 10.96 secs

	Epoch: 7 Validation Loss 0.154 Time taken: 1.52 secs| Accuracy 78.0

Epoch: 8 Train Loss 0.022 Time taken: 11.27 secs

	Epoch: 8 Validation Loss 0.147 Time taken: 1.6 secs| Accuracy 81.0

Epoch: 9 Train Loss 0.021 Time taken: 11.54 secs

	Epoch: 9 Validation Loss 0.167 Time taken: 1.59 secs| Accuracy 79.25

Epoch: 10 Train Loss 0.019 Time taken: 12.06 secs

	Epoch: 10 Validation Loss 0.167 Time taken: 1.61 secs| Accuracy 79.75

Epoch: 11 Train Loss 0.018 Time taken: 12.01 secs

	Epoch: 11 Validation Loss 0.166 Time taken: 1.55 secs| Accuracy 82.0

Epoch: 12 Train Loss 0.016 Time taken: 13.07 secs

	Epoch: 12 Validation Loss 0.153 Time taken: 1.58 secs| Accuracy 81.5

Epoch: 13 Train Loss 0.01 Time taken: 13.24 secs

	Epoch: 13 Validation Loss 0.15 Time taken: 1.58 secs| Accuracy 81.75

Epoch: 14 Train Loss 0.009 Time taken: 13.67 secs

	Epoch: 14 Validation Loss 0.15 Time taken: 1.57 secs| Accuracy 81.75

Epoch: 15 Train Loss 0.008 Time taken: 14.33 secs

	Epoch: 15 Validation Loss 0.15 Time taken: 1.59 secs| Accuracy 81.5

Epoch: 16 Train Loss 0.008 Time taken: 14.34 secs

	Epoch: 16 Validation Loss 0.148 Time taken: 1.53 secs| Accuracy 82.25

Epoch: 17 Train Loss 0.007 Time taken: 14.33 secs

	Epoch: 17 Validation Loss 0.152 Time taken: 1.55 secs| Accuracy 81.25

Epoch: 18 Train Loss 0.008 Time taken: 14.73 secs

	Epoch: 18 Validation Loss 0.152 Time taken: 1.53 secs| Accuracy 81.75

Epoch: 19 Train Loss 0.007 Time taken: 15.24 secs

	Epoch: 19 Validation Loss 0.15 Time taken: 1.57 secs| Accuracy 82.0

Epoch: 20 Train Loss 0.007 Time taken: 15.31 secs

	Epoch: 20 Validation Loss 0.152 Time taken: 1.63 secs| Accuracy 82.0

Epoch: 21 Train Loss 0.006 Time taken: 15.31 secs

	Epoch: 21 Validation Loss 0.152 Time taken: 1.57 secs| Accuracy 81.75

Epoch: 22 Train Loss 0.006 Time taken: 15.56 secs

	Epoch: 22 Validation Loss 0.151 Time taken: 1.52 secs| Accuracy 82.5

Epoch: 23 Train Loss 0.006 Time taken: 15.98 secs

	Epoch: 23 Validation Loss 0.149 Time taken: 1.54 secs| Accuracy 82.5

Epoch: 24 Train Loss 0.006 Time taken: 16.15 secs

	Epoch: 24 Validation Loss 0.154 Time taken: 1.56 secs| Accuracy 82.25

Epoch: 25 Train Loss 0.006 Time taken: 17.31 secs

	Epoch: 25 Validation Loss 0.152 Time taken: 1.55 secs| Accuracy 82.75

Epoch: 26 Train Loss 0.006 Time taken: 17.54 secs

	Epoch: 26 Validation Loss 0.151 Time taken: 1.56 secs| Accuracy 82.5

Epoch: 27 Train Loss 0.005 Time taken: 17.7 secs

	Epoch: 27 Validation Loss 0.152 Time taken: 1.56 secs| Accuracy 82.25

Epoch: 28 Train Loss 0.005 Time taken: 18.0 secs

	Epoch: 28 Validation Loss 0.153 Time taken: 1.56 secs| Accuracy 82.25

Epoch: 29 Train Loss 0.005 Time taken: 18.51 secs

	Epoch: 29 Validation Loss 0.153 Time taken: 1.58 secs| Accuracy 82.5

Epoch: 30 Train Loss 0.005 Time taken: 18.87 secs

	Epoch: 30 Validation Loss 0.156 Time taken: 1.52 secs| Accuracy 82.5
T\P		 0		 1
0		0174		0026		
1		0046		0154		

Accuracy 82.0
Time taken: 1.56 secs
498.04 secs


On the best model
T\P		 0		 1
0		0168		0032		
1		0044		0156		

Accuracy 81.0
Time taken: 1.53 secs


======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 15:23:03 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.178 GB
29 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.731 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)

X X X X X X X X X X X X X X X X X X X X 
'float' object is not subscriptable
X X X X X X X X X X X X X X X X X X X X 



======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 15:28:08 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.859 GB
28 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.476 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)

X X X X X X X X X X X X X X X X X X X X 
name 'mn' is not defined
X X X X X X X X X X X X X X X X X X X X 



======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 15:32:18 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.187 GB
28 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 1.103 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)

X X X X X X X X X X X X X X X X X X X X 
name 'mat_bin' is not defined
X X X X X X X X X X X X X X X X X X X X 



======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 15:48:08 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -1.158 GB
29 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 5.532 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)

X X X X X X X X X X X X X X X X X X X X 
can only concatenate str (not "numpy.float64") to str
X X X X X X X X X X X X X X X X X X X X 



======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 15:49:52 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.103 GB
29 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.763 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 15:56:41 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.089 GB
29 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.768 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 16:01:32 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.325 GB
29 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.781 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
				run_ID=95
Fri Jul 30 16:05:13 2021
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.279 GB
28 seconds taken

inshorts_400pairs | 123 random seed | arch:SequenceClassification | random negative-sentence: False| model:bert-base-multilingual-cased| max_len: 300
Memory taken on GPU 0.663 GB
Memory taken on RAM 3.339 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.241 Time taken: 8945.43 secs

Epoch: 2 Train Loss 0.156 Time taken: 8924.6 secs

Epoch: 3 Train Loss 0.091 Time taken: 8901.46 secs

Epoch: 4 Train Loss 0.055 Time taken: 8918.19 secs

Epoch: 5 Train Loss 0.042 Time taken: 8941.14 secs

Epoch: 6 Train Loss 0.033 Time taken: 8953.76 secs

Epoch: 7 Train Loss 0.025 Time taken: 8937.54 secs

Epoch: 8 Train Loss 0.022 Time taken: 8939.73 secs

Epoch: 9 Train Loss 0.021 Time taken: 8977.81 secs

Epoch: 10 Train Loss 0.019 Time taken: 8950.1 secs

Epoch: 11 Train Loss 0.018 Time taken: 8951.41 secs

Epoch: 12 Train Loss 0.016 Time taken: 8949.49 secs

Epoch: 13 Train Loss 0.01 Time taken: 8884.76 secs

Epoch: 14 Train Loss 0.009 Time taken: 9039.09 secs

Epoch: 15 Train Loss 0.008 Time taken: 9019.53 secs

Epoch: 16 Train Loss 0.008 Time taken: 8937.51 secs

Epoch: 17 Train Loss 0.007 Time taken: 8935.99 secs



======================================================================
----------------------------------------------------------------------
                run_ID=96
Sat Apr  2 14:41:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.868 GB
27 seconds taken

{
    "run_ID": 96,
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE"
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.01 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.322 Time taken: 599.4 secs

    Epoch: 1 Validation Loss 0.175 Time taken: 22.28 secs| Accuracy 93.289

Epoch: 2 Train Loss 0.127 Time taken: 587.13 secs

    Epoch: 2 Validation Loss 0.133 Time taken: 22.28 secs| Accuracy 94.928

Epoch: 3 Train Loss 0.068 Time taken: 585.02 secs

    Epoch: 3 Validation Loss 0.146 Time taken: 22.29 secs| Accuracy 95.013

Epoch: 4 Train Loss 0.045 Time taken: 586.1 secs

    Epoch: 4 Validation Loss 0.15 Time taken: 22.32 secs| Accuracy 95.069

Epoch: 5 Train Loss 0.032 Time taken: 585.49 secs

    Epoch: 5 Validation Loss 0.147 Time taken: 22.31 secs| Accuracy 95.451
T\P      0       1
0       4982        0327        
1       0201        5107        

Accuracy 95.0268
Time taken: 33.25 secs
3107.103 secs


On the best model
T\P      0       1
0       4982        0327        
1       0201        5107        

Accuracy 95.0268
Time taken: 33.85 secs


======================================================================
----------------------------------------------------------------------
                run_ID=97
Sat Apr  2 18:25:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.08 GB
28 seconds taken

{
    "run_ID": 97,
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE"
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.383 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.438 Time taken: 612.96 secs

    Epoch: 1 Validation Loss 0.192 Time taken: 22.51 secs| Accuracy 92.569

Epoch: 2 Train Loss 0.171 Time taken: 600.23 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 22.87 secs| Accuracy 94.292

Epoch: 3 Train Loss 0.108 Time taken: 597.57 secs

    Epoch: 3 Validation Loss 0.119 Time taken: 22.62 secs| Accuracy 95.691

Epoch: 4 Train Loss 0.072 Time taken: 596.5 secs

    Epoch: 4 Validation Loss 0.118 Time taken: 22.59 secs| Accuracy 95.634

Epoch: 5 Train Loss 0.052 Time taken: 591.89 secs

    Epoch: 5 Validation Loss 0.139 Time taken: 22.51 secs| Accuracy 95.493
T\P      0       1
0       5149        0160        
1       0301        5007        

Accuracy 95.6579
Time taken: 33.38 secs
3172.21 secs


On the best model
T\P      0       1
0       5054        0255        
1       0202        5106        

Accuracy 95.6956
Time taken: 33.9 secs


======================================================================
----------------------------------------------------------------------
                run_ID=98
Sat Apr  2 20:45:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.238 GB
0 seconds taken

{
    "run_ID": 98,
    "createData": false,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE"
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.469 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.438 Time taken: 616.88 secs

    Epoch: 1 Validation Loss 0.192 Time taken: 22.52 secs| Accuracy 92.569

Epoch: 2 Train Loss 0.171 Time taken: 605.36 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 22.79 secs| Accuracy 94.292

Epoch: 3 Train Loss 0.108 Time taken: 603.39 secs

    Epoch: 3 Validation Loss 0.119 Time taken: 22.7 secs| Accuracy 95.691

Epoch: 4 Train Loss 0.072 Time taken: 600.19 secs

    Epoch: 4 Validation Loss 0.118 Time taken: 22.79 secs| Accuracy 95.634

Epoch: 5 Train Loss 0.052 Time taken: 599.0 secs

    Epoch: 5 Validation Loss 0.139 Time taken: 22.68 secs| Accuracy 95.493
T\P      0       1
0       5149        0160        
1       0301        5007        

Accuracy 95.6579
Time taken: 33.48 secs
3207.59 secs


On the best model
T\P      0       1
0       5054        0255        
1       0202        5106        

Accuracy 95.6956
Time taken: 33.53 secs


======================================================================
----------------------------------------------------------------------
                run_ID=99
Sat Apr  2 20:47:19 2022
GPU is available GeForce RTX 3090 24 GB

Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
28 seconds taken

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.238 GB
0 seconds taken

{
    "run_ID": 99,
    "createData": false,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE"
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.465 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.438 Time taken: 592.32 secs

    Epoch: 1 Validation Loss 0.192 Time taken: 22.16 secs| Accuracy 92.569

Epoch: 2 Train Loss 0.171 Time taken: 587.39 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 22.28 secs| Accuracy 94.292

Epoch: 3 Train Loss 0.108 Time taken: 588.03 secs

    Epoch: 3 Validation Loss 0.119 Time taken: 22.32 secs| Accuracy 95.691

Epoch: 4 Train Loss 0.072 Time taken: 587.06 secs

    Epoch: 4 Validation Loss 0.118 Time taken: 22.36 secs| Accuracy 95.634

Epoch: 5 Train Loss 0.052 Time taken: 587.82 secs

    Epoch: 5 Validation Loss 0.139 Time taken: 22.31 secs| Accuracy 95.493
T\P      0       1
0       5149        0160        
1       0301        5007        

Accuracy 95.6579
Time taken: 33.18 secs
3121.176 secs


On the best model
T\P      0       1
0       5054        0255        
1       0202        5106        

Accuracy 95.6956
Time taken: 32.97 secs


======================================================================
----------------------------------------------------------------------
                run_ID=100
Sat Apr  2 20:51:29 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 2212 | Validation_batches: 885 | Test_batches: 1328                      
Memory taken on GPU 0.0 GB                                  
Memory taken on RAM 5.109 GB
Batch_size: 8
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.238 GB
0 seconds taken

{
    "run_ID": 100,
    "createData": false,
    "bs": 8,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE"
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 4.443 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.362 Time taken: 1500.46 secs

    Epoch: 1 Validation Loss 0.226 Time taken: 56.84 secs| Accuracy 92.173

Epoch: 2 Train Loss 0.139 Time taken: 1500.85 secs

    Epoch: 2 Validation Loss 0.118 Time taken: 55.97 secs| Accuracy 95.408

Epoch: 3 Train Loss 0.089 Time taken: 1484.39 secs

    Epoch: 3 Validation Loss 0.147 Time taken: 56.27 secs| Accuracy 95.182

Epoch: 4 Train Loss 0.06 Time taken: 1493.17 secs

    Epoch: 4 Validation Loss 0.129 Time taken: 56.39 secs| Accuracy 95.776

Epoch: 5 Train Loss 0.046 Time taken: 1492.8 secs

    Epoch: 5 Validation Loss 0.139 Time taken: 56.32 secs| Accuracy 96.016
T\P      0       1
0       5121        0188        
1       0204        5104        

Accuracy 96.3078
Time taken: 84.3 secs
7872.945 secs


On the best model
T\P      0       1
0       5121        0188        
1       0204        5104        

Accuracy 96.3078
Time taken: 84.52 secs



======================================================================
----------------------------------------------------------------------
                run_ID=101
Sun Apr  3 01:14:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.245 GB
0 seconds taken

{
    "run_ID": 101,
    "createData": false,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE"
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.556 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.494 Time taken: 628.25 secs

    Epoch: 1 Validation Loss 0.23 Time taken: 23.48 secs| Accuracy 90.562

Epoch: 2 Train Loss 0.187 Time taken: 616.5 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 23.07 secs| Accuracy 93.882

Epoch: 3 Train Loss 0.115 Time taken: 616.57 secs

    Epoch: 3 Validation Loss 0.135 Time taken: 23.35 secs| Accuracy 95.013

Epoch: 4 Train Loss 0.081 Time taken: 611.78 secs

    Epoch: 4 Validation Loss 0.122 Time taken: 23.39 secs| Accuracy 95.38

Epoch: 5 Train Loss 0.054 Time taken: 622.63 secs

    Epoch: 5 Validation Loss 0.122 Time taken: 23.24 secs| Accuracy 95.988
T\P      0       1
0       4933        0376        
1       0119        5189        

Accuracy 95.3377
Time taken: 34.22 secs
3292.24 secs


On the best model
T\P      0       1
0       4933        0376        
1       0119        5189        

Accuracy 95.3377
Time taken: 36.07 secs



======================================================================
----------------------------------------------------------------------
                run_ID=102
Sun Apr  3 01:23:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 16
Old sets
Old batches
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.073 GB
0 seconds taken

{
    "run_ID": 102,
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": false,
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 11.029 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.555 Time taken: 616.51 secs

    Epoch: 1 Validation Loss 0.334 Time taken: 24.93 secs| Accuracy 86.507

Epoch: 2 Train Loss 0.259 Time taken: 605.17 secs

    Epoch: 2 Validation Loss 0.24 Time taken: 25.05 secs| Accuracy 90.28

Epoch: 3 Train Loss 0.167 Time taken: 604.91 secs

    Epoch: 3 Validation Loss 0.221 Time taken: 25.06 secs| Accuracy 91.127

Epoch: 4 Train Loss 0.104 Time taken: 611.7 secs

    Epoch: 4 Validation Loss 0.232 Time taken: 24.91 secs| Accuracy 91.707

Epoch: 5 Train Loss 0.059 Time taken: 599.91 secs

    Epoch: 5 Validation Loss 0.278 Time taken: 24.72 secs| Accuracy 91.537
T\P      0       1
0       4808        0501        
1       0438        4870        

Accuracy 91.1557
Time taken: 36.4 secs
3208.321 secs


On the best model
T\P      0       1
0       4943        0366        
1       0576        4732        

Accuracy 91.1274
Time taken: 36.44 secs


======================================================================
----------------------------------------------------------------------
                run_ID=103
Sun Apr  3 02:18:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.811 GB
28 seconds taken

{
    "run_ID": 103,
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": false,
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.063 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.299 Time taken: 622.58 secs

    Epoch: 1 Validation Loss 0.156 Time taken: 24.48 secs| Accuracy 94.306

Epoch: 2 Train Loss 0.106 Time taken: 623.49 secs

    Epoch: 2 Validation Loss 0.188 Time taken: 24.52 secs| Accuracy 92.809

Epoch: 3 Train Loss 0.06 Time taken: 618.46 secs

    Epoch: 3 Validation Loss 0.147 Time taken: 24.36 secs| Accuracy 95.055

Epoch: 4 Train Loss 0.04 Time taken: 622.19 secs

    Epoch: 4 Validation Loss 0.142 Time taken: 24.31 secs| Accuracy 95.691

Epoch: 5 Train Loss 0.026 Time taken: 617.81 secs

    Epoch: 5 Validation Loss 0.255 Time taken: 24.21 secs| Accuracy 93.642
T\P      0       1
0       5218        0091        
1       0577        4731        

Accuracy 93.7082
Time taken: 35.81 secs
3280.102 secs


On the best model
T\P      0       1
0       5067        0242        
1       0249        5059        

Accuracy 95.3753
Time taken: 35.82 secs


======================================================================
----------------------------------------------------------------------
                run_ID=104
Sun Apr  3 03:15:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.211 GB
91 seconds taken

{
    "run_ID": 104,
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-mlm-100-1280",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 1280
}
myBert(
  (model): XLMModel(
 xlm-mlm-100-1280 )
Memory taken on GPU 2.162 GB
Memory taken on RAM 5.607 GB
69 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=104
Sun Apr  3 14:21:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.191 GB
72 seconds taken

{
    "run_ID": 104,
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-mlm-100-1280",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 1280
}
myBert(
  (model): XLMModel(
 xlm-mlm-100-1280 )
Memory taken on GPU 2.162 GB
Memory taken on RAM 5.377 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 1671.11 secs

    Epoch: 1 Validation Loss 0.695 Time taken: 62.66 secs| Accuracy 50.014

Epoch: 2 Train Loss 0.694 Time taken: 1676.78 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 62.67 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.694 Time taken: 1688.55 secs

    Epoch: 3 Validation Loss 0.694 Time taken: 62.97 secs| Accuracy 50.014

Epoch: 4 Train Loss 0.695 Time taken: 1676.56 secs

    Epoch: 4 Validation Loss 0.694 Time taken: 62.73 secs| Accuracy 50.014

Epoch: 5 Train Loss 0.694 Time taken: 1687.75 secs

    Epoch: 5 Validation Loss 0.693 Time taken: 62.37 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.0047
Time taken: 93.49 secs
8844.037 secs


On the best model
T\P      0       1
0       0000        5309        
1       0000        5308        

Accuracy 49.9953
Time taken: 93.38 secs


======================================================================
----------------------------------------------------------------------
                run_ID=105
Sun Apr  3 16:52:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.218 GB
97 seconds taken

{
    "run_ID": 105,
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-mlm-tlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-tlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 5.138 GB
57 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.703 Time taken: 934.25 secs

    Epoch: 1 Validation Loss 0.697 Time taken: 32.31 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.698 Time taken: 929.2 secs

    Epoch: 2 Validation Loss 0.694 Time taken: 32.38 secs| Accuracy 50.014

Epoch: 3 Train Loss 0.696 Time taken: 934.51 secs

    Epoch: 3 Validation Loss 0.694 Time taken: 32.35 secs| Accuracy 49.986

Epoch: 4 Train Loss 0.696 Time taken: 931.94 secs

    Epoch: 4 Validation Loss 0.698 Time taken: 32.46 secs| Accuracy 50.014

Epoch: 5 Train Loss 0.695 Time taken: 935.27 secs

    Epoch: 5 Validation Loss 0.697 Time taken: 32.46 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.0047
Time taken: 48.29 secs
4893.983 secs


On the best model
T\P      0       1
0       0000        5309        
1       0000        5308        

Accuracy 49.9953
Time taken: 48.11 secs


======================================================================
----------------------------------------------------------------------
                run_ID=106
Sun Apr  3 18:17:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.289 GB
88 seconds taken

{
    "run_ID": 106,
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-mlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 9.683 GB
76 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.702 Time taken: 931.96 secs

    Epoch: 1 Validation Loss 0.7 Time taken: 32.57 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.699 Time taken: 935.44 secs

    Epoch: 2 Validation Loss 0.694 Time taken: 32.5 secs| Accuracy 50.014

Epoch: 3 Train Loss 0.695 Time taken: 929.29 secs

    Epoch: 3 Validation Loss 0.694 Time taken: 32.67 secs| Accuracy 49.986

Epoch: 4 Train Loss 0.695 Time taken: 944.13 secs

    Epoch: 4 Validation Loss 0.698 Time taken: 33.4 secs| Accuracy 50.014

Epoch: 5 Train Loss 0.695 Time taken: 948.09 secs

    Epoch: 5 Validation Loss 0.696 Time taken: 33.04 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.0047
Time taken: 49.01 secs
4920.47 secs


On the best model
T\P      0       1
0       0000        5309        
1       0000        5308        

Accuracy 49.9953
Time taken: 48.28 secs



======================================================================
----------------------------------------------------------------------
                run_ID=107
Mon Apr  4 11:09:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.058 GB
31 seconds taken

{
    "run_ID": 107,
    "message": "repeating 99 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.391 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.456 Time taken: 589.45 secs

    Epoch: 1 Validation Loss 0.209 Time taken: 22.76 secs| Accuracy 91.947

Epoch: 2 Train Loss 0.171 Time taken: 586.53 secs

    Epoch: 2 Validation Loss 0.14 Time taken: 22.9 secs| Accuracy 94.744

Epoch: 3 Train Loss 0.108 Time taken: 586.08 secs

    Epoch: 3 Validation Loss 0.12 Time taken: 22.91 secs| Accuracy 95.973

Epoch: 4 Train Loss 0.069 Time taken: 584.36 secs

    Epoch: 4 Validation Loss 0.133 Time taken: 22.84 secs| Accuracy 95.479

Epoch: 5 Train Loss 0.048 Time taken: 584.45 secs

    Epoch: 5 Validation Loss 0.127 Time taken: 22.81 secs| Accuracy 96.185
T\P      0       1
0       5056        0253        
1       0179        5129        

Accuracy 95.9311
Time taken: 33.56 secs
3113.09 secs


On the best model
T\P      0       1
0       5056        0253        
1       0179        5129        

Accuracy 95.9311
Time taken: 33.56 secs



======================================================================
----------------------------------------------------------------------
                run_ID=108
Mon Apr  4 16:01:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.062 GB
28 seconds taken

{
    "run_ID": 108,
    "message": "99 with random sents on",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.371 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.268 Time taken: 592.71 secs

    Epoch: 1 Validation Loss 0.101 Time taken: 22.64 secs| Accuracy 97.061

Epoch: 2 Train Loss 0.133 Time taken: 590.12 secs

    Epoch: 2 Validation Loss 0.093 Time taken: 22.62 secs| Accuracy 97.118

Epoch: 3 Train Loss 0.09 Time taken: 592.33 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 22.61 secs| Accuracy 98.163

Epoch: 4 Train Loss 0.062 Time taken: 586.05 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 22.66 secs| Accuracy 98.63

Epoch: 5 Train Loss 0.047 Time taken: 585.35 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 22.66 secs| Accuracy 98.63
T\P      0       1
0       5235        0074        
1       0043        5265        

Accuracy 98.898
Time taken: 33.4 secs
3148.345 secs


On the best model
T\P      0       1
0       5276        0033        
1       0087        5221        

Accuracy 98.8697
Time taken: 33.29 secs


======================================================================
----------------------------------------------------------------------
                run_ID=109
Mon Apr  4 18:09:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.033 GB
27 seconds taken

{
    "run_ID": 109,
    "message": "108 with a different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.374 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 588.0 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 23.23 secs| Accuracy 97.711

Epoch: 2 Train Loss 0.071 Time taken: 587.64 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 23.54 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.052 Time taken: 587.1 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.52 secs| Accuracy 98.672

Epoch: 4 Train Loss 0.034 Time taken: 596.76 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.4 secs| Accuracy 99.11

Epoch: 5 Train Loss 0.023 Time taken: 587.29 secs

    Epoch: 5 Validation Loss 0.085 Time taken: 23.33 secs| Accuracy 97.853
T\P      0       1
0       5119        0190        
1       0028        5280        

Accuracy 97.9467
Time taken: 34.5 secs
3138.321 secs


On the best model
T\P      0       1
0       5281        0028        
1       0064        5244        

Accuracy 99.1335
Time taken: 34.19 secs


======================================================================
----------------------------------------------------------------------
                run_ID=110
Mon Apr  4 19:03:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
27 seconds taken

{
    "run_ID": 110,
    "message": "109 with cls",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.372 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.168 Time taken: 588.65 secs

    Epoch: 1 Validation Loss 0.05 Time taken: 23.47 secs| Accuracy 98.559

Epoch: 2 Train Loss 0.064 Time taken: 593.98 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 23.42 secs| Accuracy 98.771

Epoch: 3 Train Loss 0.043 Time taken: 587.04 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.48 secs| Accuracy 99.025

Epoch: 4 Train Loss 0.033 Time taken: 587.4 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 23.58 secs| Accuracy 99.124

Epoch: 5 Train Loss 0.025 Time taken: 587.24 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 23.53 secs| Accuracy 98.856
T\P      0       1
0       5253        0056        
1       0049        5259        

Accuracy 99.011
Time taken: 34.54 secs
3128.617 secs


On the best model
T\P      0       1
0       5260        0049        
1       0048        5260        

Accuracy 99.0864
Time taken: 34.47 secs


======================================================================
----------------------------------------------------------------------
                run_ID=111
Mon Apr  4 19:57:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.058 GB
29 seconds taken

{
    "run_ID": 111,
    "message": "109 with fc3",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.25 Time taken: 588.41 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 22.89 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.073 Time taken: 591.09 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.85 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.048 Time taken: 585.88 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.92 secs| Accuracy 98.841

Epoch: 4 Train Loss 0.035 Time taken: 585.76 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 23.01 secs| Accuracy 99.053

Epoch: 5 Train Loss 0.025 Time taken: 585.55 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.09 secs| Accuracy 98.997
T\P      0       1
0       5274        0035        
1       0059        5249        

Accuracy 99.1146
Time taken: 33.86 secs
3122.41 secs


On the best model
T\P      0       1
0       5282        0027        
1       0069        5239        

Accuracy 99.0958
Time taken: 33.63 secs


======================================================================
----------------------------------------------------------------------
                run_ID=112
Mon Apr  4 20:50:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
30 seconds taken

{
    "run_ID": 112,
    "message": "109 with fc1",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.405 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.165 Time taken: 588.87 secs

    Epoch: 1 Validation Loss 0.051 Time taken: 23.11 secs| Accuracy 98.644

Epoch: 2 Train Loss 0.058 Time taken: 593.41 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.48 secs| Accuracy 98.7

Epoch: 3 Train Loss 0.043 Time taken: 587.18 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 23.51 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.029 Time taken: 586.34 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 23.48 secs| Accuracy 99.025

Epoch: 5 Train Loss 0.022 Time taken: 586.45 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 23.5 secs| Accuracy 99.082
T\P      0       1
0       5260        0049        
1       0055        5253        

Accuracy 99.0204
Time taken: 34.52 secs
3123.864 secs


On the best model
T\P      0       1
0       5260        0049        
1       0055        5253        

Accuracy 99.0204
Time taken: 34.23 secs



======================================================================
----------------------------------------------------------------------
                run_ID=113
Tue Apr  5 12:13:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.069 GB
28 seconds taken

{
    "run_ID": 113,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 1.967 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 588.53 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.72 secs| Accuracy 98.404

Epoch: 2 Train Loss 0.078 Time taken: 585.45 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.81 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.055 Time taken: 586.25 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 22.78 secs| Accuracy 98.884

Epoch: 4 Train Loss 0.038 Time taken: 584.38 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 22.81 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.027 Time taken: 586.5 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 22.91 secs| Accuracy 99.011
T\P      0       1
0       5263        0046        
1       0048        5260        

Accuracy 99.1146
Time taken: 33.61 secs
3119.75 secs


On the best model
T\P      0       1
0       5244        0065        
1       0048        5260        

Accuracy 98.9357
Time taken: 33.52 secs
Tue Apr  5 13:07:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=114
Tue Apr  5 13:07:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.062 GB
27 seconds taken

{
    "run_ID": 114,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.354 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.174 Time taken: 588.99 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 22.98 secs| Accuracy 98.545

Epoch: 2 Train Loss 0.065 Time taken: 588.9 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.92 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.042 Time taken: 592.1 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.92 secs| Accuracy 98.757

Epoch: 4 Train Loss 0.029 Time taken: 586.47 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 23.01 secs| Accuracy 99.025

Epoch: 5 Train Loss 0.023 Time taken: 585.56 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 22.99 secs| Accuracy 98.686
T\P      0       1
0       5209        0100        
1       0039        5269        

Accuracy 98.6908
Time taken: 33.9 secs
3126.483 secs


On the best model
T\P      0       1
0       5258        0051        
1       0062        5246        

Accuracy 98.9357
Time taken: 33.72 secs
Tue Apr  5 14:01:05 2022



======================================================================
----------------------------------------------------------------------
                run_ID=115
Tue Apr  5 17:33:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.065 GB
28 seconds taken

{
    "run_ID": 115,
    "message": "112 with random off",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.372 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.394 Time taken: 592.65 secs

    Epoch: 1 Validation Loss 0.176 Time taken: 22.64 secs| Accuracy 93.105

Epoch: 2 Train Loss 0.151 Time taken: 592.47 secs

    Epoch: 2 Validation Loss 0.146 Time taken: 22.64 secs| Accuracy 95.013

Epoch: 3 Train Loss 0.097 Time taken: 595.29 secs

    Epoch: 3 Validation Loss 0.125 Time taken: 22.67 secs| Accuracy 95.903

Epoch: 4 Train Loss 0.066 Time taken: 588.75 secs

    Epoch: 4 Validation Loss 0.129 Time taken: 22.72 secs| Accuracy 95.889

Epoch: 5 Train Loss 0.047 Time taken: 595.02 secs

    Epoch: 5 Validation Loss 0.146 Time taken: 22.63 secs| Accuracy 95.535
T\P      0       1
0       5004        0305        
1       0157        5151        

Accuracy 95.6485
Time taken: 33.52 secs
3161.835 secs


On the best model
T\P      0       1
0       5044        0265        
1       0182        5126        

Accuracy 95.7898
Time taken: 33.33 secs
Tue Apr  5 18:27:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=116
Tue Apr  5 18:27:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.07 GB
28 seconds taken

{
    "run_ID": 116,
    "message": "115 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.366 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.4 Time taken: 596.67 secs

    Epoch: 1 Validation Loss 0.189 Time taken: 22.87 secs| Accuracy 93.077

Epoch: 2 Train Loss 0.144 Time taken: 588.11 secs

    Epoch: 2 Validation Loss 0.129 Time taken: 22.88 secs| Accuracy 95.309

Epoch: 3 Train Loss 0.099 Time taken: 593.17 secs

    Epoch: 3 Validation Loss 0.115 Time taken: 23.17 secs| Accuracy 95.648

Epoch: 4 Train Loss 0.068 Time taken: 587.49 secs

    Epoch: 4 Validation Loss 0.116 Time taken: 23.27 secs| Accuracy 95.988

Epoch: 5 Train Loss 0.051 Time taken: 589.01 secs

    Epoch: 5 Validation Loss 0.109 Time taken: 23.3 secs| Accuracy 96.256
T\P      0       1
0       5051        0258        
1       0148        5160        

Accuracy 96.1759
Time taken: 33.91 secs
3169.262 secs


On the best model
T\P      0       1
0       5051        0258        
1       0148        5160        

Accuracy 96.1759
Time taken: 33.78 secs
Tue Apr  5 19:22:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=117
Tue Apr  5 19:22:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.073 GB
33 seconds taken

{
    "run_ID": 117,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.366 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.368 Time taken: 595.55 secs

    Epoch: 1 Validation Loss 0.148 Time taken: 22.92 secs| Accuracy 94.278

Epoch: 2 Train Loss 0.144 Time taken: 587.75 secs

    Epoch: 2 Validation Loss 0.13 Time taken: 23.2 secs| Accuracy 94.928

Epoch: 3 Train Loss 0.094 Time taken: 594.77 secs

    Epoch: 3 Validation Loss 0.126 Time taken: 22.91 secs| Accuracy 95.225

Epoch: 4 Train Loss 0.066 Time taken: 586.92 secs

    Epoch: 4 Validation Loss 0.121 Time taken: 23.26 secs| Accuracy 95.776

Epoch: 5 Train Loss 0.045 Time taken: 587.21 secs

    Epoch: 5 Validation Loss 0.123 Time taken: 23.02 secs| Accuracy 95.86
T\P      0       1
0       4964        0345        
1       0101        5207        

Accuracy 95.7992
Time taken: 33.71 secs
3171.289 secs


On the best model
T\P      0       1
0       4964        0345        
1       0101        5207        

Accuracy 95.7992
Time taken: 33.55 secs
Tue Apr  5 20:17:04 2022



======================================================================
----------------------------------------------------------------------
                run_ID=118
Tue Apr  5 21:06:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.153 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(-1.)tensor(1.)tensor(-1.)tensor(1.)...


{
    "run_ID": 118,
    "message": "112 with Hinge loss",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.372 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss -0.338 Time taken: 585.6 secs

    Epoch: 1 Validation Loss -0.448 Time taken: 22.66 secs| Accuracy 0.834

Epoch: 2 Train Loss -0.412 Time taken: 586.28 secs

    Epoch: 2 Validation Loss -0.43 Time taken: 22.62 secs| Accuracy 0.791

Epoch: 3 Train Loss -0.403 Time taken: 586.03 secs

    Epoch: 3 Validation Loss -0.45 Time taken: 22.6 secs| Accuracy 0.932

Epoch: 4 Train Loss -0.401 Time taken: 585.5 secs

    Epoch: 4 Validation Loss -0.462 Time taken: 22.68 secs| Accuracy 1.399

Epoch: 5 Train Loss -0.382 Time taken: 587.87 secs

    Epoch: 5 Validation Loss -0.385 Time taken: 22.81 secs| Accuracy 0.339
T\P      0       1
0       0000        0553        4756        
1       0000        0000        0000        
2       0000        5253        0055        

Accuracy 0.518
Time taken: 33.53 secs
3116.387 secs


On the best model
T\P      0       1
0       0000        0061        5248        
1       0000        0000        0000        
2       0000        5172        0136        

Accuracy 1.281
Time taken: 33.26 secs
Tue Apr  5 21:59:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=119
Tue Apr  5 21:59:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.078 GB
29 seconds taken
Example: ... tensor(-1.)tensor(1.)tensor(-1.)tensor(-1.)tensor(1.)tensor(-1.)...


{
    "run_ID": 119,
    "message": "118 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 8.721 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss -0.329 Time taken: 589.71 secs

    Epoch: 1 Validation Loss -0.45 Time taken: 22.47 secs| Accuracy 0.989

Epoch: 2 Train Loss -0.413 Time taken: 585.36 secs

    Epoch: 2 Validation Loss -0.449 Time taken: 22.62 secs| Accuracy 1.992

Epoch: 3 Train Loss -0.411 Time taken: 585.34 secs

    Epoch: 3 Validation Loss -0.452 Time taken: 22.54 secs| Accuracy 1.003

Epoch: 4 Train Loss -0.391 Time taken: 584.37 secs

    Epoch: 4 Validation Loss -0.425 Time taken: 22.57 secs| Accuracy 0.735

Epoch: 5 Train Loss -0.4 Time taken: 585.0 secs

    Epoch: 5 Validation Loss -0.434 Time taken: 22.54 secs| Accuracy 0.918
T\P      0       1
0       0000        0218        5091        
1       0000        0000        0000        
2       0000        5210        0098        

Accuracy 0.923
Time taken: 33.46 secs
3098.627 secs


On the best model
T\P      0       1
0       0000        0051        5258        
1       0000        0000        0000        
2       0000        5104        0204        

Accuracy 1.9214
Time taken: 33.4 secs
Tue Apr  5 22:52:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=120
Tue Apr  5 22:52:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.073 GB
30 seconds taken
Example: ... tensor(-1.)tensor(1.)tensor(-1.)tensor(-1.)tensor(-1.)tensor(-1.)...


{
    "run_ID": 120,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.398 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss -0.295 Time taken: 586.63 secs

    Epoch: 1 Validation Loss -0.434 Time taken: 22.64 secs| Accuracy 2.091

Epoch: 2 Train Loss -0.389 Time taken: 586.57 secs

    Epoch: 2 Validation Loss -0.303 Time taken: 22.61 secs| Accuracy 0.494

Epoch: 3 Train Loss -0.139 Time taken: 584.52 secs

    Epoch: 3 Validation Loss -0.424 Time taken: 22.71 secs| Accuracy 0.862

Epoch: 4 Train Loss -0.352 Time taken: 594.03 secs

    Epoch: 4 Validation Loss -0.428 Time taken: 22.95 secs| Accuracy 1.498

Epoch: 5 Train Loss -0.399 Time taken: 590.19 secs

    Epoch: 5 Validation Loss -0.448 Time taken: 22.73 secs| Accuracy 1.201
T\P      0       1
0       0000        0144        5165        
1       0000        0000        0000        
2       0000        5184        0124        

Accuracy 1.1679
Time taken: 33.85 secs
3105.937 secs


On the best model
T\P      0       1
0       0000        0127        5182        
1       0000        0000        0000        
2       0000        5118        0190        

Accuracy 1.7896
Time taken: 33.61 secs
Tue Apr  5 23:46:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=121
Tue Apr  5 23:46:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.233 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 121,
    "message": "112 with fc2",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.448 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 616.14 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 22.79 secs| Accuracy 97.711

Epoch: 2 Train Loss 0.071 Time taken: 588.56 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 22.87 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.052 Time taken: 587.08 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.87 secs| Accuracy 98.672

Epoch: 4 Train Loss 0.034 Time taken: 594.53 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 22.86 secs| Accuracy 99.11

Epoch: 5 Train Loss 0.023 Time taken: 587.27 secs

    Epoch: 5 Validation Loss 0.085 Time taken: 22.94 secs| Accuracy 97.853
T\P      0       1
0       5119        0190        
1       0028        5280        

Accuracy 97.9467
Time taken: 33.75 secs
3157.824 secs


On the best model
T\P      0       1
0       5281        0028        
1       0064        5244        

Accuracy 99.1335
Time taken: 33.73 secs
Wed Apr  6 00:40:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=122
Wed Apr  6 00:40:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.114 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 122,
    "message": "121 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.38 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.198 Time taken: 588.39 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.69 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.075 Time taken: 584.95 secs

    Epoch: 2 Validation Loss 0.127 Time taken: 22.78 secs| Accuracy 96.228

Epoch: 3 Train Loss 0.049 Time taken: 592.52 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 22.72 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.035 Time taken: 585.12 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 22.79 secs| Accuracy 99.053

Epoch: 5 Train Loss 0.027 Time taken: 584.97 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 22.81 secs| Accuracy 99.138
T\P      0       1
0       5271        0038        
1       0047        5261        

Accuracy 99.1994
Time taken: 33.57 secs
3119.073 secs


On the best model
T\P      0       1
0       5271        0038        
1       0047        5261        

Accuracy 99.1994
Time taken: 33.47 secs
Wed Apr  6 01:34:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=123
Wed Apr  6 01:34:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.08 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 123,
    "message": "121 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.386 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.222 Time taken: 590.71 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 22.63 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.081 Time taken: 594.28 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.55 secs| Accuracy 98.785

Epoch: 3 Train Loss 0.05 Time taken: 588.4 secs

    Epoch: 3 Validation Loss 0.076 Time taken: 22.62 secs| Accuracy 97.485

Epoch: 4 Train Loss 0.036 Time taken: 586.1 secs

    Epoch: 4 Validation Loss 0.029 Time taken: 22.85 secs| Accuracy 99.181

Epoch: 5 Train Loss 0.024 Time taken: 585.13 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 22.65 secs| Accuracy 99.195
T\P      0       1
0       5247        0062        
1       0050        5258        

Accuracy 98.9451
Time taken: 33.36 secs
3129.574 secs


On the best model
T\P      0       1
0       5247        0062        
1       0050        5258        

Accuracy 98.9451
Time taken: 33.28 secs
Wed Apr  6 02:28:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=124
Wed Apr  6 02:28:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.066 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 124,
    "message": "112 with fc3",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.383 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.25 Time taken: 592.56 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 22.77 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.073 Time taken: 597.67 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.63 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.048 Time taken: 586.89 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.72 secs| Accuracy 98.841

Epoch: 4 Train Loss 0.035 Time taken: 587.15 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 22.7 secs| Accuracy 99.053

Epoch: 5 Train Loss 0.025 Time taken: 584.78 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 22.67 secs| Accuracy 98.997
T\P      0       1
0       5274        0035        
1       0059        5249        

Accuracy 99.1146
Time taken: 33.54 secs
3134.778 secs


On the best model
T\P      0       1
0       5282        0027        
1       0069        5239        

Accuracy 99.0958
Time taken: 33.36 secs
Wed Apr  6 03:22:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=125
Wed Apr  6 03:22:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.075 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 125,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.447 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.274 Time taken: 588.02 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.64 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.085 Time taken: 584.76 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.71 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.054 Time taken: 590.17 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 22.67 secs| Accuracy 99.096

Epoch: 4 Train Loss 0.041 Time taken: 589.61 secs

    Epoch: 4 Validation Loss 0.032 Time taken: 22.69 secs| Accuracy 99.11

Epoch: 5 Train Loss 0.03 Time taken: 584.68 secs

    Epoch: 5 Validation Loss 0.029 Time taken: 22.71 secs| Accuracy 99.166
T\P      0       1
0       5275        0034        
1       0058        5250        

Accuracy 99.1335
Time taken: 33.42 secs
3128.517 secs


On the best model
T\P      0       1
0       5275        0034        
1       0058        5250        

Accuracy 99.1335
Time taken: 33.32 secs
Wed Apr  6 04:16:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=126
Wed Apr  6 04:16:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 126,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 588.69 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.76 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.066 Time taken: 585.5 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.81 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.043 Time taken: 585.45 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.83 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.029 Time taken: 585.09 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 22.77 secs| Accuracy 98.884

Epoch: 5 Train Loss 0.026 Time taken: 584.69 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 22.77 secs| Accuracy 98.644
T\P      0       1
0       5172        0137        
1       0030        5278        

Accuracy 98.4271
Time taken: 33.53 secs
3107.573 secs


On the best model
T\P      0       1
0       5200        0109        
1       0033        5275        

Accuracy 98.6625
Time taken: 33.73 secs
Wed Apr  6 05:09:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=127
Wed Apr  6 05:09:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 127,
    "message": "112 with BCEwithLogits",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.35 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.168 Time taken: 589.41 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 22.75 secs| Accuracy 98.573

Epoch: 2 Train Loss 0.059 Time taken: 587.25 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.91 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.04 Time taken: 587.64 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 22.8 secs| Accuracy 98.177

Epoch: 4 Train Loss 0.03 Time taken: 586.79 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 22.86 secs| Accuracy 99.11

Epoch: 5 Train Loss 0.023 Time taken: 585.6 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 22.68 secs| Accuracy 99.053
T\P      0       1
0       5264        0045        
1       0080        5228        

Accuracy 98.8226
Time taken: 33.5 secs
3116.496 secs


On the best model
T\P      0       1
0       5274        0035        
1       0058        5250        

Accuracy 99.124
Time taken: 33.47 secs
Wed Apr  6 06:03:05 2022


======================================================================
----------------------------------------------------------------------
                run_ID=128
Wed Apr  6 06:03:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 128,
    "message": "127 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.359 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.221 Time taken: 588.16 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.74 secs| Accuracy 98.262

Epoch: 2 Train Loss 0.08 Time taken: 585.65 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.71 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.054 Time taken: 592.32 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 22.74 secs| Accuracy 98.898

Epoch: 4 Train Loss 0.039 Time taken: 584.81 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 22.87 secs| Accuracy 99.124

Epoch: 5 Train Loss 0.029 Time taken: 594.39 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 22.67 secs| Accuracy 98.912
T\P      0       1
0       5247        0062        
1       0043        5265        

Accuracy 99.011
Time taken: 33.65 secs
3133.465 secs


On the best model
T\P      0       1
0       5257        0052        
1       0054        5254        

Accuracy 99.0016
Time taken: 33.58 secs
Wed Apr  6 06:56:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=129
Wed Apr  6 06:57:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.044 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 129,
    "message": "127 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.406 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.173 Time taken: 589.46 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 22.76 secs| Accuracy 98.601

Epoch: 2 Train Loss 0.07 Time taken: 591.24 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 22.78 secs| Accuracy 98.785

Epoch: 3 Train Loss 0.044 Time taken: 592.78 secs

    Epoch: 3 Validation Loss 0.034 Time taken: 22.72 secs| Accuracy 99.039

Epoch: 4 Train Loss 0.03 Time taken: 585.33 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 22.76 secs| Accuracy 98.997

Epoch: 5 Train Loss 0.023 Time taken: 584.8 secs

    Epoch: 5 Validation Loss 0.031 Time taken: 22.74 secs| Accuracy 99.11
T\P      0       1
0       5269        0040        
1       0058        5250        

Accuracy 99.077
Time taken: 33.59 secs
3132.489 secs


On the best model
T\P      0       1
0       5269        0040        
1       0058        5250        

Accuracy 99.077
Time taken: 33.49 secs
Wed Apr  6 07:50:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=130
Wed Apr  6 07:50:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.046 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 130,
    "message": "112 with CLS",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.358 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 590.24 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.02 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.062 Time taken: 587.86 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.02 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.039 Time taken: 585.55 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 22.89 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.029 Time taken: 585.11 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 22.92 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.02 Time taken: 586.25 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 22.84 secs| Accuracy 99.082
T\P      0       1
0       5245        0064        
1       0049        5259        

Accuracy 98.9357
Time taken: 33.59 secs
3126.859 secs


On the best model
T\P      0       1
0       5245        0064        
1       0049        5259        

Accuracy 98.9357
Time taken: 33.52 secs
Wed Apr  6 08:44:39 2022



======================================================================
----------------------------------------------------------------------
                run_ID=131
Wed Apr  6 11:11:07 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.064 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(-1.)tensor(1.)tensor(-1.)tensor(1.)...


{
    "run_ID": 131,
    "message": "112 with Hinge loss [fixed] [118 had some problems] [in this one I added 'out=1-out' in forward function]",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.367 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.082 Time taken: 589.23 secs

    Epoch: 1 Validation Loss 0.022 Time taken: 23.01 secs| Accuracy 50.014 # validation ke code me ek chota sa bug tha, I fixed it in run_ID 133

Epoch: 2 Train Loss 0.04 Time taken: 586.62 secs

    Epoch: 2 Validation Loss 0.022 Time taken: 23.11 secs| Accuracy 50.014

Epoch: 3 Train Loss 0.036 Time taken: 596.1 secs

    Epoch: 3 Validation Loss 0.029 Time taken: 23.0 secs| Accuracy 50.014

Epoch: 4 Train Loss 0.033 Time taken: 594.52 secs

    Epoch: 4 Validation Loss 0.027 Time taken: 22.94 secs| Accuracy 50.014

Epoch: 5 Train Loss 0.033 Time taken: 586.69 secs

    Epoch: 5 Validation Loss 0.028 Time taken: 23.07 secs| Accuracy 50.014
T\P      0       1
0       5100        0209        
1       0065        5243        

Accuracy 97.4192
Time taken: 33.92 secs
3119.728 secs


On the best model
T\P      0       1
0       5144        0165        
1       0070        5238        

Accuracy 97.7866
Time taken: 33.7 secs
Wed Apr  6 12:04:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=132
Wed Apr  6 12:04:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.061 GB
28 seconds taken
Example: ... tensor(-1.)tensor(1.)tensor(-1.)tensor(-1.)tensor(1.)tensor(-1.)...


{
    "run_ID": 132,
    "message": "131 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.357 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.094 Time taken: 588.41 secs

    Epoch: 1 Validation Loss 0.022 Time taken: 22.77 secs| Accuracy 50.014

Epoch: 2 Train Loss 0.036 Time taken: 592.84 secs

    Epoch: 2 Validation Loss 0.02 Time taken: 22.67 secs| Accuracy 50.014

Epoch: 3 Train Loss 0.033 Time taken: 591.75 secs

    Epoch: 3 Validation Loss 0.016 Time taken: 22.74 secs| Accuracy 50.014

Epoch: 4 Train Loss 0.026 Time taken: 585.96 secs

    Epoch: 4 Validation Loss 0.02 Time taken: 22.82 secs| Accuracy 50.014

Epoch: 5 Train Loss 0.029 Time taken: 587.46 secs

    Epoch: 5 Validation Loss 0.019 Time taken: 22.84 secs| Accuracy 50.014
T\P      0       1
0       5252        0057        
1       0111        5197        

Accuracy 98.4176
Time taken: 33.72 secs
3110.153 secs


On the best model
T\P      0       1
0       5236        0073        
1       0119        5189        

Accuracy 98.1916
Time taken: 33.53 secs
Wed Apr  6 12:58:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=133
Wed Apr  6 12:58:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.033 GB
30 seconds taken
Example: ... tensor(-1.)tensor(1.)tensor(-1.)tensor(-1.)tensor(-1.)tensor(-1.)...


{
    "run_ID": 133,
    "message": "131 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.361 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.085 Time taken: 590.22 secs

    Epoch: 1 Validation Loss 0.023 Time taken: 22.91 secs| Accuracy 97.824

Epoch: 2 Train Loss 0.046 Time taken: 587.27 secs

    Epoch: 2 Validation Loss 0.024 Time taken: 23.01 secs| Accuracy 96.977

Epoch: 3 Train Loss 0.036 Time taken: 587.48 secs

    Epoch: 3 Validation Loss 0.017 Time taken: 22.96 secs| Accuracy 98.319

Epoch: 4 Train Loss 0.045 Time taken: 587.2 secs

    Epoch: 4 Validation Loss 0.023 Time taken: 23.05 secs| Accuracy 97.683

Epoch: 5 Train Loss 0.039 Time taken: 587.73 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.02 secs| Accuracy 95.592
T\P      0       1
0       4928        0381        
1       0048        5260        

Accuracy 95.9593
Time taken: 33.8 secs
3111.7 secs


On the best model
T\P      0       1
0       5239        0070        
1       0101        5207        

Accuracy 98.3894
Time taken: 33.63 secs
Wed Apr  6 13:51:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=134
Wed Apr  6 14:25:45 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 134,
    "message": "130 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.362 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.15 Time taken: 589.29 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 22.93 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.069 Time taken: 587.46 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 22.92 secs| Accuracy 99.025

Epoch: 3 Train Loss 0.043 Time taken: 587.12 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.04 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.03 Time taken: 588.04 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.05 secs| Accuracy 98.686

Epoch: 5 Train Loss 0.021 Time taken: 587.43 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.01 secs| Accuracy 99.138
T\P      0       1
0       5264        0045        
1       0044        5264        

Accuracy 99.1617
Time taken: 33.76 secs
3120.912 secs


On the best model
T\P      0       1
0       5264        0045        
1       0044        5264        

Accuracy 99.1617
Time taken: 33.91 secs
Wed Apr  6 15:19:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=135
Wed Apr  6 15:19:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 135,
    "message": "130 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.365 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.162 Time taken: 591.12 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 22.88 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.078 Time taken: 598.07 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 22.82 secs| Accuracy 98.644

Epoch: 3 Train Loss 0.053 Time taken: 593.44 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 22.81 secs| Accuracy 98.87

Epoch: 4 Train Loss 0.037 Time taken: 594.58 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 22.96 secs| Accuracy 99.11

Epoch: 5 Train Loss 0.03 Time taken: 587.65 secs

    Epoch: 5 Validation Loss 0.031 Time taken: 22.83 secs| Accuracy 99.223
T\P      0       1
0       5261        0048        
1       0042        5266        

Accuracy 99.1523
Time taken: 33.54 secs
3160.313 secs


On the best model
T\P      0       1
0       5261        0048        
1       0042        5266        

Accuracy 99.1523
Time taken: 33.44 secs
Wed Apr  6 16:13:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=136
Wed Apr  6 16:13:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.044 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 136,
    "message": "121 with random sents off",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.369 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.456 Time taken: 593.28 secs

    Epoch: 1 Validation Loss 0.209 Time taken: 22.77 secs| Accuracy 91.947

Epoch: 2 Train Loss 0.171 Time taken: 588.58 secs

    Epoch: 2 Validation Loss 0.14 Time taken: 22.81 secs| Accuracy 94.744

Epoch: 3 Train Loss 0.108 Time taken: 589.07 secs

    Epoch: 3 Validation Loss 0.12 Time taken: 22.8 secs| Accuracy 95.973

Epoch: 4 Train Loss 0.069 Time taken: 587.29 secs

    Epoch: 4 Validation Loss 0.133 Time taken: 22.79 secs| Accuracy 95.479

Epoch: 5 Train Loss 0.048 Time taken: 587.91 secs

    Epoch: 5 Validation Loss 0.127 Time taken: 22.8 secs| Accuracy 96.185
T\P      0       1
0       5056        0253        
1       0179        5129        

Accuracy 95.9311
Time taken: 33.56 secs
3129.383 secs


On the best model
T\P      0       1
0       5056        0253        
1       0179        5129        

Accuracy 95.9311
Time taken: 33.44 secs
Wed Apr  6 17:07:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=137
Wed Apr  6 17:07:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.193 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 137,
    "message": "136 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.69 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.411 Time taken: 590.11 secs

    Epoch: 1 Validation Loss 0.192 Time taken: 22.71 secs| Accuracy 92.54

Epoch: 2 Train Loss 0.152 Time taken: 590.53 secs

    Epoch: 2 Validation Loss 0.203 Time taken: 22.79 secs| Accuracy 92.724

Epoch: 3 Train Loss 0.1 Time taken: 587.6 secs

    Epoch: 3 Validation Loss 0.135 Time taken: 22.78 secs| Accuracy 95.324

Epoch: 4 Train Loss 0.071 Time taken: 586.84 secs

    Epoch: 4 Validation Loss 0.169 Time taken: 22.78 secs| Accuracy 94.123

Epoch: 5 Train Loss 0.05 Time taken: 586.93 secs

    Epoch: 5 Validation Loss 0.129 Time taken: 22.73 secs| Accuracy 96.058
T\P      0       1
0       5117        0192        
1       0197        5111        

Accuracy 96.3361
Time taken: 33.73 secs
3127.551 secs


On the best model
T\P      0       1
0       5117        0192        
1       0197        5111        

Accuracy 96.3361
Time taken: 33.58 secs
Wed Apr  6 18:01:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=138
Wed Apr  6 18:01:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 138,
    "message": "136 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.361 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.415 Time taken: 589.96 secs

    Epoch: 1 Validation Loss 0.2 Time taken: 22.83 secs| Accuracy 92.441

Epoch: 2 Train Loss 0.162 Time taken: 595.52 secs

    Epoch: 2 Validation Loss 0.151 Time taken: 22.84 secs| Accuracy 94.236

Epoch: 3 Train Loss 0.103 Time taken: 594.75 secs

    Epoch: 3 Validation Loss 0.116 Time taken: 22.86 secs| Accuracy 95.889

Epoch: 4 Train Loss 0.07 Time taken: 588.42 secs

    Epoch: 4 Validation Loss 0.244 Time taken: 22.98 secs| Accuracy 91.622

Epoch: 5 Train Loss 0.051 Time taken: 587.87 secs

    Epoch: 5 Validation Loss 0.117 Time taken: 22.95 secs| Accuracy 95.959
T\P      0       1
0       5168        0141        
1       0295        5013        

Accuracy 95.8934
Time taken: 33.78 secs
3143.817 secs


On the best model
T\P      0       1
0       5168        0141        
1       0295        5013        

Accuracy 95.8934
Time taken: 33.7 secs
Wed Apr  6 18:55:16 2022




======================================================================
----------------------------------------------------------------------
                run_ID=139
Wed Apr  6 20:06:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.058 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 139,
    "message": "112 with query_expansion",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": true,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.358 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.165 Time taken: 594.38 secs

    Epoch: 1 Validation Loss 0.051 Time taken: 22.85 secs| Accuracy 98.644

Epoch: 2 Train Loss 0.058 Time taken: 598.0 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.89 secs| Accuracy 98.7

Epoch: 3 Train Loss 0.043 Time taken: 590.05 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 22.81 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.029 Time taken: 588.27 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 22.82 secs| Accuracy 99.025

Epoch: 5 Train Loss 0.022 Time taken: 587.9 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 22.84 secs| Accuracy 99.082
T\P      0       1
0       5260        0049        
1       0055        5253        

Accuracy 99.0204
Time taken: 33.52 secs
3146.932 secs


On the best model
T\P      0       1
0       5260        0049        
1       0055        5253        

Accuracy 99.0204
Time taken: 33.46 secs
Wed Apr  6 21:00:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=140
Wed Apr  6 21:00:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.075 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 140,
    "message": "139 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": true,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.364 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 590.3 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.69 secs| Accuracy 98.404

Epoch: 2 Train Loss 0.078 Time taken: 587.82 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.79 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.055 Time taken: 586.75 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 22.78 secs| Accuracy 98.884

Epoch: 4 Train Loss 0.038 Time taken: 588.59 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 22.84 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.027 Time taken: 589.81 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 22.75 secs| Accuracy 99.011
T\P      0       1
0       5263        0046        
1       0048        5260        

Accuracy 99.1146
Time taken: 33.79 secs
3133.581 secs


On the best model
T\P      0       1
0       5244        0065        
1       0048        5260        

Accuracy 98.9357
Time taken: 33.43 secs
Wed Apr  6 21:54:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=141
Wed Apr  6 21:54:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.058 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 141,
    "message": "139 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": true,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.365 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.174 Time taken: 606.84 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 23.16 secs| Accuracy 98.545

Epoch: 2 Train Loss 0.065 Time taken: 628.3 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.77 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.042 Time taken: 603.06 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.94 secs| Accuracy 98.757

Epoch: 4 Train Loss 0.029 Time taken: 613.97 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 23.42 secs| Accuracy 99.025

Epoch: 5 Train Loss 0.023 Time taken: 611.19 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 23.08 secs| Accuracy 98.686
T\P      0       1
0       5209        0100        
1       0039        5269        

Accuracy 98.6908
Time taken: 33.94 secs
3255.873 secs


On the best model
T\P      0       1
0       5258        0051        
1       0062        5246        

Accuracy 98.9357
Time taken: 33.86 secs
Wed Apr  6 22:50:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=142
Wed Apr  6 22:50:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.104 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 142,
    "message": "112 with indic bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.125 GB
Memory taken on RAM 4.159 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.235 Time taken: 611.46 secs

    Epoch: 1 Validation Loss 0.101 Time taken: 24.07 secs| Accuracy 96.68

Epoch: 2 Train Loss 0.101 Time taken: 580.41 secs

    Epoch: 2 Validation Loss 0.082 Time taken: 23.93 secs| Accuracy 97.386

Epoch: 3 Train Loss 0.067 Time taken: 616.25 secs

    Epoch: 3 Validation Loss 0.078 Time taken: 23.92 secs| Accuracy 97.485

Epoch: 4 Train Loss 0.042 Time taken: 574.36 secs

    Epoch: 4 Validation Loss 0.081 Time taken: 23.02 secs| Accuracy 97.711

Epoch: 5 Train Loss 0.027 Time taken: 570.6 secs

    Epoch: 5 Validation Loss 0.083 Time taken: 22.94 secs| Accuracy 97.584
T\P      0       1
0       5163        0146        
1       0092        5216        

Accuracy 97.7583
Time taken: 33.95 secs
3110.241 secs


On the best model
T\P      0       1
0       5253        0056        
1       0190        5118        

Accuracy 97.683
Time taken: 33.81 secs
Wed Apr  6 23:44:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=143
Wed Apr  6 23:44:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.101 GB
34 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 143,
    "message": "142 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.125 GB
Memory taken on RAM 4.161 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.234 Time taken: 569.65 secs

    Epoch: 1 Validation Loss 0.094 Time taken: 22.8 secs| Accuracy 96.906

Epoch: 2 Train Loss 0.091 Time taken: 570.46 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 22.83 secs| Accuracy 97.499

Epoch: 3 Train Loss 0.063 Time taken: 569.15 secs

    Epoch: 3 Validation Loss 0.071 Time taken: 22.8 secs| Accuracy 97.598

Epoch: 4 Train Loss 0.041 Time taken: 570.14 secs

    Epoch: 4 Validation Loss 0.077 Time taken: 22.89 secs| Accuracy 97.725

Epoch: 5 Train Loss 0.028 Time taken: 570.08 secs

    Epoch: 5 Validation Loss 0.084 Time taken: 22.93 secs| Accuracy 97.796
T\P      0       1
0       5190        0119        
1       0103        5205        

Accuracy 97.909
Time taken: 33.78 secs
3002.694 secs


On the best model
T\P      0       1
0       5190        0119        
1       0103        5205        

Accuracy 97.909
Time taken: 33.76 secs
Thu Apr  7 00:36:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=144
Thu Apr  7 00:36:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.097 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 144,
    "message": "142 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.125 GB
Memory taken on RAM 4.149 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.252 Time taken: 570.88 secs

    Epoch: 1 Validation Loss 0.098 Time taken: 22.83 secs| Accuracy 96.595

Epoch: 2 Train Loss 0.102 Time taken: 570.87 secs

    Epoch: 2 Validation Loss 0.096 Time taken: 22.88 secs| Accuracy 97.104

Epoch: 3 Train Loss 0.073 Time taken: 569.74 secs

    Epoch: 3 Validation Loss 0.078 Time taken: 22.86 secs| Accuracy 97.386

Epoch: 4 Train Loss 0.05 Time taken: 570.13 secs

    Epoch: 4 Validation Loss 0.083 Time taken: 22.85 secs| Accuracy 97.655

Epoch: 5 Train Loss 0.032 Time taken: 570.29 secs

    Epoch: 5 Validation Loss 0.091 Time taken: 22.85 secs| Accuracy 97.739
T\P      0       1
0       5198        0111        
1       0121        5187        

Accuracy 97.8148
Time taken: 33.86 secs
3004.932 secs


On the best model
T\P      0       1
0       5198        0111        
1       0121        5187        

Accuracy 97.8148
Time taken: 33.84 secs
Thu Apr  7 01:27:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=145
Thu Apr  7 01:27:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.86 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 145,
    "message": "112 with mbert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.988 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.131 Time taken: 583.06 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 22.68 secs| Accuracy 98.107

Epoch: 2 Train Loss 0.048 Time taken: 580.86 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 22.62 secs| Accuracy 98.276

Epoch: 3 Train Loss 0.024 Time taken: 580.05 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.72 secs| Accuracy 98.361

Epoch: 4 Train Loss 0.015 Time taken: 579.97 secs

    Epoch: 4 Validation Loss 0.064 Time taken: 22.64 secs| Accuracy 98.545

Epoch: 5 Train Loss 0.01 Time taken: 579.61 secs

    Epoch: 5 Validation Loss 0.057 Time taken: 22.6 secs| Accuracy 98.418
T\P      0       1
0       5243        0066        
1       0075        5233        

Accuracy 98.6719
Time taken: 33.64 secs
3076.817 secs


On the best model
T\P      0       1
0       5282        0027        
1       0101        5207        

Accuracy 98.7944
Time taken: 33.39 secs
Thu Apr  7 02:20:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=146
Thu Apr  7 02:20:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.861 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 146,
    "message": "145 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.99 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.139 Time taken: 584.62 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.04 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.053 Time taken: 581.68 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.05 secs| Accuracy 98.474

Epoch: 3 Train Loss 0.027 Time taken: 582.59 secs

    Epoch: 3 Validation Loss 0.09 Time taken: 23.13 secs| Accuracy 97.754

Epoch: 4 Train Loss 0.017 Time taken: 581.08 secs

    Epoch: 4 Validation Loss 0.068 Time taken: 23.01 secs| Accuracy 98.545

Epoch: 5 Train Loss 0.01 Time taken: 585.26 secs

    Epoch: 5 Validation Loss 0.077 Time taken: 23.02 secs| Accuracy 98.474
T\P      0       1
0       5183        0126        
1       0043        5265        

Accuracy 98.4082
Time taken: 33.87 secs
3083.279 secs


On the best model
T\P      0       1
0       5224        0085        
1       0051        5257        

Accuracy 98.719
Time taken: 34.02 secs
Thu Apr  7 03:13:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=147
Thu Apr  7 03:13:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.874 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 147,
    "message": "145 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 5.428 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.158 Time taken: 585.48 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.09 secs| Accuracy 97.641

Epoch: 2 Train Loss 0.056 Time taken: 587.91 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.31 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.028 Time taken: 589.03 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.42 secs| Accuracy 98.418

Epoch: 4 Train Loss 0.021 Time taken: 582.82 secs

    Epoch: 4 Validation Loss 0.077 Time taken: 23.36 secs| Accuracy 98.093

Epoch: 5 Train Loss 0.012 Time taken: 589.6 secs

    Epoch: 5 Validation Loss 0.059 Time taken: 23.29 secs| Accuracy 98.714
T\P      0       1
0       5230        0079        
1       0063        5245        

Accuracy 98.6625
Time taken: 34.12 secs
3108.895 secs


On the best model
T\P      0       1
0       5230        0079        
1       0063        5245        

Accuracy 98.6625
Time taken: 34.21 secs
Thu Apr  7 04:07:05 2022



======================================================================
----------------------------------------------------------------------
                run_ID=148
Thu Apr  7 12:40:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.058 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 148,
    "message": "112 with SequenceClassification",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "SequenceClassification",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )         )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=1, bias=True)
    )
  )
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.367 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.077 Time taken: 596.61 secs

    Epoch: 1 Validation Loss 0.018 Time taken: 22.97 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.031 Time taken: 594.78 secs

    Epoch: 2 Validation Loss 0.021 Time taken: 23.12 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.022 Time taken: 601.13 secs

    Epoch: 3 Validation Loss 0.022 Time taken: 23.0 secs| Accuracy 97.598

Epoch: 4 Train Loss 0.019 Time taken: 593.06 secs

    Epoch: 4 Validation Loss 0.015 Time taken: 23.07 secs| Accuracy 98.248

Epoch: 5 Train Loss 0.015 Time taken: 593.09 secs

    Epoch: 5 Validation Loss 0.016 Time taken: 23.04 secs| Accuracy 98.714
T\P      0       1
0       5232        0077        
1       0048        5260        

Accuracy 98.8226
Time taken: 33.81 secs
3164.743 secs


On the best model
T\P      0       1
0       5232        0077        
1       0048        5260        

Accuracy 98.8226
Time taken: 33.73 secs
Thu Apr  7 13:34:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=149
Thu Apr  7 13:34:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.046 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 149,
    "message": "148 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "SequenceClassification",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )         )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=1, bias=True)
    )
  )
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.362 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.074 Time taken: 594.02 secs

    Epoch: 1 Validation Loss 0.015 Time taken: 22.78 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.026 Time taken: 597.3 secs

    Epoch: 2 Validation Loss 0.012 Time taken: 22.8 secs| Accuracy 98.644

Epoch: 3 Train Loss 0.02 Time taken: 590.48 secs

    Epoch: 3 Validation Loss 0.011 Time taken: 22.99 secs| Accuracy 98.743

Epoch: 4 Train Loss 0.016 Time taken: 591.02 secs

    Epoch: 4 Validation Loss 0.009 Time taken: 22.95 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.013 Time taken: 590.68 secs

    Epoch: 5 Validation Loss 0.009 Time taken: 22.79 secs| Accuracy 98.884
T\P      0       1
0       5272        0037        
1       0057        5251        

Accuracy 99.1146
Time taken: 33.73 secs
3151.943 secs


On the best model
T\P      0       1
0       5276        0033        
1       0064        5244        

Accuracy 99.0864
Time taken: 33.62 secs
Thu Apr  7 14:28:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=150
Thu Apr  7 14:28:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.309 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 150,
    "message": "148 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": false,
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "SequenceClassification",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )         )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=1, bias=True)
    )
  )
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.493 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.061 Time taken: 594.5 secs

    Epoch: 1 Validation Loss 0.014 Time taken: 22.7 secs| Accuracy 98.107

Epoch: 2 Train Loss 0.025 Time taken: 600.32 secs

    Epoch: 2 Validation Loss 0.014 Time taken: 22.85 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.019 Time taken: 589.44 secs

    Epoch: 3 Validation Loss 0.009 Time taken: 22.84 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.015 Time taken: 590.62 secs

    Epoch: 4 Validation Loss 0.013 Time taken: 22.9 secs| Accuracy 98.375

Epoch: 5 Train Loss 0.013 Time taken: 592.08 secs

    Epoch: 5 Validation Loss 0.009 Time taken: 22.91 secs| Accuracy 99.082
T\P      0       1
0       5246        0063        
1       0054        5254        

Accuracy 98.898
Time taken: 33.62 secs
3157.119 secs


On the best model
T\P      0       1
0       5246        0063        
1       0054        5254        

Accuracy 98.898
Time taken: 33.49 secs
Thu Apr  7 15:23:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=151
Fri Apr  8 18:11:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 151,
    "message": "112 with first half BERT frozen for first two epochs",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "151",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.363 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 532.72 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 22.53 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.081 Time taken: 528.55 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.61 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.056 Time taken: 589.48 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.64 secs| Accuracy 98.587

Epoch: 4 Train Loss 0.035 Time taken: 588.79 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 22.64 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.024 Time taken: 590.32 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 22.77 secs| Accuracy 99.068
T\P      0       1
0       5295        0014        
1       0072        5236        

Accuracy 99.19
Time taken: 33.5 secs
3014.263 secs


On the best model
T\P      0       1
0       5295        0014        
1       0072        5236        

Accuracy 99.19
Time taken: 33.41 secs
Fri Apr  8 19:03:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=152
Fri Apr  8 19:03:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.064 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 152,
    "message": "151 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "151",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.366 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.225 Time taken: 535.0 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 22.9 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.083 Time taken: 531.44 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 23.09 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.058 Time taken: 589.49 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.04 secs| Accuracy 98.771

Epoch: 4 Train Loss 0.037 Time taken: 589.15 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.08 secs| Accuracy 98.743

Epoch: 5 Train Loss 0.026 Time taken: 589.18 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 22.97 secs| Accuracy 98.658
T\P      0       1
0       5191        0118        
1       0026        5282        

Accuracy 98.6437
Time taken: 33.81 secs
3014.649 secs


On the best model
T\P      0       1
0       5218        0091        
1       0031        5277        

Accuracy 98.8509
Time taken: 33.74 secs
Fri Apr  8 19:55:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=153
Fri Apr  8 19:55:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.059 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 153,
    "message": "151 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "151",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.408 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 532.61 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 23.22 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.084 Time taken: 534.7 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.25 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.055 Time taken: 592.59 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.5 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.035 Time taken: 590.61 secs

    Epoch: 4 Validation Loss 0.032 Time taken: 23.13 secs| Accuracy 99.251

Epoch: 5 Train Loss 0.023 Time taken: 589.48 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.1 secs| Accuracy 99.039
T\P      0       1
0       5249        0060        
1       0053        5255        

Accuracy 98.9357
Time taken: 33.92 secs
3019.162 secs


On the best model
T\P      0       1
0       5277        0032        
1       0056        5252        

Accuracy 99.1711
Time taken: 33.72 secs
Fri Apr  8 20:47:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=154
Fri Apr  8 20:47:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.058 GB
33 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 154,
    "message": "112 with second half BERT frozen for first two epochs",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "154",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.406 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.363 Time taken: 573.88 secs

    Epoch: 1 Validation Loss 0.122 Time taken: 23.37 secs| Accuracy 97.443

Epoch: 2 Train Loss 0.184 Time taken: 577.2 secs

    Epoch: 2 Validation Loss 0.084 Time taken: 23.35 secs| Accuracy 97.838

Epoch: 3 Train Loss 0.083 Time taken: 596.53 secs

    Epoch: 3 Validation Loss 0.077 Time taken: 23.46 secs| Accuracy 97.725

Epoch: 4 Train Loss 0.053 Time taken: 589.73 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.41 secs| Accuracy 98.686

Epoch: 5 Train Loss 0.036 Time taken: 596.02 secs

    Epoch: 5 Validation Loss 0.047 Time taken: 23.13 secs| Accuracy 98.841
T\P      0       1
0       5262        0047        
1       0069        5239        

Accuracy 98.9074
Time taken: 33.98 secs
3118.426 secs


On the best model
T\P      0       1
0       5262        0047        
1       0069        5239        

Accuracy 98.9074
Time taken: 33.93 secs
Fri Apr  8 21:40:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=155
Fri Apr  8 21:41:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 155,
    "message": "154 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "154",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.372 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.479 Time taken: 574.54 secs

    Epoch: 1 Validation Loss 0.242 Time taken: 23.42 secs| Accuracy 96.92

Epoch: 2 Train Loss 0.295 Time taken: 570.55 secs

    Epoch: 2 Validation Loss 0.177 Time taken: 23.36 secs| Accuracy 97.174

Epoch: 3 Train Loss 0.155 Time taken: 594.92 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 23.17 secs| Accuracy 97.909

Epoch: 4 Train Loss 0.083 Time taken: 593.44 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 23.12 secs| Accuracy 98.531

Epoch: 5 Train Loss 0.061 Time taken: 593.14 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 23.15 secs| Accuracy 98.771
T\P      0       1
0       5237        0072        
1       0068        5240        

Accuracy 98.6814
Time taken: 33.93 secs
3115.088 secs


On the best model
T\P      0       1
0       5237        0072        
1       0068        5240        

Accuracy 98.6814
Time taken: 33.99 secs
Fri Apr  8 22:34:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=156
Fri Apr  8 22:34:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.06 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 156,
    "message": "154 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "154",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.358 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.357 Time taken: 572.44 secs

    Epoch: 1 Validation Loss 0.117 Time taken: 23.44 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.173 Time taken: 567.89 secs

    Epoch: 2 Validation Loss 0.081 Time taken: 23.3 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.083 Time taken: 588.94 secs

    Epoch: 3 Validation Loss 0.069 Time taken: 23.2 secs| Accuracy 98.262

Epoch: 4 Train Loss 0.049 Time taken: 595.12 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 23.18 secs| Accuracy 98.799

Epoch: 5 Train Loss 0.038 Time taken: 589.4 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 23.58 secs| Accuracy 98.841
T\P      0       1
0       5263        0046        
1       0083        5225        

Accuracy 98.785
Time taken: 34.31 secs
3107.838 secs


On the best model
T\P      0       1
0       5263        0046        
1       0083        5225        

Accuracy 98.785
Time taken: 34.26 secs
Fri Apr  8 23:28:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=157
Fri Apr  8 23:49:07 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.062 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 157,
    "message": "112 with first half BERT frozen for first two epochs, then second half BERT frozen for next 2 epochs, then nothing frozen for the last epoch",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "157",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.368 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 532.19 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 22.96 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.081 Time taken: 529.72 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 23.08 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.053 Time taken: 573.73 secs

    Epoch: 3 Validation Loss 0.032 Time taken: 23.06 secs| Accuracy 99.053

Epoch: 4 Train Loss 0.04 Time taken: 573.07 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 23.1 secs| Accuracy 99.053

Epoch: 5 Train Loss 0.041 Time taken: 588.62 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.05 secs| Accuracy 99.181
T\P      0       1
0       5260        0049        
1       0044        5264        

Accuracy 99.124
Time taken: 33.86 secs
2977.638 secs


On the best model
T\P      0       1
0       5260        0049        
1       0044        5264        

Accuracy 99.124
Time taken: 33.86 secs
Sat Apr  9 00:40:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=158
Sat Apr  9 00:40:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 158,
    "message": "157 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "157",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.367 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.225 Time taken: 533.67 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.27 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.083 Time taken: 530.97 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 23.26 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.054 Time taken: 573.55 secs

    Epoch: 3 Validation Loss 0.032 Time taken: 23.41 secs| Accuracy 99.039

Epoch: 4 Train Loss 0.037 Time taken: 573.4 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 23.38 secs| Accuracy 98.94

Epoch: 5 Train Loss 0.038 Time taken: 590.81 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 23.75 secs| Accuracy 98.644
T\P      0       1
0       5219        0090        
1       0035        5273        

Accuracy 98.8226
Time taken: 34.24 secs
2983.184 secs


On the best model
T\P      0       1
0       5273        0036        
1       0051        5257        

Accuracy 99.1806
Time taken: 34.41 secs
Sat Apr  9 01:31:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=159
Sat Apr  9 01:31:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 159,
    "message": "157 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "157",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.359 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 533.12 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 23.23 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.084 Time taken: 536.49 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.16 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.052 Time taken: 574.11 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.32 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.033 Time taken: 573.7 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 23.3 secs| Accuracy 98.969

Epoch: 5 Train Loss 0.035 Time taken: 599.48 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.16 secs| Accuracy 99.138
T\P      0       1
0       5254        0055        
1       0044        5264        

Accuracy 99.0675
Time taken: 34.02 secs
3001.462 secs


On the best model
T\P      0       1
0       5254        0055        
1       0044        5264        

Accuracy 99.0675
Time taken: 34.11 secs
Sat Apr  9 02:23:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=160
Sat Apr  9 02:23:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.069 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 160,
    "message": "112 with first half BERT frozen for all epochs",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.381 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 533.44 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 23.05 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.081 Time taken: 531.11 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 23.22 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.059 Time taken: 530.99 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.16 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.046 Time taken: 530.38 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 23.25 secs| Accuracy 98.799

Epoch: 5 Train Loss 0.036 Time taken: 530.12 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.25 secs| Accuracy 98.997
T\P      0       1
0       5273        0036        
1       0071        5237        

Accuracy 98.9922
Time taken: 33.87 secs
2843.391 secs


On the best model
T\P      0       1
0       5273        0036        
1       0071        5237        

Accuracy 98.9922
Time taken: 33.75 secs
Sat Apr  9 03:12:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=161
Sat Apr  9 03:12:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 161,
    "message": "160 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.401 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.225 Time taken: 533.23 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.08 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.083 Time taken: 530.55 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 23.32 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.058 Time taken: 531.06 secs

    Epoch: 3 Validation Loss 0.032 Time taken: 23.16 secs| Accuracy 98.969

Epoch: 4 Train Loss 0.045 Time taken: 530.59 secs

    Epoch: 4 Validation Loss 0.028 Time taken: 23.11 secs| Accuracy 99.138

Epoch: 5 Train Loss 0.036 Time taken: 530.33 secs

    Epoch: 5 Validation Loss 0.029 Time taken: 23.18 secs| Accuracy 99.096
T\P      0       1
0       5273        0036        
1       0054        5254        

Accuracy 99.1523
Time taken: 33.94 secs
2841.873 secs


On the best model
T\P      0       1
0       5252        0057        
1       0044        5264        

Accuracy 99.0487
Time taken: 33.88 secs
Sat Apr  9 04:01:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=162
Sat Apr  9 04:01:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 162,
    "message": "160 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.363 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 534.8 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 23.08 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.084 Time taken: 536.46 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.09 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.058 Time taken: 530.77 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.2 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.044 Time taken: 531.34 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.5 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.036 Time taken: 531.91 secs

    Epoch: 5 Validation Loss 0.037 Time taken: 23.26 secs| Accuracy 98.955
T\P      0       1
0       5224        0085        
1       0036        5272        

Accuracy 98.8603
Time taken: 33.99 secs
2843.654 secs


On the best model
T\P      0       1
0       5288        0021        
1       0076        5232        

Accuracy 99.0864
Time taken: 33.78 secs
Sat Apr  9 04:50:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=163
Sat Apr  9 04:50:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.974 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 163,
    "message": "112 with first third of BERT frozen for all epochs",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "163",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.367 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.176 Time taken: 552.97 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.18 secs| Accuracy 98.488

Epoch: 2 Train Loss 0.066 Time taken: 556.94 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 23.08 secs| Accuracy 98.827

Epoch: 3 Train Loss 0.046 Time taken: 548.49 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.15 secs| Accuracy 98.615

Epoch: 4 Train Loss 0.036 Time taken: 548.12 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 23.23 secs| Accuracy 98.912

Epoch: 5 Train Loss 0.031 Time taken: 557.45 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 22.98 secs| Accuracy 99.181
T\P      0       1
0       5262        0047        
1       0053        5255        

Accuracy 99.0581
Time taken: 33.79 secs
2947.363 secs


On the best model
T\P      0       1
0       5262        0047        
1       0053        5255        

Accuracy 99.0581
Time taken: 33.73 secs
Sat Apr  9 05:41:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=164
Sat Apr  9 05:41:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.002 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 164,
    "message": "163 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "163",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.325 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.206 Time taken: 553.74 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.84 secs| Accuracy 98.361

Epoch: 2 Train Loss 0.076 Time taken: 555.34 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 23.01 secs| Accuracy 98.884

Epoch: 3 Train Loss 0.053 Time taken: 548.82 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 22.92 secs| Accuracy 98.898

Epoch: 4 Train Loss 0.042 Time taken: 554.4 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 22.98 secs| Accuracy 98.997

Epoch: 5 Train Loss 0.031 Time taken: 552.79 secs

    Epoch: 5 Validation Loss 0.037 Time taken: 22.85 secs| Accuracy 99.039
T\P      0       1
0       5279        0030        
1       0073        5235        

Accuracy 99.0299
Time taken: 33.67 secs
2955.693 secs


On the best model
T\P      0       1
0       5279        0030        
1       0073        5235        

Accuracy 99.0299
Time taken: 33.52 secs
Sat Apr  9 06:32:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=165
Sat Apr  9 06:32:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.051 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 165,
    "message": "163 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "163",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.427 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.17 Time taken: 550.89 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 22.95 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.078 Time taken: 555.56 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 22.9 secs| Accuracy 98.743

Epoch: 3 Train Loss 0.057 Time taken: 553.92 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.12 secs| Accuracy 98.686

Epoch: 4 Train Loss 0.04 Time taken: 555.15 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.03 secs| Accuracy 98.813

Epoch: 5 Train Loss 0.033 Time taken: 548.51 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.06 secs| Accuracy 99.181
T\P      0       1
0       5252        0057        
1       0049        5259        

Accuracy 99.0016
Time taken: 33.96 secs
2948.584 secs


On the best model
T\P      0       1
0       5252        0057        
1       0049        5259        

Accuracy 99.0016
Time taken: 33.8 secs
Sat Apr  9 07:23:16 2022




======================================================================
----------------------------------------------------------------------
                run_ID=166
Sat Apr  9 14:11:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.385 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 166,
    "message": "124 with random_sents off",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 1234,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.369 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.463 Time taken: 594.39 secs

    Epoch: 1 Validation Loss 0.215 Time taken: 22.99 secs| Accuracy 91.608

Epoch: 2 Train Loss 0.173 Time taken: 593.25 secs

    Epoch: 2 Validation Loss 0.164 Time taken: 22.93 secs| Accuracy 93.911

Epoch: 3 Train Loss 0.103 Time taken: 591.78 secs

    Epoch: 3 Validation Loss 0.132 Time taken: 22.97 secs| Accuracy 95.055

Epoch: 4 Train Loss 0.073 Time taken: 592.48 secs

    Epoch: 4 Validation Loss 0.142 Time taken: 22.94 secs| Accuracy 95.493

Epoch: 5 Train Loss 0.055 Time taken: 592.21 secs

    Epoch: 5 Validation Loss 0.191 Time taken: 23.06 secs| Accuracy 94.49
T\P      0       1
0       4769        0540        
1       0069        5239        

Accuracy 94.2639
Time taken: 33.95 secs
3141.094 secs


On the best model
T\P      0       1
0       4924        0385        
1       0117        5191        

Accuracy 95.2717
Time taken: 33.82 secs
Sat Apr  9 15:05:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=167
Sat Apr  9 15:05:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 167,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 12,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.518 Time taken: 596.57 secs

    Epoch: 1 Validation Loss 0.267 Time taken: 22.92 secs| Accuracy 88.372

Epoch: 2 Train Loss 0.201 Time taken: 597.2 secs

    Epoch: 2 Validation Loss 0.14 Time taken: 22.89 secs| Accuracy 94.97

Epoch: 3 Train Loss 0.127 Time taken: 592.0 secs

    Epoch: 3 Validation Loss 0.137 Time taken: 22.98 secs| Accuracy 94.857

Epoch: 4 Train Loss 0.081 Time taken: 591.18 secs

    Epoch: 4 Validation Loss 0.16 Time taken: 22.94 secs| Accuracy 94.688

Epoch: 5 Train Loss 0.055 Time taken: 590.73 secs

    Epoch: 5 Validation Loss 0.147 Time taken: 22.93 secs| Accuracy 95.747
T\P      0       1
0       5051        0258        
1       0205        5103        

Accuracy 95.6391
Time taken: 33.77 secs
3144.465 secs


On the best model
T\P      0       1
0       5051        0258        
1       0205        5103        

Accuracy 95.6391
Time taken: 33.72 secs
Sat Apr  9 15:59:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=168
Sat Apr  9 15:59:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.059 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 168,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.523 Time taken: 594.99 secs

    Epoch: 1 Validation Loss 0.268 Time taken: 23.08 secs| Accuracy 88.938

Epoch: 2 Train Loss 0.238 Time taken: 591.24 secs

    Epoch: 2 Validation Loss 0.161 Time taken: 23.2 secs| Accuracy 94.052

Epoch: 3 Train Loss 0.142 Time taken: 591.2 secs

    Epoch: 3 Validation Loss 0.132 Time taken: 23.15 secs| Accuracy 95.211

Epoch: 4 Train Loss 0.099 Time taken: 591.12 secs

    Epoch: 4 Validation Loss 0.118 Time taken: 23.13 secs| Accuracy 95.719

Epoch: 5 Train Loss 0.069 Time taken: 593.58 secs

    Epoch: 5 Validation Loss 0.133 Time taken: 23.24 secs| Accuracy 95.451
T\P      0       1
0       4905        0404        
1       0106        5202        

Accuracy 95.1964
Time taken: 34.22 secs
3146.77 secs


On the best model
T\P      0       1
0       5097        0212        
1       0266        5042        

Accuracy 95.4978
Time taken: 34.18 secs
Sat Apr  9 16:54:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=169
Sat Apr  9 16:54:07 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 169,
    "message": "126 with cls",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.223 Time taken: 596.86 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 23.12 secs| Accuracy 98.531

Epoch: 2 Train Loss 0.071 Time taken: 602.79 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 23.27 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.052 Time taken: 592.78 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 23.16 secs| Accuracy 98.7

Epoch: 4 Train Loss 0.033 Time taken: 592.76 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.19 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.024 Time taken: 592.57 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.13 secs| Accuracy 99.152
T\P      0       1
0       5252        0057        
1       0054        5254        

Accuracy 98.9545
Time taken: 34.12 secs
3163.863 secs


On the best model
T\P      0       1
0       5252        0057        
1       0054        5254        

Accuracy 98.9545
Time taken: 33.91 secs
Sat Apr  9 17:48:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=170
Sat Apr  9 17:48:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 170,
    "message": "126 with BCEwithLogits",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 1.668 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 596.07 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.96 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.066 Time taken: 594.43 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 23.15 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.044 Time taken: 592.36 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 23.02 secs| Accuracy 98.361

Epoch: 4 Train Loss 0.03 Time taken: 593.14 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 22.97 secs| Accuracy 99.039

Epoch: 5 Train Loss 0.023 Time taken: 591.78 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.0 secs| Accuracy 98.898
T\P      0       1
0       5221        0088        
1       0041        5267        

Accuracy 98.785
Time taken: 34.02 secs
3145.085 secs


On the best model
T\P      0       1
0       5232        0077        
1       0040        5268        

Accuracy 98.898
Time taken: 33.78 secs
Sat Apr  9 18:42:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=171
Sat Apr  9 18:42:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
28 seconds taken
Example: ... tensor(-1.)tensor(1.)tensor(-1.)tensor(-1.)tensor(-1.)tensor(-1.)...


{
    "run_ID": 171,
    "message": "126 with HingeEmbeddingLoss",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): Tanh()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): Tanh()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.362 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.098 Time taken: 597.94 secs

    Epoch: 1 Validation Loss 0.023 Time taken: 23.26 secs| Accuracy 97.344

Epoch: 2 Train Loss 0.04 Time taken: 593.25 secs

    Epoch: 2 Validation Loss 0.022 Time taken: 23.29 secs| Accuracy 97.626

Epoch: 3 Train Loss 0.039 Time taken: 593.02 secs

    Epoch: 3 Validation Loss 0.021 Time taken: 23.32 secs| Accuracy 98.107

Epoch: 4 Train Loss 0.034 Time taken: 593.12 secs

    Epoch: 4 Validation Loss 0.019 Time taken: 23.27 secs| Accuracy 92.229

Epoch: 5 Train Loss 0.033 Time taken: 592.49 secs

    Epoch: 5 Validation Loss 0.023 Time taken: 23.29 secs| Accuracy 97.683
T\P      0       1
0       5133        0176        
1       0068        5240        

Accuracy 97.7018
Time taken: 34.17 secs
3155.002 secs


On the best model
T\P      0       1
0       5238        0071        
1       0118        5190        

Accuracy 98.2198
Time taken: 34.08 secs
Sat Apr  9 19:36:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=172
Sat Apr  9 19:36:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.567 GB
39 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 172,
    "message": "126 with indic bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.153 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.449 Time taken: 574.89 secs

    Epoch: 1 Validation Loss 0.172 Time taken: 23.07 secs| Accuracy 95.917

Epoch: 2 Train Loss 0.13 Time taken: 576.26 secs

    Epoch: 2 Validation Loss 0.107 Time taken: 23.13 secs| Accuracy 96.355

Epoch: 3 Train Loss 0.09 Time taken: 575.7 secs

    Epoch: 3 Validation Loss 0.085 Time taken: 23.16 secs| Accuracy 97.203

Epoch: 4 Train Loss 0.066 Time taken: 576.52 secs

    Epoch: 4 Validation Loss 0.083 Time taken: 23.4 secs| Accuracy 97.528

Epoch: 5 Train Loss 0.046 Time taken: 576.66 secs

    Epoch: 5 Validation Loss 0.084 Time taken: 23.34 secs| Accuracy 97.584
T\P      0       1
0       5217        0092        
1       0151        5157        

Accuracy 97.7112
Time taken: 34.28 secs
3035.64 secs


On the best model
T\P      0       1
0       5217        0092        
1       0151        5157        

Accuracy 97.7112
Time taken: 34.92 secs
Sat Apr  9 20:29:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=173
Sat Apr  9 20:29:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 173,
    "message": "126 with mbert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.99 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.211 Time taken: 589.34 secs

    Epoch: 1 Validation Loss 0.081 Time taken: 22.88 secs| Accuracy 97.838

Epoch: 2 Train Loss 0.057 Time taken: 587.39 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.04 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.032 Time taken: 586.88 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 23.09 secs| Accuracy 98.276

Epoch: 4 Train Loss 0.016 Time taken: 586.81 secs

    Epoch: 4 Validation Loss 0.066 Time taken: 22.96 secs| Accuracy 98.404

Epoch: 5 Train Loss 0.014 Time taken: 586.64 secs

    Epoch: 5 Validation Loss 0.071 Time taken: 23.01 secs| Accuracy 98.389
T\P      0       1
0       5252        0057        
1       0084        5224        

Accuracy 98.6719
Time taken: 34.02 secs
3104.751 secs


On the best model
T\P      0       1
0       5237        0072        
1       0077        5231        

Accuracy 98.5966
Time taken: 33.84 secs
Sat Apr  9 21:22:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=174
Sat Apr  9 21:22:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.067 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 174,
    "message": "126 with first half BERT frozen for first two epochs",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "151",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.277 Time taken: 534.73 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 22.75 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.08 Time taken: 538.21 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 22.72 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.057 Time taken: 591.63 secs

    Epoch: 3 Validation Loss 0.031 Time taken: 22.91 secs| Accuracy 99.068

Epoch: 4 Train Loss 0.033 Time taken: 590.43 secs

    Epoch: 4 Validation Loss 0.029 Time taken: 22.9 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.022 Time taken: 590.59 secs

    Epoch: 5 Validation Loss 0.031 Time taken: 22.94 secs| Accuracy 99.096
T\P      0       1
0       5219        0090        
1       0028        5280        

Accuracy 98.8886
Time taken: 33.88 secs
3032.528 secs


On the best model
T\P      0       1
0       5243        0066        
1       0036        5272        

Accuracy 99.0393
Time taken: 33.67 secs
Sat Apr  9 22:14:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=175
Sat Apr  9 22:14:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 175,
    "message": "126 with first half BERT frozen for first two epochs, then second half BERT frozen for next 2 epochs, then nothing frozen for the last epoch",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "157",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.378 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.277 Time taken: 532.94 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 22.8 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.08 Time taken: 536.67 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 22.85 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.054 Time taken: 572.77 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 22.9 secs| Accuracy 98.969

Epoch: 4 Train Loss 0.033 Time taken: 574.38 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 23.01 secs| Accuracy 99.152

Epoch: 5 Train Loss 0.037 Time taken: 591.34 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 22.9 secs| Accuracy 98.827
T\P      0       1
0       5222        0087        
1       0034        5274        

Accuracy 98.8603
Time taken: 33.79 secs
2994.439 secs


On the best model
T\P      0       1
0       5270        0039        
1       0048        5260        

Accuracy 99.1806
Time taken: 33.65 secs
Sat Apr  9 23:06:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=176
Sat Apr  9 23:06:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.043 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 176,
    "message": "126 with first half BERT frozen for all epochs",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.381 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.277 Time taken: 535.48 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 23.14 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.08 Time taken: 540.33 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 23.15 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.057 Time taken: 533.1 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.22 secs| Accuracy 98.615

Epoch: 4 Train Loss 0.04 Time taken: 533.16 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 23.14 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.034 Time taken: 532.77 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.27 secs| Accuracy 99.11
T\P      0       1
0       5239        0070        
1       0043        5265        

Accuracy 98.9357
Time taken: 33.99 secs
2862.285 secs


On the best model
T\P      0       1
0       5239        0070        
1       0043        5265        

Accuracy 98.9357
Time taken: 33.9 secs
Sat Apr  9 23:55:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=177
Sat Apr  9 23:55:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.073 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 177,
    "message": "126 with first third of BERT frozen for all epochs",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "163",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.372 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.263 Time taken: 551.54 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 22.93 secs| Accuracy 98.517

Epoch: 2 Train Loss 0.076 Time taken: 556.46 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 22.95 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.054 Time taken: 555.23 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 22.92 secs| Accuracy 98.728

Epoch: 4 Train Loss 0.036 Time taken: 548.52 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 23.06 secs| Accuracy 98.785

Epoch: 5 Train Loss 0.031 Time taken: 548.76 secs

    Epoch: 5 Validation Loss 0.031 Time taken: 22.98 secs| Accuracy 99.152
T\P      0       1
0       5241        0068        
1       0039        5269        

Accuracy 98.9922
Time taken: 33.8 secs
2947.205 secs


On the best model
T\P      0       1
0       5241        0068        
1       0039        5269        

Accuracy 98.9922
Time taken: 33.73 secs
Sun Apr 10 00:46:34 2022



======================================================================
----------------------------------------------------------------------
                run_ID=178
Mon Apr 11 12:21:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.067 GB
36 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 178,
    "message": "126 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.376 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 592.41 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.54 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.066 Time taken: 591.05 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.63 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.043 Time taken: 591.09 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.75 secs| Accuracy 98.658


======================================================================
----------------------------------------------------------------------
                run_ID=179
Mon Apr 11 13:09:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.093 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 179,
    "message": "126 repeat with changes in databaseSearch set max_len=300",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.359 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 1439.06 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.64 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.066 Time taken: 1370.33 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.68 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.043 Time taken: 1368.75 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.63 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.029 Time taken: 1370.45 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 22.84 secs| Accuracy 98.884


======================================================================
----------------------------------------------------------------------
                run_ID=180
Mon Apr 11 17:08:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.053 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 180,
    "message": "126 repeat for gradients",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 121,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 281.31 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 23.17 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.068 Time taken: 443.74 secs

    Epoch: 2 Validation Loss 0.077 Time taken: 23.03 secs| Accuracy 98.05

Epoch: 3 Train Loss 0.049 Time taken: 614.78 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 22.96 secs| Accuracy 99.025

Epoch: 4 Train Loss 0.033 Time taken: 786.61 secs

    Epoch: 4 Validation Loss 0.064 Time taken: 23.06 secs| Accuracy 97.923

Epoch: 5 Train Loss 0.022 Time taken: 947.75 secs

    Epoch: 5 Validation Loss 0.049 Time taken: 23.02 secs| Accuracy 98.743
T\P      0       1
0       5206        0103        
1       0021        5287        

Accuracy 98.8321
Time taken: 34.0 secs
3246.811 secs


On the best model
T\P      0       1
0       5266        0043        
1       0072        5236        

Accuracy 98.9168
Time taken: 33.97 secs
Mon Apr 11 18:04:05 2022




======================================================================
----------------------------------------------------------------------
                run_ID=181
Thu Apr 14 20:17:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 181,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.605 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.265 Time taken: 593.69 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 23.12 secs| Accuracy 97.966

Epoch: 2 Train Loss 0.082 Time taken: 601.72 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.06 secs| Accuracy 98.7

Epoch: 3 Train Loss 0.053 Time taken: 592.25 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.16 secs| Accuracy 98.94

Epoch: 4 Train Loss 0.041 Time taken: 592.75 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 23.08 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.028 Time taken: 592.42 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.12 secs| Accuracy 99.209
T\P      0       1
0       5246        0063        
1       0038        5270        

Accuracy 99.0487
Time taken: 33.92 secs
3164.351 secs


On the best model
T\P      0       1
0       5246        0063        
1       0038        5270        

Accuracy 99.0487
Time taken: 33.88 secs
Thu Apr 14 21:11:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=182
Thu Apr 14 21:11:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 182,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.357 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.296 Time taken: 595.09 secs

    Epoch: 1 Validation Loss 0.081 Time taken: 23.03 secs| Accuracy 97.838

Epoch: 2 Train Loss 0.101 Time taken: 593.93 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 23.07 secs| Accuracy 98.107

Epoch: 3 Train Loss 0.072 Time taken: 595.86 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 23.02 secs| Accuracy 98.022

Epoch: 4 Train Loss 0.053 Time taken: 593.3 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 23.01 secs| Accuracy 97.951

Epoch: 5 Train Loss 0.04 Time taken: 593.78 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 22.96 secs| Accuracy 98.714
T\P      0       1
0       5222        0087        
1       0040        5268        

Accuracy 98.8038
Time taken: 33.93 secs
3152.418 secs


On the best model
T\P      0       1
0       5222        0087        
1       0040        5268        

Accuracy 98.8038
Time taken: 33.84 secs
Thu Apr 14 22:05:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=183
Thu Apr 14 22:05:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -9.851 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 183,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.397 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.303 Time taken: 592.95 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.87 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.098 Time taken: 597.38 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 22.81 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.064 Time taken: 591.42 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.86 secs| Accuracy 98.46

Epoch: 4 Train Loss 0.046 Time taken: 592.55 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 22.89 secs| Accuracy 98.7

Epoch: 5 Train Loss 0.033 Time taken: 599.5 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 22.78 secs| Accuracy 99.068
T\P      0       1
0       5285        0024        
1       0071        5237        

Accuracy 99.1052
Time taken: 33.65 secs
3167.659 secs


On the best model
T\P      0       1
0       5285        0024        
1       0071        5237        

Accuracy 99.1052
Time taken: 33.63 secs
Thu Apr 14 23:00:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=184
Thu Apr 14 23:00:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 184,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.353 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.234 Time taken: 596.74 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 23.17 secs| Accuracy 98.036

Epoch: 2 Train Loss 0.073 Time taken: 604.05 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.25 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.048 Time taken: 602.14 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.44 secs| Accuracy 98.898

Epoch: 4 Train Loss 0.033 Time taken: 594.45 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 23.45 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.023 Time taken: 594.0 secs

    Epoch: 5 Validation Loss 0.052 Time taken: 23.59 secs| Accuracy 98.743
T\P      0       1
0       5212        0097        
1       0034        5274        

Accuracy 98.7661
Time taken: 34.43 secs
3178.455 secs


On the best model
T\P      0       1
0       5265        0044        
1       0052        5256        

Accuracy 99.0958
Time taken: 34.07 secs
Thu Apr 14 23:55:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=185
Thu Apr 14 23:55:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.05 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 185,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.351 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.312 Time taken: 593.44 secs

    Epoch: 1 Validation Loss 0.104 Time taken: 22.83 secs| Accuracy 97.457

Epoch: 2 Train Loss 0.118 Time taken: 599.38 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 22.97 secs| Accuracy 97.739

Epoch: 3 Train Loss 0.082 Time taken: 598.44 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 22.98 secs| Accuracy 98.248

Epoch: 4 Train Loss 0.062 Time taken: 590.92 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 22.97 secs| Accuracy 98.601

Epoch: 5 Train Loss 0.046 Time taken: 590.77 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 23.01 secs| Accuracy 98.7
T\P      0       1
0       5235        0074        
1       0043        5265        

Accuracy 98.898
Time taken: 33.84 secs
3170.93 secs


On the best model
T\P      0       1
0       5235        0074        
1       0043        5265        

Accuracy 98.898
Time taken: 33.73 secs
Fri Apr 15 00:49:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=186
Fri Apr 15 00:49:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.045 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 186,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.322 Time taken: 593.59 secs

    Epoch: 1 Validation Loss 0.122 Time taken: 23.17 secs| Accuracy 97.344

Epoch: 2 Train Loss 0.141 Time taken: 598.78 secs

    Epoch: 2 Validation Loss 0.08 Time taken: 23.04 secs| Accuracy 97.669

Epoch: 3 Train Loss 0.113 Time taken: 591.19 secs

    Epoch: 3 Validation Loss 0.143 Time taken: 23.2 secs| Accuracy 95.521

Epoch: 4 Train Loss 0.091 Time taken: 590.85 secs

    Epoch: 4 Validation Loss 0.058 Time taken: 23.1 secs| Accuracy 98.177

Epoch: 5 Train Loss 0.068 Time taken: 592.23 secs

    Epoch: 5 Validation Loss 0.057 Time taken: 23.2 secs| Accuracy 98.234
T\P      0       1
0       5213        0096        
1       0052        5256        

Accuracy 98.606
Time taken: 34.11 secs
3155.869 secs


On the best model
T\P      0       1
0       5213        0096        
1       0052        5256        

Accuracy 98.606
Time taken: 34.0 secs
Fri Apr 15 01:43:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=187
Fri Apr 15 01:43:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.043 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 187,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.368 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.223 Time taken: 598.86 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.84 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.073 Time taken: 604.26 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 23.55 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.047 Time taken: 596.6 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.56 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.033 Time taken: 595.61 secs

    Epoch: 4 Validation Loss 0.042 Time taken: 23.57 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.025 Time taken: 602.71 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 23.43 secs| Accuracy 98.573
T\P      0       1
0       5299        0010        
1       0103        5205        

Accuracy 98.9357
Time taken: 34.41 secs
3188.839 secs


On the best model
T\P      0       1
0       5273        0036        
1       0061        5247        

Accuracy 99.0864
Time taken: 34.2 secs
Fri Apr 15 02:38:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=202
Fri Apr 15 16:55:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.063 GB
36 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 202,
    "message": "136 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.515 Time taken: 590.11 secs

    Epoch: 1 Validation Loss 0.255 Time taken: 23.61 secs| Accuracy 90.138

Epoch: 2 Train Loss 0.205 Time taken: 595.78 secs

    Epoch: 2 Validation Loss 0.155 Time taken: 23.17 secs| Accuracy 94.278

Epoch: 3 Train Loss 0.124 Time taken: 589.69 secs

    Epoch: 3 Validation Loss 0.121 Time taken: 23.69 secs| Accuracy 95.663

Epoch: 4 Train Loss 0.085 Time taken: 595.28 secs

    Epoch: 4 Validation Loss 0.123 Time taken: 23.64 secs| Accuracy 95.281

Epoch: 5 Train Loss 0.06 Time taken: 596.31 secs

    Epoch: 5 Validation Loss 0.113 Time taken: 23.91 secs| Accuracy 96.072
T\P      0       1
0       5038        0271        
1       0164        5144        

Accuracy 95.903
Time taken: 34.42 secs
3156.812 secs


On the best model
T\P      0       1
0       5038        0271        
1       0164        5144        

Accuracy 95.903
Time taken: 34.19 secs
Fri Apr 15 17:50:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=203
Fri Apr 15 17:50:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 203,
    "message": "136 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.438 Time taken: 598.19 secs

    Epoch: 1 Validation Loss 0.192 Time taken: 23.51 secs| Accuracy 92.569

Epoch: 2 Train Loss 0.171 Time taken: 592.58 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 23.31 secs| Accuracy 94.292

Epoch: 3 Train Loss 0.108 Time taken: 591.99 secs

    Epoch: 3 Validation Loss 0.119 Time taken: 23.27 secs| Accuracy 95.691

Epoch: 4 Train Loss 0.072 Time taken: 593.01 secs

    Epoch: 4 Validation Loss 0.118 Time taken: 23.27 secs| Accuracy 95.634

Epoch: 5 Train Loss 0.052 Time taken: 592.12 secs

    Epoch: 5 Validation Loss 0.139 Time taken: 23.4 secs| Accuracy 95.493
T\P      0       1
0       5149        0160        
1       0301        5007        

Accuracy 95.658
Time taken: 34.42 secs
3152.512 secs


On the best model
T\P      0       1
0       5054        0255        
1       0202        5106        

Accuracy 95.696
Time taken: 34.15 secs
Fri Apr 15 18:44:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=204
Fri Apr 15 18:44:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.05 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 204,
    "message": "136 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.554 Time taken: 597.13 secs

    Epoch: 1 Validation Loss 0.277 Time taken: 23.24 secs| Accuracy 89.164

Epoch: 2 Train Loss 0.225 Time taken: 594.04 secs

    Epoch: 2 Validation Loss 0.149 Time taken: 23.23 secs| Accuracy 94.052

Epoch: 3 Train Loss 0.125 Time taken: 591.13 secs

    Epoch: 3 Validation Loss 0.127 Time taken: 23.19 secs| Accuracy 95.422

Epoch: 4 Train Loss 0.085 Time taken: 591.79 secs

    Epoch: 4 Validation Loss 0.123 Time taken: 23.07 secs| Accuracy 95.705

Epoch: 5 Train Loss 0.059 Time taken: 591.24 secs

    Epoch: 5 Validation Loss 0.147 Time taken: 23.21 secs| Accuracy 95.239
T\P      0       1
0       5194        0115        
1       0331        4977        

Accuracy 95.799
Time taken: 34.17 secs
3152.729 secs


On the best model
T\P      0       1
0       5144        0165        
1       0265        5043        

Accuracy 95.95
Time taken: 33.85 secs
Fri Apr 15 19:38:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=205
Fri Apr 15 19:38:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.053 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 205,
    "message": "136 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.191 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.478 Time taken: 593.24 secs

    Epoch: 1 Validation Loss 0.273 Time taken: 22.93 secs| Accuracy 88.274

Epoch: 2 Train Loss 0.206 Time taken: 593.39 secs

    Epoch: 2 Validation Loss 0.137 Time taken: 22.98 secs| Accuracy 94.9

Epoch: 3 Train Loss 0.124 Time taken: 590.08 secs

    Epoch: 3 Validation Loss 0.128 Time taken: 23.07 secs| Accuracy 95.281

Epoch: 4 Train Loss 0.088 Time taken: 593.69 secs

    Epoch: 4 Validation Loss 0.112 Time taken: 22.99 secs| Accuracy 95.776

Epoch: 5 Train Loss 0.06 Time taken: 590.94 secs

    Epoch: 5 Validation Loss 0.123 Time taken: 23.09 secs| Accuracy 95.677
T\P      0       1
0       5018        0291        
1       0188        5120        

Accuracy 95.488
Time taken: 33.98 secs
3149.968 secs


On the best model
T\P      0       1
0       5141        0168        
1       0265        5043        

Accuracy 95.922
Time taken: 34.03 secs
Fri Apr 15 20:32:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=206
Fri Apr 15 20:33:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.996 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 206,
    "message": "136 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.416 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.424 Time taken: 595.08 secs

    Epoch: 1 Validation Loss 0.18 Time taken: 23.19 secs| Accuracy 92.936

Epoch: 2 Train Loss 0.158 Time taken: 605.58 secs

    Epoch: 2 Validation Loss 0.124 Time taken: 22.89 secs| Accuracy 95.253

Epoch: 3 Train Loss 0.099 Time taken: 594.06 secs

    Epoch: 3 Validation Loss 0.108 Time taken: 23.37 secs| Accuracy 96.086

Epoch: 4 Train Loss 0.067 Time taken: 592.68 secs

    Epoch: 4 Validation Loss 0.13 Time taken: 23.09 secs| Accuracy 95.55

Epoch: 5 Train Loss 0.05 Time taken: 595.15 secs

    Epoch: 5 Validation Loss 0.118 Time taken: 23.21 secs| Accuracy 96.496
T\P      0       1
0       5098        0211        
1       0169        5139        

Accuracy 96.421
Time taken: 34.2 secs
3174.373 secs


On the best model
T\P      0       1
0       5098        0211        
1       0169        5139        

Accuracy 96.421
Time taken: 33.93 secs
Fri Apr 15 21:27:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=207
Fri Apr 15 21:27:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.184 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 207,
    "message": "136 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.41 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.501 Time taken: 594.03 secs

    Epoch: 1 Validation Loss 0.268 Time taken: 23.08 secs| Accuracy 89.107

Epoch: 2 Train Loss 0.204 Time taken: 595.29 secs

    Epoch: 2 Validation Loss 0.148 Time taken: 23.1 secs| Accuracy 94.589

Epoch: 3 Train Loss 0.12 Time taken: 593.96 secs

    Epoch: 3 Validation Loss 0.129 Time taken: 23.45 secs| Accuracy 95.281

Epoch: 4 Train Loss 0.079 Time taken: 592.09 secs

    Epoch: 4 Validation Loss 0.143 Time taken: 23.11 secs| Accuracy 95.309

Epoch: 5 Train Loss 0.051 Time taken: 594.42 secs

    Epoch: 5 Validation Loss 0.149 Time taken: 23.3 secs| Accuracy 95.762
T\P      0       1
0       5149        0160        
1       0267        5041        

Accuracy 95.978
Time taken: 33.75 secs
3163.647 secs


On the best model
T\P      0       1
0       5149        0160        
1       0267        5041        

Accuracy 95.978
Time taken: 33.75 secs
Fri Apr 15 22:22:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=221
Sat Apr 16 01:27:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.07 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 221,
    "message": "fc2 with first half of bert frozen with random sents on",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.383 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.218 Time taken: 533.21 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 23.27 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.081 Time taken: 534.85 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 23.31 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.056 Time taken: 532.41 secs

    Epoch: 3 Validation Loss 0.034 Time taken: 23.4 secs| Accuracy 98.827

Epoch: 4 Train Loss 0.041 Time taken: 532.45 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 23.31 secs| Accuracy 99.082

Epoch: 5 Train Loss 0.036 Time taken: 533.86 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.24 secs| Accuracy 99.138
T\P      0       1
0       5238        0071        
1       0039        5269        

Accuracy 98.964
Time taken: 33.95 secs
2866.649 secs


On the best model
T\P      0       1
0       5238        0071        
1       0039        5269        

Accuracy 98.964
Time taken: 33.77 secs
Sat Apr 16 02:16:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=222
Sat Apr 16 02:17:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 222,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.182 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.268 Time taken: 536.43 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 23.49 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.094 Time taken: 532.99 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.5 secs| Accuracy 98.7

Epoch: 3 Train Loss 0.062 Time taken: 536.26 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.49 secs| Accuracy 98.827

Epoch: 4 Train Loss 0.043 Time taken: 535.8 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 23.67 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.036 Time taken: 536.09 secs

    Epoch: 5 Validation Loss 0.037 Time taken: 23.66 secs| Accuracy 98.969
T\P      0       1
0       5263        0046        
1       0050        5258        

Accuracy 99.096
Time taken: 34.57 secs
2872.755 secs


On the best model
T\P      0       1
0       5263        0046        
1       0050        5258        

Accuracy 99.096
Time taken: 34.2 secs
Sat Apr 16 03:06:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=223
Sat Apr 16 03:06:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.241 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 223,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.525 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.324 Time taken: 537.82 secs

    Epoch: 1 Validation Loss 0.093 Time taken: 23.28 secs| Accuracy 97.768

Epoch: 2 Train Loss 0.109 Time taken: 542.43 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.19 secs| Accuracy 98.573

Epoch: 3 Train Loss 0.064 Time taken: 544.54 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 23.16 secs| Accuracy 98.262

Epoch: 4 Train Loss 0.05 Time taken: 539.58 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.2 secs| Accuracy 99.025

Epoch: 5 Train Loss 0.038 Time taken: 534.08 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.31 secs| Accuracy 99.053
T\P      0       1
0       5252        0057        
1       0038        5270        

Accuracy 99.105
Time taken: 34.12 secs
2883.81 secs


On the best model
T\P      0       1
0       5252        0057        
1       0038        5270        

Accuracy 99.105
Time taken: 33.85 secs
Sat Apr 16 03:56:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=224
Sat Apr 16 03:56:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.231 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 224,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.215 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.266 Time taken: 536.71 secs

    Epoch: 1 Validation Loss 0.088 Time taken: 23.26 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.106 Time taken: 541.96 secs

    Epoch: 2 Validation Loss 0.074 Time taken: 23.18 secs| Accuracy 97.782

Epoch: 3 Train Loss 0.076 Time taken: 535.97 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 23.29 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.062 Time taken: 534.08 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.44 secs| Accuracy 98.969

Epoch: 5 Train Loss 0.043 Time taken: 534.53 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.3 secs| Accuracy 99.096
T\P      0       1
0       5270        0039        
1       0065        5243        

Accuracy 99.02
Time taken: 34.21 secs
2877.591 secs


On the best model
T\P      0       1
0       5270        0039        
1       0065        5243        

Accuracy 99.02
Time taken: 34.1 secs
Sat Apr 16 04:45:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=225
Sat Apr 16 04:45:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.042 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 225,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.218 Time taken: 534.63 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 23.14 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.077 Time taken: 533.19 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.15 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.058 Time taken: 538.91 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.09 secs| Accuracy 98.714

Epoch: 4 Train Loss 0.044 Time taken: 534.04 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 23.23 secs| Accuracy 98.799

Epoch: 5 Train Loss 0.036 Time taken: 534.85 secs

    Epoch: 5 Validation Loss 0.031 Time taken: 23.25 secs| Accuracy 99.068
T\P      0       1
0       5257        0052        
1       0054        5254        

Accuracy 99.002
Time taken: 33.89 secs
2863.335 secs


On the best model
T\P      0       1
0       5257        0052        
1       0054        5254        

Accuracy 99.002
Time taken: 33.98 secs
Sat Apr 16 05:35:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=226
Sat Apr 16 05:35:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 226,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.361 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.238 Time taken: 536.77 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 23.2 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.089 Time taken: 534.84 secs

    Epoch: 2 Validation Loss 0.077 Time taken: 23.22 secs| Accuracy 97.245

Epoch: 3 Train Loss 0.063 Time taken: 534.37 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.23 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.049 Time taken: 534.61 secs

    Epoch: 4 Validation Loss 0.042 Time taken: 23.35 secs| Accuracy 98.559

Epoch: 5 Train Loss 0.039 Time taken: 535.12 secs

    Epoch: 5 Validation Loss 0.037 Time taken: 23.42 secs| Accuracy 98.87
T\P      0       1
0       5290        0019        
1       0081        5227        

Accuracy 99.058
Time taken: 34.1 secs
2857.428 secs


On the best model
T\P      0       1
0       5290        0019        
1       0081        5227        

Accuracy 99.058
Time taken: 34.12 secs
Sat Apr 16 06:24:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=227
Sat Apr 16 06:24:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 227,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.279 Time taken: 535.54 secs

    Epoch: 1 Validation Loss 0.084 Time taken: 23.09 secs| Accuracy 97.626

Epoch: 2 Train Loss 0.108 Time taken: 534.22 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.06 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.068 Time taken: 533.61 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.2 secs| Accuracy 98.997

Epoch: 4 Train Loss 0.054 Time taken: 532.61 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.25 secs| Accuracy 98.743

Epoch: 5 Train Loss 0.041 Time taken: 533.65 secs

    Epoch: 5 Validation Loss 0.027 Time taken: 23.34 secs| Accuracy 99.152
T\P      0       1
0       5277        0032        
1       0045        5263        

Accuracy 99.275
Time taken: 34.11 secs
2859.656 secs


On the best model
T\P      0       1
0       5277        0032        
1       0045        5263        

Accuracy 99.275
Time taken: 33.64 secs
Sat Apr 16 07:13:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=228
Sat Apr 16 07:14:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 228,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 535.05 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 23.04 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.08 Time taken: 541.93 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.16 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.059 Time taken: 533.83 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 23.23 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.049 Time taken: 540.29 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.31 secs| Accuracy 98.799

Epoch: 5 Train Loss 0.035 Time taken: 533.39 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.55 secs| Accuracy 98.856
T\P      0       1
0       5293        0016        
1       0094        5214        

Accuracy 98.964
Time taken: 33.99 secs
2879.068 secs


On the best model
T\P      0       1
0       5293        0016        
1       0094        5214        

Accuracy 98.964
Time taken: 33.89 secs
Sat Apr 16 08:03:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=229
Sat Apr 16 08:03:45 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 229,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.223 Time taken: 534.93 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.19 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.089 Time taken: 533.0 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 23.34 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.066 Time taken: 533.09 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.16 secs| Accuracy 98.672

Epoch: 4 Train Loss 0.054 Time taken: 532.47 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 23.32 secs| Accuracy 98.785

Epoch: 5 Train Loss 0.039 Time taken: 534.57 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.42 secs| Accuracy 99.011
T\P      0       1
0       5276        0033        
1       0071        5237        

Accuracy 99.02
Time taken: 34.13 secs
2865.553 secs


On the best model
T\P      0       1
0       5276        0033        
1       0071        5237        

Accuracy 99.02
Time taken: 34.05 secs
Sat Apr 16 08:53:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=230
Sat Apr 16 08:53:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.051 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 230,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.359 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.259 Time taken: 537.63 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.27 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.096 Time taken: 540.23 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.13 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.061 Time taken: 534.34 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.18 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.048 Time taken: 533.55 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.34 secs| Accuracy 98.771

Epoch: 5 Train Loss 0.036 Time taken: 533.58 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 23.31 secs| Accuracy 99.039
T\P      0       1
0       5255        0054        
1       0042        5266        

Accuracy 99.096
Time taken: 33.84 secs
2873.5 secs


On the best model
T\P      0       1
0       5255        0054        
1       0042        5266        

Accuracy 99.096
Time taken: 34.0 secs
Sat Apr 16 09:42:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=251
Sat Apr 16 10:15:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 53087 | Validation_set: 21230 | Test_set: 31853
Train_batches: 1659 | Validation_batches: 664 | Test_batches: 996
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.323 GB
39 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 251,
    "message": "188 with repeated negatives four times",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 4,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.426 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.097 Time taken: 1043.77 secs

    Epoch: 1 Validation Loss 0.034 Time taken: 67.56 secs| Macro F: 0.982

Epoch: 2 Train Loss 0.041 Time taken: 1044.14 secs

    Epoch: 2 Validation Loss 0.021 Time taken: 67.51 secs| Macro F: 0.989

Epoch: 3 Train Loss 0.029 Time taken: 1040.56 secs

    Epoch: 3 Validation Loss 0.02 Time taken: 66.92 secs| Macro F: 0.989

Epoch: 4 Train Loss 0.02 Time taken: 1037.23 secs

    Epoch: 4 Validation Loss 0.017 Time taken: 67.06 secs| Macro F: 0.99

Epoch: 5 Train Loss 0.015 Time taken: 1040.1 secs

    Epoch: 5 Validation Loss 0.016 Time taken: 67.35 secs| Macro F: 0.992
T\P      0       1
0       26478       0067        
1       0072        5236        

Macro F: 0.992
Time taken: 99.65 secs
5682.03 secs


On the best model
T\P      0       1
0       26478       0067        
1       0072        5236        

Macro F: 0.992
Time taken: 99.38 secs
Sat Apr 16 11:52:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=252
Sat Apr 16 11:52:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 53087 | Validation_set: 21230 | Test_set: 31853
Train_batches: 1659 | Validation_batches: 664 | Test_batches: 996
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.278 GB
42 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 252,
    "message": "251 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 4,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.384 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.116 Time taken: 1043.52 secs

    Epoch: 1 Validation Loss 0.086 Time taken: 67.11 secs| Macro F: 0.926

Epoch: 2 Train Loss 0.045 Time taken: 1041.44 secs

    Epoch: 2 Validation Loss 0.028 Time taken: 67.28 secs| Macro F: 0.985

Epoch: 3 Train Loss 0.029 Time taken: 1049.6 secs

    Epoch: 3 Validation Loss 0.027 Time taken: 67.09 secs| Macro F: 0.984

Epoch: 4 Train Loss 0.021 Time taken: 1044.19 secs

    Epoch: 4 Validation Loss 0.024 Time taken: 67.34 secs| Macro F: 0.988

Epoch: 5 Train Loss 0.016 Time taken: 1044.74 secs

    Epoch: 5 Validation Loss 0.021 Time taken: 68.07 secs| Macro F: 0.99
T\P      0       1
0       26441       0104        
1       0069        5239        

Macro F: 0.99
Time taken: 99.87 secs
5701.018 secs


On the best model
T\P      0       1
0       26441       0104        
1       0069        5239        

Macro F: 0.99
Time taken: 99.46 secs
Sat Apr 16 13:30:48 2022



======================================================================
----------------------------------------------------------------------
                run_ID=253
Sat Apr 16 14:35:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 53087 | Validation_set: 21230 | Test_set: 31853
Train_batches: 1659 | Validation_batches: 664 | Test_batches: 996
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.136 GB
39 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 253,
    "message": "251 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 4,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.76 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.092 Time taken: 1047.63 secs

    Epoch: 1 Validation Loss 0.028 Time taken: 67.57 secs| Macro F: 0.986

Epoch: 2 Train Loss 0.034 Time taken: 1043.24 secs

    Epoch: 2 Validation Loss 0.025 Time taken: 67.48 secs| Macro F: 0.987

Epoch: 3 Train Loss 0.021 Time taken: 1041.88 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 67.26 secs| Macro F: 0.984

Epoch: 4 Train Loss 0.015 Time taken: 1043.07 secs

    Epoch: 4 Validation Loss 0.017 Time taken: 67.21 secs| Macro F: 0.991

Epoch: 5 Train Loss 0.012 Time taken: 1040.49 secs

    Epoch: 5 Validation Loss 0.02 Time taken: 67.06 secs| Macro F: 0.992
T\P      0       1
0       26496       0049        
1       0095        5213        

Macro F: 0.992
Time taken: 99.44 secs
5711.149 secs


On the best model
T\P      0       1
0       26496       0049        
1       0095        5213        

Macro F: 0.992
Time taken: 99.33 secs
Sat Apr 16 16:13:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=258
Sat Apr 16 16:13:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 70783 | Validation_set: 28306 | Test_set: 42471
Train_batches: 2212 | Validation_batches: 885 | Test_batches: 1328
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.061 GB
42 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 258,
    "message": "257 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 6,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.332 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.071 Time taken: 1275.69 secs

    Epoch: 1 Validation Loss 0.025 Time taken: 89.13 secs| Macro F: 0.985

Epoch: 2 Train Loss 0.027 Time taken: 1279.96 secs

    Epoch: 2 Validation Loss 0.017 Time taken: 89.04 secs| Macro F: 0.989

Epoch: 3 Train Loss 0.018 Time taken: 1278.76 secs

    Epoch: 3 Validation Loss 0.016 Time taken: 88.51 secs| Macro F: 0.99

Epoch: 4 Train Loss 0.012 Time taken: 1269.94 secs

    Epoch: 4 Validation Loss 0.016 Time taken: 89.86 secs| Macro F: 0.99

Epoch: 5 Train Loss 0.009 Time taken: 1271.57 secs

    Epoch: 5 Validation Loss 0.018 Time taken: 89.89 secs| Macro F: 0.991
T\P      0       1
0       37089       0074        
1       0103        5205        

Macro F: 0.99
Time taken: 133.38 secs
7017.287 secs


On the best model
T\P      0       1
0       37089       0074        
1       0103        5205        

Macro F: 0.99
Time taken: 132.9 secs
Sat Apr 16 18:13:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=260
Sat Apr 16 18:14:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.859 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 260,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.321 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.208 Time taken: 600.69 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.39 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.074 Time taken: 608.15 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 23.4 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.047 Time taken: 597.35 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.61 secs| Accuracy 98.672

Epoch: 4 Train Loss 0.039 Time taken: 596.88 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 23.48 secs| Accuracy 99.082

Epoch: 5 Train Loss 0.026 Time taken: 597.32 secs

    Epoch: 5 Validation Loss 0.049 Time taken: 23.58 secs| Accuracy 98.87
T\P      0       1
0       5258        0051        
1       0045        5263        

Accuracy 99.096
Time taken: 34.24 secs
3197.449 secs


On the best model
T\P      0       1
0       5265        0044        
1       0055        5253        

Accuracy 99.068
Time taken: 34.1 secs
Sat Apr 16 19:09:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=261
Sat Apr 16 19:09:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.069 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 261,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.508 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.199 Time taken: 601.34 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 23.74 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.077 Time taken: 608.15 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.84 secs| Accuracy 98.573

Epoch: 3 Train Loss 0.05 Time taken: 607.32 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 23.73 secs| Accuracy 98.884

Epoch: 4 Train Loss 0.037 Time taken: 597.36 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.79 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.026 Time taken: 597.84 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.72 secs| Accuracy 99.011
T\P      0       1
0       5253        0056        
1       0057        5251        

Accuracy 98.936
Time taken: 34.65 secs
3219.222 secs


On the best model
T\P      0       1
0       5253        0056        
1       0057        5251        

Accuracy 98.936
Time taken: 34.35 secs
Sat Apr 16 20:04:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=262
Sat Apr 16 20:04:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.032 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 262,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.575 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.266 Time taken: 596.22 secs

    Epoch: 1 Validation Loss 0.094 Time taken: 23.32 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.099 Time taken: 591.64 secs

    Epoch: 2 Validation Loss 0.075 Time taken: 23.36 secs| Accuracy 97.994

Epoch: 3 Train Loss 0.073 Time taken: 592.57 secs

    Epoch: 3 Validation Loss 0.074 Time taken: 23.62 secs| Accuracy 97.386

Epoch: 4 Train Loss 0.057 Time taken: 592.91 secs

    Epoch: 4 Validation Loss 0.042 Time taken: 23.72 secs| Accuracy 98.743

Epoch: 5 Train Loss 0.038 Time taken: 591.55 secs

    Epoch: 5 Validation Loss 0.063 Time taken: 23.53 secs| Accuracy 98.347
T\P      0       1
0       5211        0098        
1       0042        5266        

Accuracy 98.681
Time taken: 34.33 secs
3150.782 secs


On the best model
T\P      0       1
0       5230        0079        
1       0049        5259        

Accuracy 98.794
Time taken: 34.12 secs
Sat Apr 16 20:58:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=263
Sat Apr 16 20:58:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.943 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 263,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 132,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.625 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.208 Time taken: 597.5 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 23.36 secs| Accuracy 98.389

Epoch: 2 Train Loss 0.072 Time taken: 601.3 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 23.24 secs| Accuracy 98.785

Epoch: 3 Train Loss 0.045 Time taken: 589.22 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.28 secs| Accuracy 98.785

Epoch: 4 Train Loss 0.032 Time taken: 589.22 secs

    Epoch: 4 Validation Loss 0.044 Time taken: 23.31 secs| Accuracy 98.856

Epoch: 5 Train Loss 0.027 Time taken: 589.02 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 23.29 secs| Accuracy 99.181
T\P      0       1
0       5271        0038        
1       0062        5246        

Accuracy 99.058
Time taken: 33.99 secs
3157.269 secs


On the best model
T\P      0       1
0       5271        0038        
1       0062        5246        

Accuracy 99.058
Time taken: 33.87 secs
Sat Apr 16 21:53:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=264
Sat Apr 16 21:53:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 264,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 133,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.365 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 595.98 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 23.3 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.076 Time taken: 592.7 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.29 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.055 Time taken: 594.33 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 23.31 secs| Accuracy 98.375

Epoch: 4 Train Loss 0.042 Time taken: 593.65 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 23.41 secs| Accuracy 98.432

Epoch: 5 Train Loss 0.029 Time taken: 600.31 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 23.3 secs| Accuracy 98.785
T\P      0       1
0       5293        0016        
1       0105        5203        

Accuracy 98.86
Time taken: 34.02 secs
3169.397 secs


On the best model
T\P      0       1
0       5293        0016        
1       0105        5203        

Accuracy 98.86
Time taken: 33.92 secs
Sat Apr 16 22:47:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=265
Sat Apr 16 22:47:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 265,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 134,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.355 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.206 Time taken: 595.25 secs

    Epoch: 1 Validation Loss 0.048 Time taken: 23.45 secs| Accuracy 98.531

Epoch: 2 Train Loss 0.069 Time taken: 601.25 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 23.41 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.045 Time taken: 593.65 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.54 secs| Accuracy 98.771

Epoch: 4 Train Loss 0.031 Time taken: 600.98 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.46 secs| Accuracy 98.926

Epoch: 5 Train Loss 0.024 Time taken: 594.31 secs

    Epoch: 5 Validation Loss 0.056 Time taken: 23.49 secs| Accuracy 98.615
T\P      0       1
0       5216        0093        
1       0034        5274        

Accuracy 98.804
Time taken: 34.5 secs
3181.955 secs


On the best model
T\P      0       1
0       5248        0061        
1       0042        5266        

Accuracy 99.03
Time taken: 34.19 secs
Sat Apr 16 23:42:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=266
Sat Apr 16 23:42:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 266,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 135,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.365 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.207 Time taken: 596.72 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 23.54 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.063 Time taken: 602.74 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 23.33 secs| Accuracy 98.672

Epoch: 3 Train Loss 0.044 Time taken: 602.88 secs

    Epoch: 3 Validation Loss 0.029 Time taken: 23.28 secs| Accuracy 99.181

Epoch: 4 Train Loss 0.028 Time taken: 594.02 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.56 secs| Accuracy 98.856

Epoch: 5 Train Loss 0.023 Time taken: 595.24 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.38 secs| Accuracy 98.983
T\P      0       1
0       5252        0057        
1       0036        5272        

Accuracy 99.124
Time taken: 34.46 secs
3176.685 secs


On the best model
T\P      0       1
0       5283        0026        
1       0058        5250        

Accuracy 99.209
Time taken: 34.13 secs
Sun Apr 17 00:37:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=267
Sun Apr 17 00:37:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.033 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 267,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 136,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.216 Time taken: 596.1 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 23.18 secs| Accuracy 98.474

Epoch: 2 Train Loss 0.075 Time taken: 594.4 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 23.05 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.05 Time taken: 600.83 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 22.94 secs| Accuracy 99.068

Epoch: 4 Train Loss 0.036 Time taken: 592.71 secs

    Epoch: 4 Validation Loss 0.042 Time taken: 22.93 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.03 Time taken: 603.23 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 23.04 secs| Accuracy 99.181
T\P      0       1
0       5269        0040        
1       0052        5256        

Accuracy 99.133
Time taken: 33.65 secs
3175.489 secs


On the best model
T\P      0       1
0       5269        0040        
1       0052        5256        

Accuracy 99.133
Time taken: 33.87 secs
Sun Apr 17 01:31:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=268
Sun Apr 17 01:31:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.8 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 268,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 137,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.404 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.231 Time taken: 595.07 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 22.76 secs| Accuracy 97.81

Epoch: 2 Train Loss 0.077 Time taken: 602.82 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 22.73 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.054 Time taken: 605.46 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.78 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.039 Time taken: 594.98 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 22.82 secs| Accuracy 98.884

Epoch: 5 Train Loss 0.032 Time taken: 594.37 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 22.89 secs| Accuracy 98.912
T\P      0       1
0       5251        0058        
1       0048        5260        

Accuracy 99.002
Time taken: 33.49 secs
3198.03 secs


On the best model
T\P      0       1
0       5251        0058        
1       0048        5260        

Accuracy 99.002
Time taken: 33.49 secs
Sun Apr 17 02:26:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=269
Sun Apr 17 02:26:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 269,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 138,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.211 Time taken: 596.48 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 22.99 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.079 Time taken: 607.7 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.89 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.053 Time taken: 593.89 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 23.04 secs| Accuracy 98.644

Epoch: 4 Train Loss 0.036 Time taken: 594.05 secs

    Epoch: 4 Validation Loss 0.05 Time taken: 23.12 secs| Accuracy 98.234

Epoch: 5 Train Loss 0.027 Time taken: 595.79 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.14 secs| Accuracy 98.969
T\P      0       1
0       5245        0064        
1       0040        5268        

Accuracy 99.02
Time taken: 33.83 secs
3195.84 secs


On the best model
T\P      0       1
0       5245        0064        
1       0040        5268        

Accuracy 99.02
Time taken: 33.63 secs
Sun Apr 17 03:21:41 2022




======================================================================
----------------------------------------------------------------------
                run_ID=270
Sun Apr 17 11:48:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.081 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 270,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.517 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.178 Time taken: 598.35 secs

    Epoch: 1 Validation Loss 0.104 Time taken: 23.69 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.067 Time taken: 598.58 secs

    Epoch: 2 Validation Loss 0.047 Time taken: 23.55 secs| Accuracy 98.63

Epoch: 3 Train Loss 0.043 Time taken: 593.29 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 23.58 secs| Accuracy 98.206

Epoch: 4 Train Loss 0.031 Time taken: 593.33 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 23.7 secs| Accuracy 98.856

Epoch: 5 Train Loss 0.023 Time taken: 595.09 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.62 secs| Accuracy 98.955
T\P      0       1
0       5281        0028        
1       0078        5230        

Accuracy 99.002
Time taken: 34.43 secs
3131.803 secs



======================================================================
----------------------------------------------------------------------
                run_ID=271
Sun Apr 17 16:06:45 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.25 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 271,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.407 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 596.6 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 23.54 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.073 Time taken: 597.24 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.66 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.049 Time taken: 596.31 secs

    Epoch: 3 Validation Loss 0.034 Time taken: 23.54 secs| Accuracy 98.94

Epoch: 4 Train Loss 0.037 Time taken: 600.08 secs

    Epoch: 4 Validation Loss 0.03 Time taken: 23.42 secs| Accuracy 99.068

Epoch: 5 Train Loss 0.024 Time taken: 595.29 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 23.71 secs| Accuracy 98.785
T\P      0       1
0       5218        0091        
1       0030        5278        

Accuracy 98.86
Time taken: 34.56 secs
3138.398 secs

Sun Apr 17 17:00:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=272
Sun Apr 17 17:00:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.138 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 272,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 4.91 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.17 Time taken: 601.36 secs

    Epoch: 1 Validation Loss 0.048 Time taken: 23.58 secs| Accuracy 98.517

Epoch: 2 Train Loss 0.062 Time taken: 599.5 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 23.65 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.041 Time taken: 599.44 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 23.69 secs| Accuracy 98.644

Epoch: 4 Train Loss 0.028 Time taken: 599.37 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 23.61 secs| Accuracy 98.997

Epoch: 5 Train Loss 0.022 Time taken: 598.28 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.63 secs| Accuracy 98.997
T\P      0       1
0       5241        0068        
1       0038        5270        

Accuracy 99.002
Time taken: 34.54 secs
3151.244 secs

Sun Apr 17 17:53:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=273
Sun Apr 17 17:53:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.001 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 273,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 132,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM -6.315 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.182 Time taken: 602.13 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 23.94 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.067 Time taken: 598.78 secs

    Epoch: 2 Validation Loss 0.033 Time taken: 23.68 secs| Accuracy 98.898

Epoch: 3 Train Loss 0.042 Time taken: 598.72 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.79 secs| Accuracy 98.743

Epoch: 4 Train Loss 0.029 Time taken: 598.55 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 24.1 secs| Accuracy 99.152

Epoch: 5 Train Loss 0.02 Time taken: 599.14 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.59 secs| Accuracy 99.124
T\P      0       1
0       5270        0039        
1       0056        5252        

Accuracy 99.105
Time taken: 34.47 secs
3151.384 secs

Sun Apr 17 18:47:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=274
Sun Apr 17 18:47:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.081 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 274,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 133,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.661 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.207 Time taken: 602.08 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 23.47 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.074 Time taken: 607.66 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.54 secs| Accuracy 98.545

Epoch: 3 Train Loss 0.045 Time taken: 602.1 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.75 secs| Accuracy 98.87

Epoch: 4 Train Loss 0.032 Time taken: 599.82 secs

    Epoch: 4 Validation Loss 0.056 Time taken: 23.61 secs| Accuracy 98.644

Epoch: 5 Train Loss 0.026 Time taken: 604.0 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.5 secs| Accuracy 99.082
T\P      0       1
0       5268        0041        
1       0063        5245        

Accuracy 99.02
Time taken: 34.32 secs
3168.31 secs

Sun Apr 17 19:40:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=275
Sun Apr 17 19:41:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.867 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 275,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 134,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 9.106 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.161 Time taken: 600.19 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 23.73 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.058 Time taken: 603.88 secs

    Epoch: 2 Validation Loss 0.033 Time taken: 23.52 secs| Accuracy 99.053

Epoch: 3 Train Loss 0.038 Time taken: 607.22 secs

    Epoch: 3 Validation Loss 0.025 Time taken: 23.59 secs| Accuracy 99.195

Epoch: 4 Train Loss 0.026 Time taken: 597.53 secs

    Epoch: 4 Validation Loss 0.084 Time taken: 23.56 secs| Accuracy 97.485

Epoch: 5 Train Loss 0.018 Time taken: 604.63 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 23.36 secs| Accuracy 98.714
T\P      0       1
0       5212        0097        
1       0026        5282        

Accuracy 98.841
Time taken: 34.41 secs
3166.11 secs

Sun Apr 17 20:34:45 2022




======================================================================
----------------------------------------------------------------------
                run_ID=276
Sun Apr 17 20:53:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.363 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 276,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 135,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 7.163 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.164 Time taken: 600.84 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 23.41 secs| Accuracy 98.432

Epoch: 2 Train Loss 0.063 Time taken: 596.09 secs

    Epoch: 2 Validation Loss 0.034 Time taken: 23.93 secs| Accuracy 99.025

Epoch: 3 Train Loss 0.045 Time taken: 599.44 secs

    Epoch: 3 Validation Loss 0.033 Time taken: 23.83 secs| Accuracy 99.025

Epoch: 4 Train Loss 0.037 Time taken: 599.02 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.91 secs| Accuracy 98.827

Epoch: 5 Train Loss 0.028 Time taken: 599.09 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.89 secs| Accuracy 98.813
T\P      0       1
0       5260        0049        
1       0045        5263        

Accuracy 99.115
Time taken: 34.79 secs
3148.879 secs

Sun Apr 17 21:46:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=311
Sun Apr 17 21:46:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.459 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 311,
    "message": "EL0 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.355 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 580.61 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 23.43 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.069 Time taken: 586.71 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 23.31 secs| Accuracy 98.771

Epoch: 3 Train Loss 0.048 Time taken: 586.52 secs

    Epoch: 3 Validation Loss 0.03 Time taken: 23.24 secs| Accuracy 99.068

Epoch: 4 Train Loss 0.037 Time taken: 578.37 secs

    Epoch: 4 Validation Loss 0.091 Time taken: 23.52 secs| Accuracy 97.316

Epoch: 5 Train Loss 0.03 Time taken: 583.34 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.3 secs| Accuracy 99.082
T\P      0       1
0       5286        0023        
1       0082        5226        

Accuracy 99.011
Time taken: 34.13 secs
3066.911 secs

Sun Apr 17 22:39:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=312
Sun Apr 17 22:39:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.837 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 312,
    "message": "311 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 0.523 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 583.13 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 23.4 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.069 Time taken: 580.1 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.4 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.047 Time taken: 580.36 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.38 secs| Accuracy 98.29

Epoch: 4 Train Loss 0.035 Time taken: 578.55 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 23.36 secs| Accuracy 98.404

Epoch: 5 Train Loss 0.025 Time taken: 579.07 secs

    Epoch: 5 Validation Loss 0.055 Time taken: 23.33 secs| Accuracy 98.672
T\P      0       1
0       5292        0017        
1       0111        5197        

Accuracy 98.794
Time taken: 34.07 secs
3052.599 secs

Sun Apr 17 23:30:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=313
Sun Apr 17 23:30:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.872 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 313,
    "message": "311 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.39 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.171 Time taken: 578.75 secs

    Epoch: 1 Validation Loss 0.048 Time taken: 23.08 secs| Accuracy 98.488

Epoch: 2 Train Loss 0.062 Time taken: 575.24 secs

    Epoch: 2 Validation Loss 0.047 Time taken: 23.25 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.045 Time taken: 581.12 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 23.02 secs| Accuracy 98.955

Epoch: 4 Train Loss 0.033 Time taken: 575.25 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 23.17 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.023 Time taken: 577.77 secs

    Epoch: 5 Validation Loss 0.057 Time taken: 23.09 secs| Accuracy 98.531
T\P      0       1
0       5177        0132        
1       0030        5278        

Accuracy 98.474
Time taken: 33.89 secs
3037.978 secs

Mon Apr 18 00:22:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=314
Mon Apr 18 00:22:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.089 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 314,
    "message": "311 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 10.034 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.206 Time taken: 583.22 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 23.08 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.086 Time taken: 580.09 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.33 secs| Accuracy 98.361

Epoch: 3 Train Loss 0.064 Time taken: 585.15 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.11 secs| Accuracy 98.728

Epoch: 4 Train Loss 0.049 Time taken: 585.65 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 23.1 secs| Accuracy 98.502

Epoch: 5 Train Loss 0.037 Time taken: 578.12 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.06 secs| Accuracy 98.983
T\P      0       1
0       5254        0055        
1       0047        5261        

Accuracy 99.039
Time taken: 34.07 secs
3062.396 secs

Mon Apr 18 01:14:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=315
Mon Apr 18 01:14:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.995 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 315,
    "message": "311 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.322 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.155 Time taken: 578.6 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.11 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.071 Time taken: 582.98 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.08 secs| Accuracy 98.615

Epoch: 3 Train Loss 0.047 Time taken: 580.19 secs

    Epoch: 3 Validation Loss 0.054 Time taken: 23.07 secs| Accuracy 98.686

Epoch: 4 Train Loss 0.036 Time taken: 582.44 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 23.18 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.03 Time taken: 581.67 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 23.23 secs| Accuracy 98.728
T\P      0       1
0       5250        0059        
1       0042        5266        

Accuracy 99.049
Time taken: 34.12 secs
3056.241 secs

Mon Apr 18 02:06:42 2022


======================================================================
----------------------------------------------------------------------
                run_ID=316
Mon Apr 18 02:06:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.788 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 316,
    "message": "EL0 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.343 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.194 Time taken: 581.64 secs

    Epoch: 1 Validation Loss 0.049 Time taken: 23.2 secs| Accuracy 98.531

Epoch: 2 Train Loss 0.063 Time taken: 586.17 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 23.11 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.046 Time taken: 585.09 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 23.17 secs| Accuracy 98.94

Epoch: 4 Train Loss 0.031 Time taken: 579.57 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 23.28 secs| Accuracy 99.068

Epoch: 5 Train Loss 0.025 Time taken: 578.67 secs

    Epoch: 5 Validation Loss 0.026 Time taken: 23.66 secs| Accuracy 99.209
T\P      0       1
0       5239        0070        
1       0036        5272        

Accuracy 99.002
Time taken: 34.41 secs
3062.6 secs

Mon Apr 18 02:58:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=317
Mon Apr 18 02:58:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.999 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 317,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.391 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.218 Time taken: 581.25 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.14 secs| Accuracy 97.881

Epoch: 2 Train Loss 0.086 Time taken: 587.44 secs

    Epoch: 2 Validation Loss 0.078 Time taken: 23.08 secs| Accuracy 97.443

Epoch: 3 Train Loss 0.056 Time taken: 579.35 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 23.31 secs| Accuracy 98.573

Epoch: 4 Train Loss 0.041 Time taken: 580.18 secs

    Epoch: 4 Validation Loss 0.044 Time taken: 23.18 secs| Accuracy 98.672

Epoch: 5 Train Loss 0.032 Time taken: 577.38 secs

    Epoch: 5 Validation Loss 0.066 Time taken: 23.25 secs| Accuracy 97.853
T\P      0       1
0       5149        0160        
1       0033        5275        

Accuracy 98.182
Time taken: 34.39 secs
3056.375 secs

Mon Apr 18 03:50:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=318
Mon Apr 18 03:50:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.027 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 318,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.443 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.287 Time taken: 577.8 secs

    Epoch: 1 Validation Loss 0.175 Time taken: 23.15 secs| Accuracy 94.193

Epoch: 2 Train Loss 0.133 Time taken: 584.39 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 23.17 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.086 Time taken: 576.67 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.42 secs| Accuracy 98.361

Epoch: 4 Train Loss 0.06 Time taken: 577.11 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 23.26 secs| Accuracy 97.895

Epoch: 5 Train Loss 0.048 Time taken: 584.09 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 22.97 secs| Accuracy 98.559
T\P      0       1
0       5217        0092        
1       0042        5266        

Accuracy 98.738
Time taken: 33.77 secs
3050.219 secs

Mon Apr 18 04:42:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=319
Mon Apr 18 04:42:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.186 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 319,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.265 Time taken: 579.67 secs

    Epoch: 1 Validation Loss 0.116 Time taken: 23.12 secs| Accuracy 97.146

Epoch: 2 Train Loss 0.116 Time taken: 585.23 secs

    Epoch: 2 Validation Loss 0.07 Time taken: 23.02 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.082 Time taken: 586.71 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 23.12 secs| Accuracy 97.909

Epoch: 4 Train Loss 0.062 Time taken: 577.64 secs

    Epoch: 4 Validation Loss 0.053 Time taken: 23.35 secs| Accuracy 98.601

Epoch: 5 Train Loss 0.044 Time taken: 578.91 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.08 secs| Accuracy 99.025
T\P      0       1
0       5239        0070        
1       0041        5267        

Accuracy 98.955
Time taken: 33.92 secs
3058.237 secs

Mon Apr 18 05:34:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=320
Mon Apr 18 05:34:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.043 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 320,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.14 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.245 Time taken: 579.56 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 23.08 secs| Accuracy 97.796

Epoch: 2 Train Loss 0.092 Time taken: 577.61 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.06 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.065 Time taken: 577.5 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 22.99 secs| Accuracy 98.686

Epoch: 4 Train Loss 0.044 Time taken: 577.66 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 23.09 secs| Accuracy 98.262

Epoch: 5 Train Loss 0.036 Time taken: 577.72 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.08 secs| Accuracy 98.827
T\P      0       1
0       5276        0033        
1       0057        5251        

Accuracy 99.152
Time taken: 34.01 secs
3039.782 secs

Mon Apr 18 06:26:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=321
Mon Apr 18 06:26:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.079 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 321,
    "message": "EL0 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.341 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.285 Time taken: 581.55 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 23.09 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.088 Time taken: 587.34 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.1 secs| Accuracy 98.361

Epoch: 3 Train Loss 0.057 Time taken: 578.8 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.17 secs| Accuracy 98.912

Epoch: 4 Train Loss 0.043 Time taken: 578.45 secs

    Epoch: 4 Validation Loss 0.032 Time taken: 23.16 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.032 Time taken: 580.5 secs

    Epoch: 5 Validation Loss 0.029 Time taken: 23.08 secs| Accuracy 99.152
T\P      0       1
0       5249        0060        
1       0048        5260        

Accuracy 98.983
Time taken: 33.91 secs
3056.572 secs

Mon Apr 18 07:18:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=322
Mon Apr 18 07:18:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.387 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 322,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.5 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.293 Time taken: 583.76 secs

    Epoch: 1 Validation Loss 0.08 Time taken: 23.21 secs| Accuracy 97.33

Epoch: 2 Train Loss 0.111 Time taken: 586.84 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.17 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.081 Time taken: 590.31 secs

    Epoch: 3 Validation Loss 0.054 Time taken: 23.24 secs| Accuracy 98.446

Epoch: 4 Train Loss 0.063 Time taken: 578.73 secs

    Epoch: 4 Validation Loss 0.049 Time taken: 23.39 secs| Accuracy 98.517

Epoch: 5 Train Loss 0.049 Time taken: 582.19 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.13 secs| Accuracy 98.771
T\P      0       1
0       5255        0054        
1       0064        5244        

Accuracy 98.889
Time taken: 34.08 secs
3072.488 secs

Mon Apr 18 08:10:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=323
Mon Apr 18 08:10:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.898 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 323,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.341 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.274 Time taken: 581.78 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.16 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.083 Time taken: 578.69 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.36 secs| Accuracy 98.615

Epoch: 3 Train Loss 0.056 Time taken: 586.74 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.23 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.038 Time taken: 580.15 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 23.27 secs| Accuracy 98.827

Epoch: 5 Train Loss 0.03 Time taken: 578.44 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 23.31 secs| Accuracy 98.94
T\P      0       1
0       5284        0025        
1       0073        5235        

Accuracy 99.077
Time taken: 34.17 secs
3057.043 secs

Mon Apr 18 09:02:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=324
Mon Apr 18 10:04:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.231 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 324,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.345 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.231 Time taken: 582.7 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 23.27 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.071 Time taken: 580.16 secs

    Epoch: 2 Validation Loss 0.077 Time taken: 23.42 secs| Accuracy 97.626

Epoch: 3 Train Loss 0.05 Time taken: 580.5 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.31 secs| Accuracy 98.686

Epoch: 4 Train Loss 0.04 Time taken: 580.07 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 23.59 secs| Accuracy 98.841

Epoch: 5 Train Loss 0.026 Time taken: 576.66 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 23.68 secs| Accuracy 99.025
T\P      0       1
0       5263        0046        
1       0049        5259        

Accuracy 99.105
Time taken: 34.59 secs
3052.587 secs

Mon Apr 18 10:55:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=325
Mon Apr 18 10:56:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.274 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 325,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.437 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 581.94 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 23.48 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.095 Time taken: 576.47 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 23.51 secs| Accuracy 98.192

Epoch: 3 Train Loss 0.065 Time taken: 583.52 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 23.49 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.045 Time taken: 576.98 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.6 secs| Accuracy 98.743

Epoch: 5 Train Loss 0.035 Time taken: 578.08 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.59 secs| Accuracy 98.969
T\P      0       1
0       5254        0055        
1       0051        5257        

Accuracy 99.002
Time taken: 34.81 secs
3049.93 secs

Mon Apr 18 11:47:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=326
Mon Apr 18 11:48:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.272 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 326,
    "message": "EL0L1 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.152 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.164 Time taken: 573.06 secs

    Epoch: 1 Validation Loss 0.083 Time taken: 23.31 secs| Accuracy 97.344

Epoch: 2 Train Loss 0.066 Time taken: 577.02 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.42 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.045 Time taken: 579.9 secs

    Epoch: 3 Validation Loss 0.031 Time taken: 23.52 secs| Accuracy 99.025

Epoch: 4 Train Loss 0.032 Time taken: 570.4 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 23.49 secs| Accuracy 98.615

Epoch: 5 Train Loss 0.027 Time taken: 579.58 secs

    Epoch: 5 Validation Loss 0.031 Time taken: 23.27 secs| Accuracy 98.997
T\P      0       1
0       5286        0023        
1       0086        5222        

Accuracy 98.973
Time taken: 34.41 secs
3031.818 secs

Mon Apr 18 12:39:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=327
Mon Apr 18 12:39:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.114 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 327,
    "message": "326 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.4 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.243 Time taken: 573.17 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 23.39 secs| Accuracy 97.132

Epoch: 2 Train Loss 0.082 Time taken: 580.46 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 23.35 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.054 Time taken: 569.5 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.79 secs| Accuracy 98.87

Epoch: 4 Train Loss 0.043 Time taken: 568.37 secs

    Epoch: 4 Validation Loss 0.062 Time taken: 23.63 secs| Accuracy 97.612

Epoch: 5 Train Loss 0.029 Time taken: 569.56 secs

    Epoch: 5 Validation Loss 0.051 Time taken: 23.78 secs| Accuracy 98.672
T\P      0       1
0       5209        0100        
1       0033        5275        

Accuracy 98.747
Time taken: 34.64 secs
3014.237 secs

Mon Apr 18 13:30:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=328
Mon Apr 18 13:30:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.15 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 328,
    "message": "326 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.403 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.181 Time taken: 569.12 secs

    Epoch: 1 Validation Loss 0.048 Time taken: 23.18 secs| Accuracy 98.474

Epoch: 2 Train Loss 0.066 Time taken: 568.01 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 23.28 secs| Accuracy 98.573

Epoch: 3 Train Loss 0.048 Time taken: 567.87 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.2 secs| Accuracy 98.94

Epoch: 4 Train Loss 0.033 Time taken: 567.09 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.38 secs| Accuracy 98.898

Epoch: 5 Train Loss 0.024 Time taken: 567.83 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.68 secs| Accuracy 98.912
T\P      0       1
0       5245        0064        
1       0043        5265        

Accuracy 98.992
Time taken: 34.53 secs
2991.739 secs

Mon Apr 18 14:21:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=329
Mon Apr 18 14:21:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.192 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 329,
    "message": "326 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.394 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.212 Time taken: 573.7 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 23.32 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.068 Time taken: 577.5 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 23.18 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.049 Time taken: 581.2 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 23.18 secs| Accuracy 99.039

Epoch: 4 Train Loss 0.034 Time taken: 569.63 secs

    Epoch: 4 Validation Loss 0.03 Time taken: 23.25 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.026 Time taken: 567.77 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 23.17 secs| Accuracy 99.053
T\P      0       1
0       5250        0059        
1       0045        5263        

Accuracy 99.02
Time taken: 33.98 secs
3020.44 secs

Mon Apr 18 15:13:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=330
Mon Apr 18 15:13:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.915 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 330,
    "message": "326 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.443 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 559.97 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 22.82 secs| Accuracy 97.061

Epoch: 2 Train Loss 0.066 Time taken: 558.0 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 22.88 secs| Accuracy 98.841

Epoch: 3 Train Loss 0.047 Time taken: 561.19 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 23.0 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.033 Time taken: 561.59 secs

    Epoch: 4 Validation Loss 0.054 Time taken: 23.01 secs| Accuracy 98.898

Epoch: 5 Train Loss 0.023 Time taken: 571.16 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 22.99 secs| Accuracy 98.969
T\P      0       1
0       5284        0025        
1       0065        5243        

Accuracy 99.152
Time taken: 33.77 secs
2960.892 secs

Mon Apr 18 16:03:33 2022




======================================================================
----------------------------------------------------------------------
                run_ID=371
Mon Apr 18 20:56:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.27 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 371,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.298 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.218 Time taken: 578.91 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.22 secs| Accuracy 97.81

Epoch: 2 Train Loss 0.075 Time taken: 576.07 secs

    Epoch: 2 Validation Loss 0.099 Time taken: 23.39 secs| Accuracy 97.188

Epoch: 3 Train Loss 0.053 Time taken: 577.06 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 23.27 secs| Accuracy 97.909

Epoch: 4 Train Loss 0.038 Time taken: 576.87 secs

    Epoch: 4 Validation Loss 0.077 Time taken: 23.24 secs| Accuracy 98.276

Epoch: 5 Train Loss 0.033 Time taken: 577.11 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 23.31 secs| Accuracy 98.771
T\P      0       1
0       5242        0067        
1       0044        5264        

Accuracy 98.955
Time taken: 34.1 secs
3055.018 secs

Mon Apr 18 21:48:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=372
Mon Apr 18 21:48:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.065 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 372,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.454 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.212 Time taken: 578.9 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 23.21 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.068 Time taken: 578.3 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 23.27 secs| Accuracy 98.926

Epoch: 3 Train Loss 0.049 Time taken: 578.44 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 23.34 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.036 Time taken: 578.94 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.27 secs| Accuracy 99.223

Epoch: 5 Train Loss 0.026 Time taken: 577.87 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.28 secs| Accuracy 99.053
T\P      0       1
0       5248        0061        
1       0039        5269        

Accuracy 99.058
Time taken: 34.28 secs
3059.791 secs

Mon Apr 18 22:40:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=373
Mon Apr 18 22:40:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.142 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 373,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.33 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.225 Time taken: 586.19 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.26 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.083 Time taken: 602.7 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.09 secs| Accuracy 98.545

Epoch: 3 Train Loss 0.059 Time taken: 602.75 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.17 secs| Accuracy 98.757

Epoch: 4 Train Loss 0.046 Time taken: 577.77 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.31 secs| Accuracy 98.87

Epoch: 5 Train Loss 0.032 Time taken: 656.53 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 26.59 secs| Accuracy 98.87
T\P      0       1
0       5268        0041        
1       0052        5256        

Accuracy 99.124
Time taken: 39.22 secs
3206.554 secs

Mon Apr 18 23:35:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=374
Mon Apr 18 23:35:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.285 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 374,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.407 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 662.31 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 26.59 secs| Accuracy 98.192

Epoch: 2 Train Loss 0.08 Time taken: 666.14 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 26.92 secs| Accuracy 98.743

Epoch: 3 Train Loss 0.053 Time taken: 667.36 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 26.95 secs| Accuracy 98.036

Epoch: 4 Train Loss 0.043 Time taken: 664.04 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 27.01 secs| Accuracy 98.757

Epoch: 5 Train Loss 0.032 Time taken: 625.51 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 23.35 secs| Accuracy 98.983
T\P      0       1
0       5264        0045        
1       0063        5245        

Accuracy 98.983
Time taken: 34.43 secs
3470.881 secs

Tue Apr 19 00:34:18 2022


======================================================================
----------------------------------------------------------------------
                run_ID=375
Tue Apr 19 00:34:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.084 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 375,
    "message": "316 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.441 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 580.13 secs

    Epoch: 1 Validation Loss 0.138 Time taken: 23.71 secs| Accuracy 96.426

Epoch: 2 Train Loss 0.118 Time taken: 577.93 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 23.76 secs| Accuracy 97.584

Epoch: 3 Train Loss 0.076 Time taken: 584.97 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.77 secs| Accuracy 98.545

Epoch: 4 Train Loss 0.055 Time taken: 585.89 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 23.71 secs| Accuracy 98.856

Epoch: 5 Train Loss 0.038 Time taken: 580.16 secs

    Epoch: 5 Validation Loss 0.051 Time taken: 23.77 secs| Accuracy 98.601
T\P      0       1
0       5224        0085        
1       0037        5271        

Accuracy 98.851
Time taken: 34.9 secs
3082.913 secs

Tue Apr 19 01:26:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=376
Tue Apr 19 01:26:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 376,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.349 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 581.61 secs

    Epoch: 1 Validation Loss 0.082 Time taken: 23.49 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.106 Time taken: 578.97 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.45 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.07 Time taken: 578.97 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.58 secs| Accuracy 98.135

Epoch: 4 Train Loss 0.054 Time taken: 580.11 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 23.46 secs| Accuracy 98.799

Epoch: 5 Train Loss 0.041 Time taken: 581.03 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.49 secs| Accuracy 98.926
T\P      0       1
0       5282        0027        
1       0069        5239        

Accuracy 99.096
Time taken: 34.84 secs
3070.928 secs

Tue Apr 19 02:19:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=377
Tue Apr 19 02:19:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.159 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 377,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.333 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.247 Time taken: 584.66 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 23.38 secs| Accuracy 97.584

Epoch: 2 Train Loss 0.078 Time taken: 589.63 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.8 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.05 Time taken: 645.79 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 24.13 secs| Accuracy 98.601

Epoch: 4 Train Loss 0.036 Time taken: 670.17 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.86 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.027 Time taken: 680.7 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 24.38 secs| Accuracy 98.799
T\P      0       1
0       5267        0042        
1       0053        5255        

Accuracy 99.105
Time taken: 35.96 secs
3361.645 secs

Tue Apr 19 03:16:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=378
Tue Apr 19 03:16:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.995 GB
37 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 378,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.313 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.298 Time taken: 582.28 secs

    Epoch: 1 Validation Loss 0.08 Time taken: 24.05 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.111 Time taken: 664.8 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 24.19 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.077 Time taken: 593.5 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 22.88 secs| Accuracy 98.502

Epoch: 4 Train Loss 0.063 Time taken: 583.43 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 23.04 secs| Accuracy 98.587

Epoch: 5 Train Loss 0.047 Time taken: 579.2 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.09 secs| Accuracy 98.601
T\P      0       1
0       5294        0015        
1       0130        5178        

Accuracy 98.634
Time taken: 34.3 secs
3187.895 secs

Tue Apr 19 04:10:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=379
Tue Apr 19 04:10:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.805 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 379,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -3.536 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.3 Time taken: 592.24 secs

    Epoch: 1 Validation Loss 0.098 Time taken: 23.24 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.101 Time taken: 588.55 secs

    Epoch: 2 Validation Loss 0.067 Time taken: 23.25 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.084 Time taken: 593.05 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.57 secs| Accuracy 98.46

Epoch: 4 Train Loss 0.058 Time taken: 588.7 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 23.46 secs| Accuracy 98.63

Epoch: 5 Train Loss 0.042 Time taken: 627.47 secs

    Epoch: 5 Validation Loss 0.062 Time taken: 23.93 secs| Accuracy 98.305
T\P      0       1
0       5171        0138        
1       0029        5279        

Accuracy 98.427
Time taken: 35.28 secs
3160.396 secs

Tue Apr 19 05:04:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=380
Tue Apr 19 05:04:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.01 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 380,
    "message": "321 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.415 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.262 Time taken: 636.25 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 23.71 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.071 Time taken: 636.48 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 24.01 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.048 Time taken: 636.63 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.81 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.038 Time taken: 635.53 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 23.59 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.027 Time taken: 636.86 secs

    Epoch: 5 Validation Loss 0.075 Time taken: 23.8 secs| Accuracy 97.853
T\P      0       1
0       5121        0188        
1       0027        5281        

Accuracy 97.975
Time taken: 35.07 secs
3359.016 secs

Tue Apr 19 06:01:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=382
Tue Apr 19 06:01:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.133 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 382,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.2 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.274 Time taken: 659.93 secs

    Epoch: 1 Validation Loss 0.11 Time taken: 24.55 secs| Accuracy 97.547

Epoch: 2 Train Loss 0.115 Time taken: 650.85 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 24.24 secs| Accuracy 98.184

Epoch: 3 Train Loss 0.073 Time taken: 642.69 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 24.43 secs| Accuracy 98.442

Epoch: 4 Train Loss 0.056 Time taken: 633.74 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 24.44 secs| Accuracy 98.645

Epoch: 5 Train Loss 0.042 Time taken: 624.75 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 24.3 secs| Accuracy 98.93
T\P      0       1
0       5329        0073        
1       0152        5517        

Accuracy 97.968
Time taken: 36.15 secs
3405.177 secs

Tue Apr 19 06:59:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=381
Tue Apr 19 10:55:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 381,
    "message": "188 with include asha data with inshorts",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.431 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.222 Time taken: 625.08 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 25.0 secs| Accuracy 98.333

Epoch: 2 Train Loss 0.071 Time taken: 631.2 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 24.65 secs| Accuracy 98.564

Epoch: 3 Train Loss 0.057 Time taken: 632.35 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 24.73 secs| Accuracy 98.035

Epoch: 4 Train Loss 0.042 Time taken: 638.45 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 26.56 secs| Accuracy 98.862

Epoch: 5 Train Loss 0.025 Time taken: 607.43 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 24.26 secs| Accuracy 98.862
T\P      0       1
0       5286        0116        
1       0096        5573        

Accuracy 98.085
Time taken: 35.65 secs
3310.218 secs

Tue Apr 19 11:51:56 2022



======================================================================
----------------------------------------------------------------------
                run_ID=407
Tue Apr 19 12:59:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.769 GB
39 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 407,
    "message": "188 with include asha data after inshorts and E frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 13.646 GB
32 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.204 Time taken: 692.55 secs

    Epoch: 1 Validation Loss 0.049 Time taken: 23.46 secs| Accuracy 98.517

Epoch: 2 Train Loss 0.071 Time taken: 589.16 secs

    Epoch: 2 Validation Loss 0.083 Time taken: 22.29 secs| Accuracy 98.022

Epoch: 3 Train Loss 0.051 Time taken: 577.77 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 22.26 secs| Accuracy 98.107
T\P      0       1
0       5158        0151        
1       0038        5270        

Accuracy 98.22
Time taken: 33.2 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.587 Time taken: 381.56 secs

    Epoch: 4 Validation Loss 0.425 Time taken: 1.47 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.381 Time taken: 381.23 secs

    Epoch: 5 Validation Loss 0.368 Time taken: 1.42 secs| Accuracy 82.45

Epoch: 6 Train Loss 0.294 Time taken: 376.52 secs

    Epoch: 6 Validation Loss 0.301 Time taken: 1.44 secs| Accuracy 86.093
T\P      0       1
0       0148        0050        
1       0024        0232        

Accuracy 83.7
Time taken: 1.92 secs
3130.401 secs

Tue Apr 19 13:52:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=408
Tue Apr 19 13:53:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.585 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 408,
    "message": "407 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.204 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.215 Time taken: 581.91 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 22.45 secs| Accuracy 98.389

Epoch: 2 Train Loss 0.07 Time taken: 581.24 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.41 secs| Accuracy 98.827

Epoch: 3 Train Loss 0.046 Time taken: 587.21 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 22.29 secs| Accuracy 98.926
T\P      0       1
0       5263        0046        
1       0058        5250        

Accuracy 99.02
Time taken: 33.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.651 Time taken: 376.36 secs

    Epoch: 4 Validation Loss 0.488 Time taken: 1.47 secs| Accuracy 80.132

Epoch: 5 Train Loss 0.479 Time taken: 384.82 secs

    Epoch: 5 Validation Loss 0.467 Time taken: 1.74 secs| Accuracy 77.483

Epoch: 6 Train Loss 0.415 Time taken: 461.89 secs

    Epoch: 6 Validation Loss 0.416 Time taken: 1.99 secs| Accuracy 84.437
T\P      0       1
0       0166        0032        
1       0052        0204        

Accuracy 81.498
Time taken: 2.37 secs
3106.343 secs

Tue Apr 19 14:45:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=409
Tue Apr 19 14:45:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 10.451 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 409,
    "message": "407 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.388 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.215 Time taken: 725.85 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 24.2 secs| Accuracy 98.135

Epoch: 2 Train Loss 0.077 Time taken: 757.92 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 24.35 secs| Accuracy 98.474

Epoch: 3 Train Loss 0.054 Time taken: 767.66 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.94 secs| Accuracy 98.827
T\P      0       1
0       5274        0035        
1       0070        5238        

Accuracy 99.011
Time taken: 36.09 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.602 Time taken: 534.13 secs

    Epoch: 4 Validation Loss 0.482 Time taken: 2.12 secs| Accuracy 74.834

Epoch: 5 Train Loss 0.379 Time taken: 544.64 secs

    Epoch: 5 Validation Loss 0.42 Time taken: 1.88 secs| Accuracy 83.444

Epoch: 6 Train Loss 0.252 Time taken: 387.39 secs

    Epoch: 6 Validation Loss 0.288 Time taken: 1.41 secs| Accuracy 88.742
T\P      0       1
0       0168        0030        
1       0027        0229        

Accuracy 87.445
Time taken: 1.9 secs
3861.113 secs

Tue Apr 19 15:51:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=410
Tue Apr 19 15:51:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.068 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 410,
    "message": "407 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.385 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.199 Time taken: 584.5 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 22.28 secs| Accuracy 97.584

Epoch: 2 Train Loss 0.073 Time taken: 585.94 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 22.31 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.047 Time taken: 589.9 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.33 secs| Accuracy 98.799
T\P      0       1
0       5288        0021        
1       0090        5218        

Accuracy 98.955
Time taken: 33.31 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.571 Time taken: 381.71 secs

    Epoch: 4 Validation Loss 0.429 Time taken: 1.44 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.355 Time taken: 377.63 secs

    Epoch: 5 Validation Loss 0.41 Time taken: 1.49 secs| Accuracy 82.45

Epoch: 6 Train Loss 0.282 Time taken: 377.72 secs

    Epoch: 6 Validation Loss 0.32 Time taken: 1.41 secs| Accuracy 86.424
T\P      0       1
0       0175        0023        
1       0029        0227        

Accuracy 88.546
Time taken: 1.85 secs
3028.872 secs

Tue Apr 19 16:42:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=411
Tue Apr 19 16:42:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.06 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 411,
    "message": "407 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.357 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.236 Time taken: 582.69 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.26 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.081 Time taken: 580.08 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.32 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.05 Time taken: 581.83 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.43 secs| Accuracy 98.714
T\P      0       1
0       5219        0090        
1       0041        5267        

Accuracy 98.766
Time taken: 33.37 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.574 Time taken: 380.33 secs

    Epoch: 4 Validation Loss 0.428 Time taken: 1.48 secs| Accuracy 79.47

Epoch: 5 Train Loss 0.338 Time taken: 383.21 secs

    Epoch: 5 Validation Loss 0.3 Time taken: 1.47 secs| Accuracy 85.762

Epoch: 6 Train Loss 0.223 Time taken: 376.06 secs

    Epoch: 6 Validation Loss 0.255 Time taken: 1.44 secs| Accuracy 87.417
T\P      0       1
0       0166        0032        
1       0035        0221        

Accuracy 85.242
Time taken: 1.86 secs
3014.453 secs

Tue Apr 19 17:34:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=412
Tue Apr 19 17:34:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 412,
    "message": "188 with include asha data after inshorts and EL0L1 frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 2.087 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 566.66 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 22.32 secs| Accuracy 98.192

Epoch: 2 Train Loss 0.075 Time taken: 573.11 secs

    Epoch: 2 Validation Loss 0.079 Time taken: 22.44 secs| Accuracy 97.853

Epoch: 3 Train Loss 0.055 Time taken: 572.69 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 22.38 secs| Accuracy 97.655
T\P      0       1
0       5117        0192        
1       0038        5270        

Accuracy 97.834
Time taken: 33.33 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.643 Time taken: 380.86 secs

    Epoch: 4 Validation Loss 0.467 Time taken: 1.49 secs| Accuracy 78.477

Epoch: 5 Train Loss 0.417 Time taken: 374.99 secs

    Epoch: 5 Validation Loss 0.379 Time taken: 1.45 secs| Accuracy 83.444

Epoch: 6 Train Loss 0.272 Time taken: 374.68 secs

    Epoch: 6 Validation Loss 0.28 Time taken: 1.44 secs| Accuracy 86.424
T\P      0       1
0       0156        0042        
1       0021        0235        

Accuracy 86.123
Time taken: 1.91 secs
2974.774 secs

Tue Apr 19 18:24:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=413
Tue Apr 19 18:24:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.01 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 413,
    "message": "412 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.267 Time taken: 564.69 secs

    Epoch: 1 Validation Loss 0.11 Time taken: 22.31 secs| Accuracy 97.612

Epoch: 2 Train Loss 0.108 Time taken: 566.87 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.34 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.058 Time taken: 561.84 secs

    Epoch: 3 Validation Loss 0.034 Time taken: 22.38 secs| Accuracy 98.884
T\P      0       1
0       5286        0023        
1       0076        5232        

Accuracy 99.068
Time taken: 33.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.644 Time taken: 380.11 secs

    Epoch: 4 Validation Loss 0.466 Time taken: 1.59 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.433 Time taken: 373.53 secs

    Epoch: 5 Validation Loss 0.416 Time taken: 1.57 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.359 Time taken: 373.79 secs

    Epoch: 6 Validation Loss 0.334 Time taken: 1.54 secs| Accuracy 85.43
T\P      0       1
0       0149        0049        
1       0028        0228        

Accuracy 83.04
Time taken: 1.97 secs
2952.124 secs

Tue Apr 19 19:14:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=414
Tue Apr 19 19:15:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 414,
    "message": "412 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 561.4 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 22.12 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.076 Time taken: 565.86 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.09 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.056 Time taken: 558.99 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 22.31 secs| Accuracy 98.757
T\P      0       1
0       5265        0044        
1       0065        5243        

Accuracy 98.973
Time taken: 33.11 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.669 Time taken: 372.49 secs

    Epoch: 4 Validation Loss 0.483 Time taken: 1.56 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.408 Time taken: 372.27 secs

    Epoch: 5 Validation Loss 0.458 Time taken: 1.48 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.288 Time taken: 371.7 secs

    Epoch: 6 Validation Loss 0.299 Time taken: 1.44 secs| Accuracy 86.755
T\P      0       1
0       0168        0030        
1       0029        0227        

Accuracy 87.004
Time taken: 1.91 secs
2932.56 secs

Tue Apr 19 20:04:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=431
Tue Apr 19 21:05:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.205 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 431,
    "message": "188 with include asha data after inshorts and E frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.497 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.304 Time taken: 581.49 secs

    Epoch: 1 Validation Loss 0.097 Time taken: 22.46 secs| Accuracy 96.666

Epoch: 2 Train Loss 0.1 Time taken: 586.22 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.38 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.066 Time taken: 581.91 secs

    Epoch: 3 Validation Loss 0.068 Time taken: 22.44 secs| Accuracy 98.093
T\P      0       1
0       5173        0136        
1       0038        5270        

Accuracy 98.361
Time taken: 33.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.583 Time taken: 384.37 secs

    Epoch: 4 Validation Loss 0.449 Time taken: 1.46 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.407 Time taken: 377.18 secs

    Epoch: 5 Validation Loss 0.356 Time taken: 1.5 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.287 Time taken: 376.76 secs

    Epoch: 6 Validation Loss 0.303 Time taken: 1.51 secs| Accuracy 88.079
T\P      0       1
0       0176        0022        
1       0054        0202        

Accuracy 83.26
Time taken: 1.96 secs
3018.621 secs

Tue Apr 19 21:56:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=432
Tue Apr 19 21:56:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.824 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 432,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.354 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.224 Time taken: 582.15 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.39 secs| Accuracy 98.234

Epoch: 2 Train Loss 0.071 Time taken: 586.13 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 22.4 secs| Accuracy 98.517

Epoch: 3 Train Loss 0.049 Time taken: 581.75 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.48 secs| Accuracy 98.926
T\P      0       1
0       5273        0036        
1       0064        5244        

Accuracy 99.058
Time taken: 33.46 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.676 Time taken: 381.34 secs

    Epoch: 4 Validation Loss 0.456 Time taken: 1.44 secs| Accuracy 82.781

Epoch: 5 Train Loss 0.434 Time taken: 385.24 secs

    Epoch: 5 Validation Loss 0.326 Time taken: 1.47 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.302 Time taken: 376.32 secs

    Epoch: 6 Validation Loss 0.29 Time taken: 1.52 secs| Accuracy 89.735
T\P      0       1
0       0176        0022        
1       0044        0212        

Accuracy 85.463
Time taken: 1.89 secs
3023.596 secs

Tue Apr 19 22:48:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=433
Tue Apr 19 22:48:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.05 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 433,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.302 Time taken: 583.08 secs

    Epoch: 1 Validation Loss 0.094 Time taken: 22.38 secs| Accuracy 97.768

Epoch: 2 Train Loss 0.116 Time taken: 585.19 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 22.36 secs| Accuracy 98.064

Epoch: 3 Train Loss 0.084 Time taken: 592.18 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.35 secs| Accuracy 98.319
T\P      0       1
0       5278        0031        
1       0134        5174        

Accuracy 98.446
Time taken: 33.38 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.56 Time taken: 375.61 secs

    Epoch: 4 Validation Loss 0.495 Time taken: 1.52 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.425 Time taken: 375.7 secs

    Epoch: 5 Validation Loss 0.432 Time taken: 1.5 secs| Accuracy 80.795

Epoch: 6 Train Loss 0.371 Time taken: 380.01 secs

    Epoch: 6 Validation Loss 0.421 Time taken: 1.51 secs| Accuracy 82.781
T\P      0       1
0       0163        0035        
1       0035        0221        

Accuracy 84.581
Time taken: 1.92 secs
3022.249 secs

Tue Apr 19 23:39:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=434
Tue Apr 19 23:39:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.053 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 434,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.304 Time taken: 583.53 secs

    Epoch: 1 Validation Loss 0.117 Time taken: 22.34 secs| Accuracy 94.448

Epoch: 2 Train Loss 0.114 Time taken: 582.58 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.82 secs| Accuracy 98.008

Epoch: 3 Train Loss 0.074 Time taken: 588.69 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.54 secs| Accuracy 98.615
T\P      0       1
0       5232        0077        
1       0071        5237        

Accuracy 98.606
Time taken: 33.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.534 Time taken: 381.95 secs

    Epoch: 4 Validation Loss 0.41 Time taken: 1.57 secs| Accuracy 82.45

Epoch: 5 Train Loss 0.374 Time taken: 383.72 secs

    Epoch: 5 Validation Loss 0.366 Time taken: 1.57 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.31 Time taken: 380.94 secs

    Epoch: 6 Validation Loss 0.331 Time taken: 1.57 secs| Accuracy 85.762
T\P      0       1
0       0183        0015        
1       0052        0204        

Accuracy 85.242
Time taken: 2.06 secs
3033.773 secs

Wed Apr 20 00:31:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=435
Wed Apr 20 00:31:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 435,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.315 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.264 Time taken: 583.56 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.39 secs| Accuracy 98.361

Epoch: 2 Train Loss 0.081 Time taken: 588.32 secs

    Epoch: 2 Validation Loss 0.07 Time taken: 22.33 secs| Accuracy 97.796

Epoch: 3 Train Loss 0.065 Time taken: 580.37 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.38 secs| Accuracy 98.63
T\P      0       1
0       5281        0028        
1       0082        5226        

Accuracy 98.964
Time taken: 33.33 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.607 Time taken: 380.96 secs

    Epoch: 4 Validation Loss 0.499 Time taken: 1.47 secs| Accuracy 78.477

Epoch: 5 Train Loss 0.4 Time taken: 375.29 secs

    Epoch: 5 Validation Loss 0.447 Time taken: 1.54 secs| Accuracy 82.781

Epoch: 6 Train Loss 0.309 Time taken: 375.28 secs

    Epoch: 6 Validation Loss 0.287 Time taken: 1.46 secs| Accuracy 87.086
T\P      0       1
0       0181        0017        
1       0067        0189        

Accuracy 81.498
Time taken: 1.88 secs
3013.793 secs

Wed Apr 20 01:22:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=436
Wed Apr 20 01:22:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 436,
    "message": "188 with include asha data after inshorts and EL0L1 frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 24.174 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.301 Time taken: 656.0 secs

    Epoch: 1 Validation Loss 0.095 Time taken: 23.53 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.102 Time taken: 683.76 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 23.62 secs| Accuracy 97.655

Epoch: 3 Train Loss 0.066 Time taken: 671.67 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 23.46 secs| Accuracy 97.994
T\P      0       1
0       5182        0127        
1       0043        5265        

Accuracy 98.399
Time taken: 34.69 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.663 Time taken: 482.89 secs

    Epoch: 4 Validation Loss 0.465 Time taken: 1.56 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.408 Time taken: 488.38 secs

    Epoch: 5 Validation Loss 0.373 Time taken: 1.79 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.281 Time taken: 464.08 secs

    Epoch: 6 Validation Loss 0.286 Time taken: 1.71 secs| Accuracy 87.086
T\P      0       1
0       0157        0041        
1       0026        0230        

Accuracy 85.242
Time taken: 1.97 secs
3586.149 secs

Wed Apr 20 02:23:24 2022


======================================================================
----------------------------------------------------------------------
                run_ID=437
Wed Apr 20 02:23:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -1.277 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 437,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.604 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.228 Time taken: 679.07 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 22.86 secs| Accuracy 98.22

Epoch: 2 Train Loss 0.065 Time taken: 671.19 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 23.3 secs| Accuracy 98.771

Epoch: 3 Train Loss 0.042 Time taken: 668.82 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.0 secs| Accuracy 98.94
T\P      0       1
0       5254        0055        
1       0058        5250        

Accuracy 98.936
Time taken: 34.89 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.643 Time taken: 470.5 secs

    Epoch: 4 Validation Loss 0.413 Time taken: 1.44 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.366 Time taken: 468.09 secs

    Epoch: 5 Validation Loss 0.335 Time taken: 1.59 secs| Accuracy 86.093

Epoch: 6 Train Loss 0.292 Time taken: 466.06 secs

    Epoch: 6 Validation Loss 0.308 Time taken: 1.51 secs| Accuracy 86.093
T\P      0       1
0       0179        0019        
1       0056        0200        

Accuracy 83.48
Time taken: 2.18 secs
3558.737 secs

Wed Apr 20 03:24:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=438
Wed Apr 20 03:24:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.545 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 438,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.314 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.295 Time taken: 674.11 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 23.49 secs| Accuracy 97.415

Epoch: 2 Train Loss 0.097 Time taken: 680.51 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 23.1 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.076 Time taken: 673.09 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.57 secs| Accuracy 98.531
T\P      0       1
0       5237        0072        
1       0055        5253        

Accuracy 98.804
Time taken: 34.91 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.637 Time taken: 480.87 secs

    Epoch: 4 Validation Loss 0.53 Time taken: 1.68 secs| Accuracy 72.517

Epoch: 5 Train Loss 0.415 Time taken: 473.74 secs

    Epoch: 5 Validation Loss 0.45 Time taken: 2.08 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.31 Time taken: 478.59 secs

    Epoch: 6 Validation Loss 0.372 Time taken: 1.91 secs| Accuracy 82.45
T\P      0       1
0       0140        0058        
1       0015        0241        

Accuracy 83.921
Time taken: 2.27 secs
3600.467 secs

Wed Apr 20 04:25:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=455
Wed Apr 20 12:07:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.25 GB
26 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 455,
    "message": "just using asha_sentsim data",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 9.554 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.687 Time taken: 470.54 secs

    Epoch: 1 Validation Loss 0.659 Time taken: 1.56 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.672 Time taken: 469.17 secs

    Epoch: 2 Validation Loss 0.685 Time taken: 1.99 secs| Accuracy 58.94

Epoch: 3 Train Loss 0.672 Time taken: 464.36 secs

    Epoch: 3 Validation Loss 0.671 Time taken: 1.73 secs| Accuracy 58.94

Epoch: 4 Train Loss 0.663 Time taken: 452.12 secs

    Epoch: 4 Validation Loss 0.654 Time taken: 1.91 secs| Accuracy 59.272

Epoch: 5 Train Loss 0.586 Time taken: 444.69 secs

    Epoch: 5 Validation Loss 0.505 Time taken: 1.82 secs| Accuracy 78.808
T\P      0       1
0       0111        0087        
1       0000        0256        

Accuracy 80.837
Time taken: 2.01 secs
2311.91 secs



======================================================================
----------------------------------------------------------------------
                run_ID=456
Wed Apr 20 13:15:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.191 GB
24 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 456,
    "message": "455 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.795 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.685 Time taken: 465.51 secs

    Epoch: 1 Validation Loss 0.679 Time taken: 1.39 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.663 Time taken: 473.01 secs

    Epoch: 2 Validation Loss 0.663 Time taken: 1.51 secs| Accuracy 58.94

Epoch: 3 Train Loss 0.623 Time taken: 473.04 secs

    Epoch: 3 Validation Loss 0.587 Time taken: 1.39 secs| Accuracy 58.94

Epoch: 4 Train Loss 0.557 Time taken: 463.11 secs

    Epoch: 4 Validation Loss 0.466 Time taken: 1.69 secs| Accuracy 81.457

Epoch: 5 Train Loss 0.466 Time taken: 464.38 secs

    Epoch: 5 Validation Loss 0.431 Time taken: 1.44 secs| Accuracy 83.113
T\P      0       1
0       0185        0013        
1       0073        0183        

Accuracy 81.057
Time taken: 1.96 secs
2348.446 secs

Wed Apr 20 13:55:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=457
Wed Apr 20 13:55:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.507 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 457,
    "message": "455 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.264 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.697 Time taken: 472.2 secs

    Epoch: 1 Validation Loss 0.677 Time taken: 1.66 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.667 Time taken: 461.39 secs

    Epoch: 2 Validation Loss 0.634 Time taken: 1.44 secs| Accuracy 67.881

Epoch: 3 Train Loss 0.589 Time taken: 459.28 secs

    Epoch: 3 Validation Loss 0.558 Time taken: 1.48 secs| Accuracy 78.808

Epoch: 4 Train Loss 0.531 Time taken: 456.18 secs

    Epoch: 4 Validation Loss 0.522 Time taken: 1.46 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.505 Time taken: 464.15 secs

    Epoch: 5 Validation Loss 0.467 Time taken: 1.78 secs| Accuracy 78.477
T\P      0       1
0       0112        0086        
1       0007        0249        

Accuracy 79.515
Time taken: 2.04 secs
2323.081 secs

Wed Apr 20 14:35:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=464
Wed Apr 20 16:48:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.454 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 464,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 7.101 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.224 Time taken: 675.36 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.3 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.08 Time taken: 690.03 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.9 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.054 Time taken: 674.11 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.18 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.039 Time taken: 675.04 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 23.33 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.031 Time taken: 678.72 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.09 secs| Accuracy 98.969
T\P      0       1
0       5278        0031        
1       0074        5234        

Accuracy 99.011
Time taken: 34.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.547 Time taken: 459.07 secs

    Epoch: 6 Validation Loss 0.421 Time taken: 1.5 secs| Accuracy 80.464

Epoch: 7 Train Loss 0.38 Time taken: 451.2 secs

    Epoch: 7 Validation Loss 0.436 Time taken: 1.75 secs| Accuracy 83.113
T\P      0       1
0       0131        0067        
1       0025        0231        

Accuracy 79.736
Time taken: 1.95 secs
4516.146 secs

Wed Apr 20 18:05:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=465
Wed Apr 20 18:05:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.664 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 465,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.266 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.214 Time taken: 690.69 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.46 secs| Accuracy 98.333

Epoch: 2 Train Loss 0.076 Time taken: 685.0 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 23.61 secs| Accuracy 97.853

Epoch: 3 Train Loss 0.052 Time taken: 695.29 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 23.4 secs| Accuracy 98.206

Epoch: 4 Train Loss 0.039 Time taken: 685.64 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 23.51 secs| Accuracy 98.573

Epoch: 5 Train Loss 0.031 Time taken: 684.36 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 23.48 secs| Accuracy 98.955
T\P      0       1
0       5261        0048        
1       0048        5260        

Accuracy 99.096
Time taken: 34.93 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.594 Time taken: 469.45 secs

    Epoch: 6 Validation Loss 0.4 Time taken: 1.87 secs| Accuracy 80.795

Epoch: 7 Train Loss 0.345 Time taken: 474.33 secs

    Epoch: 7 Validation Loss 0.287 Time taken: 1.73 secs| Accuracy 87.417
T\P      0       1
0       0164        0034        
1       0040        0216        

Accuracy 83.7
Time taken: 2.27 secs
4588.639 secs

Wed Apr 20 19:22:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=466
Wed Apr 20 19:23:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.289 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 466,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.235 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.246 Time taken: 673.8 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 23.2 secs| Accuracy 97.711

Epoch: 2 Train Loss 0.086 Time taken: 678.75 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 23.13 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.055 Time taken: 682.77 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.7 secs| Accuracy 98.728

Epoch: 4 Train Loss 0.039 Time taken: 670.61 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 23.09 secs| Accuracy 98.545

Epoch: 5 Train Loss 0.03 Time taken: 675.53 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 23.22 secs| Accuracy 98.955
T\P      0       1
0       5247        0062        
1       0029        5279        

Accuracy 99.143
Time taken: 34.2 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.726 Time taken: 470.18 secs

    Epoch: 6 Validation Loss 0.506 Time taken: 1.48 secs| Accuracy 81.457

Epoch: 7 Train Loss 0.412 Time taken: 454.63 secs

    Epoch: 7 Validation Loss 0.374 Time taken: 1.76 secs| Accuracy 83.775
T\P      0       1
0       0180        0018        
1       0073        0183        

Accuracy 79.956
Time taken: 1.93 secs
4508.236 secs

Wed Apr 20 20:39:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=467
Wed Apr 20 20:39:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.084 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 467,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.048 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.208 Time taken: 665.36 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.3 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.074 Time taken: 705.37 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 22.85 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.047 Time taken: 699.28 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.59 secs| Accuracy 98.672

Epoch: 4 Train Loss 0.039 Time taken: 702.65 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 23.19 secs| Accuracy 99.082

Epoch: 5 Train Loss 0.026 Time taken: 693.07 secs

    Epoch: 5 Validation Loss 0.049 Time taken: 23.85 secs| Accuracy 98.87
T\P      0       1
0       5258        0051        
1       0045        5263        

Accuracy 99.096
Time taken: 35.7 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.649 Time taken: 446.65 secs

    Epoch: 6 Validation Loss 0.469 Time taken: 1.5 secs| Accuracy 76.49

Epoch: 7 Train Loss 0.353 Time taken: 465.45 secs

    Epoch: 7 Validation Loss 0.411 Time taken: 2.04 secs| Accuracy 82.119
T\P      0       1
0       0176        0022        
1       0059        0197        

Accuracy 82.159
Time taken: 2.33 secs
4581.639 secs

Wed Apr 20 21:56:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=468
Wed Apr 20 21:56:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.955 GB
34 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 468,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.724 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.199 Time taken: 725.82 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 23.64 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.077 Time taken: 715.77 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 24.34 secs| Accuracy 98.573

Epoch: 3 Train Loss 0.05 Time taken: 717.86 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 23.54 secs| Accuracy 98.884

Epoch: 4 Train Loss 0.037 Time taken: 707.23 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 24.3 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.026 Time taken: 700.38 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.81 secs| Accuracy 99.011
T\P      0       1
0       5253        0056        
1       0057        5251        

Accuracy 98.936
Time taken: 35.43 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.598 Time taken: 490.02 secs

    Epoch: 6 Validation Loss 0.421 Time taken: 1.66 secs| Accuracy 82.781

Epoch: 7 Train Loss 0.346 Time taken: 477.82 secs

    Epoch: 7 Validation Loss 0.414 Time taken: 1.91 secs| Accuracy 81.457
T\P      0       1
0       0150        0048        
1       0025        0231        

Accuracy 83.921
Time taken: 2.27 secs
4753.572 secs

Wed Apr 20 23:17:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=469
Wed Apr 20 23:17:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.989 GB
36 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 469,
    "message": "fc3 after inshorts EL0L1 frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.041 GB
28 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.264 Time taken: 696.09 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 24.01 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.072 Time taken: 678.72 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 23.87 secs| Accuracy 98.517

Epoch: 3 Train Loss 0.049 Time taken: 674.91 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.43 secs| Accuracy 98.432
T\P      0       1
0       5190        0119        
1       0034        5274        

Accuracy 98.559
Time taken: 33.44 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.727 Time taken: 380.07 secs

    Epoch: 4 Validation Loss 0.515 Time taken: 1.55 secs| Accuracy 75.828

Epoch: 5 Train Loss 0.45 Time taken: 375.49 secs

    Epoch: 5 Validation Loss 0.48 Time taken: 1.46 secs| Accuracy 81.788
T\P      0       1
0       0138        0060        
1       0036        0220        

Accuracy 78.855
Time taken: 1.94 secs
2938.541 secs

Thu Apr 21 00:07:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=470
Thu Apr 21 00:07:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 470,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.362 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.225 Time taken: 569.14 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 22.43 secs| Accuracy 98.474

Epoch: 2 Train Loss 0.075 Time taken: 570.66 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.31 secs| Accuracy 98.7

Epoch: 3 Train Loss 0.05 Time taken: 564.41 secs

    Epoch: 3 Validation Loss 0.033 Time taken: 22.35 secs| Accuracy 98.94
T\P      0       1
0       5287        0022        
1       0098        5210        

Accuracy 98.87
Time taken: 33.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.55 Time taken: 375.01 secs

    Epoch: 4 Validation Loss 0.442 Time taken: 1.41 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.384 Time taken: 375.11 secs

    Epoch: 5 Validation Loss 0.379 Time taken: 1.44 secs| Accuracy 83.775
T\P      0       1
0       0178        0020        
1       0052        0204        

Accuracy 84.141
Time taken: 1.88 secs
2582.862 secs

Thu Apr 21 00:51:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=471
Thu Apr 21 00:51:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.065 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 471,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.358 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.271 Time taken: 565.46 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 22.25 secs| Accuracy 98.107

Epoch: 2 Train Loss 0.081 Time taken: 568.04 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 22.25 secs| Accuracy 98.064

Epoch: 3 Train Loss 0.059 Time taken: 581.28 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.5 secs| Accuracy 98.29
T\P      0       1
0       5180        0129        
1       0039        5269        

Accuracy 98.418
Time taken: 33.84 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.65 Time taken: 451.1 secs

    Epoch: 4 Validation Loss 0.573 Time taken: 1.69 secs| Accuracy 66.887

Epoch: 5 Train Loss 0.449 Time taken: 414.41 secs

    Epoch: 5 Validation Loss 0.437 Time taken: 1.47 secs| Accuracy 79.801
T\P      0       1
0       0106        0092        
1       0015        0241        

Accuracy 76.432
Time taken: 2.08 secs
2709.894 secs

Thu Apr 21 01:37:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=472
Thu Apr 21 01:37:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.169 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 472,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 11.974 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.279 Time taken: 633.83 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.3 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.085 Time taken: 563.13 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.42 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.054 Time taken: 568.06 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 22.37 secs| Accuracy 98.997
T\P      0       1
0       5281        0028        
1       0067        5241        

Accuracy 99.105
Time taken: 33.36 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.602 Time taken: 378.17 secs

    Epoch: 4 Validation Loss 0.399 Time taken: 1.41 secs| Accuracy 81.457

Epoch: 5 Train Loss 0.366 Time taken: 374.45 secs

    Epoch: 5 Validation Loss 0.342 Time taken: 1.48 secs| Accuracy 84.106
T\P      0       1
0       0178        0020        
1       0080        0176        

Accuracy 77.974
Time taken: 2.01 secs
2646.352 secs

Thu Apr 21 02:23:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=473
Thu Apr 21 02:23:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.07 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 473,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.37 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 567.18 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 22.42 secs| Accuracy 98.446

Epoch: 2 Train Loss 0.07 Time taken: 571.12 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.42 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.045 Time taken: 568.89 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 22.42 secs| Accuracy 98.997
T\P      0       1
0       5289        0020        
1       0085        5223        

Accuracy 99.011
Time taken: 33.39 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.647 Time taken: 374.83 secs

    Epoch: 4 Validation Loss 0.52 Time taken: 1.5 secs| Accuracy 70.53

Epoch: 5 Train Loss 0.424 Time taken: 379.92 secs

    Epoch: 5 Validation Loss 0.382 Time taken: 1.56 secs| Accuracy 83.444
T\P      0       1
0       0160        0038        
1       0053        0203        

Accuracy 79.956
Time taken: 2.02 secs
2592.089 secs

Thu Apr 21 03:07:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=474
Thu Apr 21 03:07:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.06 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 474,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.229 Time taken: 573.17 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 22.25 secs| Accuracy 98.432

Epoch: 2 Train Loss 0.07 Time taken: 575.98 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.3 secs| Accuracy 98.63

Epoch: 3 Train Loss 0.047 Time taken: 576.97 secs

    Epoch: 3 Validation Loss 0.032 Time taken: 22.23 secs| Accuracy 98.969
T\P      0       1
0       5268        0041        
1       0067        5241        

Accuracy 98.983
Time taken: 33.23 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.559 Time taken: 380.37 secs

    Epoch: 4 Validation Loss 0.452 Time taken: 1.49 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.399 Time taken: 378.62 secs

    Epoch: 5 Validation Loss 0.391 Time taken: 1.36 secs| Accuracy 83.775
T\P      0       1
0       0178        0020        
1       0064        0192        

Accuracy 81.498
Time taken: 1.87 secs
2614.302 secs

Thu Apr 21 03:51:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=475
Thu Apr 21 03:51:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 475,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.375 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.278 Time taken: 571.08 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.32 secs| Accuracy 97.598

Epoch: 2 Train Loss 0.084 Time taken: 573.52 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.32 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.059 Time taken: 573.7 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 22.34 secs| Accuracy 98.757
T\P      0       1
0       5228        0081        
1       0053        5255        

Accuracy 98.738
Time taken: 33.17 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.593 Time taken: 377.76 secs

    Epoch: 4 Validation Loss 0.437 Time taken: 1.59 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.404 Time taken: 374.65 secs

    Epoch: 5 Validation Loss 0.353 Time taken: 1.5 secs| Accuracy 84.768
T\P      0       1
0       0173        0025        
1       0059        0197        

Accuracy 81.498
Time taken: 1.9 secs
2599.709 secs

Thu Apr 21 04:36:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=476
Thu Apr 21 04:36:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 476,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 140,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.309 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.198 Time taken: 565.67 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 22.35 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.07 Time taken: 572.3 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.28 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.048 Time taken: 560.97 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.36 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.038 Time taken: 561.29 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 22.35 secs| Accuracy 98.714

Epoch: 5 Train Loss 0.031 Time taken: 561.6 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 22.35 secs| Accuracy 99.011
T\P      0       1
0       5287        0022        
1       0065        5243        

Accuracy 99.181
Time taken: 33.21 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.579 Time taken: 377.0 secs

    Epoch: 6 Validation Loss 0.478 Time taken: 1.47 secs| Accuracy 74.834

Epoch: 7 Train Loss 0.42 Time taken: 377.32 secs

    Epoch: 7 Validation Loss 0.379 Time taken: 1.44 secs| Accuracy 83.444
T\P      0       1
0       0181        0017        
1       0064        0192        

Accuracy 82.159
Time taken: 1.91 secs
3770.078 secs

Thu Apr 21 05:40:05 2022


======================================================================
----------------------------------------------------------------------
                run_ID=477
Thu Apr 21 05:40:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.976 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 477,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 141,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.315 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 566.23 secs

    Epoch: 1 Validation Loss 0.09 Time taken: 22.36 secs| Accuracy 97.768

Epoch: 2 Train Loss 0.083 Time taken: 568.8 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.4 secs| Accuracy 98.432

Epoch: 3 Train Loss 0.055 Time taken: 562.11 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 22.49 secs| Accuracy 98.644

Epoch: 4 Train Loss 0.045 Time taken: 572.19 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 22.37 secs| Accuracy 98.884

Epoch: 5 Train Loss 0.037 Time taken: 563.04 secs

    Epoch: 5 Validation Loss 0.055 Time taken: 22.61 secs| Accuracy 98.7
T\P      0       1
0       5226        0083        
1       0043        5265        

Accuracy 98.813
Time taken: 33.37 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.727 Time taken: 377.56 secs

    Epoch: 6 Validation Loss 0.63 Time taken: 1.43 secs| Accuracy 61.921

Epoch: 7 Train Loss 0.579 Time taken: 379.55 secs

    Epoch: 7 Validation Loss 0.419 Time taken: 1.5 secs| Accuracy 83.113
T\P      0       1
0       0156        0042        
1       0059        0197        

Accuracy 77.753
Time taken: 1.95 secs
3782.475 secs

Thu Apr 21 06:44:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=478
Thu Apr 21 06:44:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.061 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 478,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 142,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.31 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.198 Time taken: 564.58 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 22.36 secs| Accuracy 98.559

Epoch: 2 Train Loss 0.067 Time taken: 567.16 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 22.31 secs| Accuracy 98.884

Epoch: 3 Train Loss 0.052 Time taken: 566.39 secs

    Epoch: 3 Validation Loss 0.033 Time taken: 22.32 secs| Accuracy 99.039

Epoch: 4 Train Loss 0.04 Time taken: 566.94 secs

    Epoch: 4 Validation Loss 0.029 Time taken: 22.29 secs| Accuracy 99.181

Epoch: 5 Train Loss 0.03 Time taken: 563.19 secs

    Epoch: 5 Validation Loss 0.029 Time taken: 22.35 secs| Accuracy 99.152
T\P      0       1
0       5258        0051        
1       0041        5267        

Accuracy 99.133
Time taken: 33.29 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.608 Time taken: 377.25 secs

    Epoch: 6 Validation Loss 0.493 Time taken: 1.48 secs| Accuracy 78.146

Epoch: 7 Train Loss 0.412 Time taken: 373.56 secs

    Epoch: 7 Validation Loss 0.388 Time taken: 1.5 secs| Accuracy 81.457
T\P      0       1
0       0183        0015        
1       0090        0166        

Accuracy 76.872
Time taken: 2.03 secs
3780.493 secs

Thu Apr 21 07:48:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=479
Thu Apr 21 07:48:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.061 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 479,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 143,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.2 Time taken: 565.52 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 22.35 secs| Accuracy 98.361

Epoch: 2 Train Loss 0.074 Time taken: 563.68 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.39 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.047 Time taken: 567.82 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 22.27 secs| Accuracy 98.841

Epoch: 4 Train Loss 0.036 Time taken: 565.72 secs

    Epoch: 4 Validation Loss 0.058 Time taken: 22.2 secs| Accuracy 98.22

Epoch: 5 Train Loss 0.027 Time taken: 561.87 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 22.27 secs| Accuracy 98.955
T\P      0       1
0       5264        0045        
1       0061        5247        

Accuracy 99.002
Time taken: 33.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.711 Time taken: 377.44 secs

    Epoch: 6 Validation Loss 0.43 Time taken: 1.5 secs| Accuracy 81.788

Epoch: 7 Train Loss 0.37 Time taken: 373.25 secs

    Epoch: 7 Validation Loss 0.304 Time taken: 1.45 secs| Accuracy 86.755
T\P      0       1
0       0145        0053        
1       0024        0232        

Accuracy 83.04
Time taken: 1.97 secs
3766.987 secs

Thu Apr 21 08:52:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=480
Thu Apr 21 08:52:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 480,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 144,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 564.53 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.33 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.071 Time taken: 572.59 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.26 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.047 Time taken: 563.86 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.19 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.034 Time taken: 562.55 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 22.25 secs| Accuracy 98.969

Epoch: 5 Train Loss 0.027 Time taken: 562.9 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 22.38 secs| Accuracy 98.912
T\P      0       1
0       5237        0072        
1       0037        5271        

Accuracy 98.973
Time taken: 33.23 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.598 Time taken: 380.54 secs

    Epoch: 6 Validation Loss 0.376 Time taken: 1.46 secs| Accuracy 82.119

Epoch: 7 Train Loss 0.373 Time taken: 374.06 secs

    Epoch: 7 Validation Loss 0.344 Time taken: 1.52 secs| Accuracy 85.099
T\P      0       1
0       0177        0021        
1       0062        0194        

Accuracy 81.718
Time taken: 1.9 secs
3772.596 secs

Thu Apr 21 09:55:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=516
Thu Apr 21 15:14:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.435 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 516,
    "message": "fc1 after inshorts EL0 frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 121,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.453 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 570.95 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 22.37 secs| Accuracy 98.446

Epoch: 2 Train Loss 0.064 Time taken: 575.11 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.46 secs| Accuracy 98.827

Epoch: 3 Train Loss 0.048 Time taken: 570.29 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.41 secs| Accuracy 98.517
T\P      0       1
0       5177        0132        
1       0033        5275        

Accuracy 98.446
Time taken: 33.2 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.714 Time taken: 383.92 secs

    Epoch: 4 Validation Loss 0.608 Time taken: 1.43 secs| Accuracy 79.801

Epoch: 5 Train Loss 0.523 Time taken: 380.46 secs

    Epoch: 5 Validation Loss 0.436 Time taken: 1.41 secs| Accuracy 81.788
T\P      0       1
0       0119        0079        
1       0027        0229        

Accuracy 76.652
Time taken: 1.85 secs
2609.549 secs

Thu Apr 21 15:58:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=517
Thu Apr 21 15:58:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -8.461 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 517,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 122,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.149 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 580.93 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 24.69 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.069 Time taken: 765.98 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 22.97 secs| Accuracy 98.771

Epoch: 3 Train Loss 0.048 Time taken: 676.0 secs

    Epoch: 3 Validation Loss 0.03 Time taken: 22.35 secs| Accuracy 99.068
T\P      0       1
0       5256        0053        
1       0054        5254        

Accuracy 98.992
Time taken: 33.2 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.595 Time taken: 381.19 secs

    Epoch: 4 Validation Loss 0.435 Time taken: 1.53 secs| Accuracy 79.801

Epoch: 5 Train Loss 0.382 Time taken: 376.11 secs

    Epoch: 5 Validation Loss 0.357 Time taken: 1.54 secs| Accuracy 83.775
T\P      0       1
0       0157        0041        
1       0045        0211        

Accuracy 81.057
Time taken: 2.08 secs
2911.894 secs

Thu Apr 21 16:48:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=518
Thu Apr 21 16:48:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.876 GB
37 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 518,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 123,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 4.201 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 678.57 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 23.3 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.069 Time taken: 662.41 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.99 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.047 Time taken: 720.21 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.57 secs| Accuracy 98.29
T\P      0       1
0       5226        0083        
1       0038        5270        

Accuracy 98.86
Time taken: 35.1 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.669 Time taken: 504.98 secs

    Epoch: 4 Validation Loss 0.503 Time taken: 1.65 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.438 Time taken: 490.85 secs

    Epoch: 5 Validation Loss 0.37 Time taken: 1.9 secs| Accuracy 78.146
T\P      0       1
0       0122        0076        
1       0017        0239        

Accuracy 79.515
Time taken: 2.05 secs
3194.805 secs

Thu Apr 21 17:42:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=519
Thu Apr 21 17:42:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.852 GB
35 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 519,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 124,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.421 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.171 Time taken: 675.49 secs

    Epoch: 1 Validation Loss 0.048 Time taken: 23.03 secs| Accuracy 98.488

Epoch: 2 Train Loss 0.062 Time taken: 632.5 secs

    Epoch: 2 Validation Loss 0.047 Time taken: 22.8 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.045 Time taken: 600.92 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 22.26 secs| Accuracy 98.955
T\P      0       1
0       5250        0059        
1       0055        5253        

Accuracy 98.926
Time taken: 33.13 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.57 Time taken: 385.45 secs

    Epoch: 4 Validation Loss 0.473 Time taken: 1.52 secs| Accuracy 79.139

Epoch: 5 Train Loss 0.357 Time taken: 377.63 secs

    Epoch: 5 Validation Loss 0.448 Time taken: 1.47 secs| Accuracy 81.457
T\P      0       1
0       0156        0042        
1       0044        0212        

Accuracy 81.057
Time taken: 1.93 secs
2802.19 secs

Thu Apr 21 18:30:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=520
Thu Apr 21 18:30:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 520,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 125,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 0.766 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.206 Time taken: 584.97 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 23.24 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.086 Time taken: 724.96 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.84 secs| Accuracy 98.361

Epoch: 3 Train Loss 0.064 Time taken: 729.18 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 24.63 secs| Accuracy 98.728
T\P      0       1
0       5255        0054        
1       0073        5235        

Accuracy 98.804
Time taken: 36.68 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.597 Time taken: 584.32 secs

    Epoch: 4 Validation Loss 0.475 Time taken: 1.93 secs| Accuracy 76.49

Epoch: 5 Train Loss 0.438 Time taken: 536.04 secs

    Epoch: 5 Validation Loss 0.398 Time taken: 1.59 secs| Accuracy 83.113
T\P      0       1
0       0189        0009        
1       0071        0185        

Accuracy 82.379
Time taken: 2.51 secs
3299.616 secs

Thu Apr 21 19:26:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=521
Thu Apr 21 19:26:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.081 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 521,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.374 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.155 Time taken: 707.7 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.65 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.071 Time taken: 743.42 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 24.85 secs| Accuracy 98.615

Epoch: 3 Train Loss 0.047 Time taken: 712.12 secs

    Epoch: 3 Validation Loss 0.054 Time taken: 22.37 secs| Accuracy 98.686
T\P      0       1
0       5231        0078        
1       0049        5259        

Accuracy 98.804
Time taken: 33.3 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.733 Time taken: 379.48 secs

    Epoch: 4 Validation Loss 0.554 Time taken: 1.49 secs| Accuracy 59.603

Epoch: 5 Train Loss 0.477 Time taken: 375.28 secs

    Epoch: 5 Validation Loss 0.448 Time taken: 1.48 secs| Accuracy 79.139
T\P      0       1
0       0186        0012        
1       0095        0161        

Accuracy 76.432
Time taken: 1.95 secs
3050.708 secs

Thu Apr 21 20:19:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=401
Mon Apr 25 17:29:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.405 GB
40 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 401,
    "message": "188 with include asha data after inshorts and some layers frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.844 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 635.12 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 22.89 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.095 Time taken: 575.88 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 22.66 secs| Accuracy 98.192

Epoch: 3 Train Loss 0.065 Time taken: 578.98 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 22.95 secs| Accuracy 98.658
T\P      0       1
0       5256        0053        
1       0082        5226        

Accuracy 98.728
Time taken: 34.9 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.7 Time taken: 416.49 secs

    Epoch: 4 Validation Loss 0.421 Time taken: 2.25 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.413 Time taken: 404.31 secs

    Epoch: 5 Validation Loss 0.326 Time taken: 1.92 secs| Accuracy 87.086

Epoch: 6 Train Loss 0.314 Time taken: 437.43 secs

    Epoch: 6 Validation Loss 0.267 Time taken: 2.27 secs| Accuracy 88.742

Epoch: 7 Train Loss 0.217 Time taken: 461.87 secs

    Epoch: 7 Validation Loss 0.257 Time taken: 1.91 secs| Accuracy 89.073

Epoch: 8 Train Loss 0.201 Time taken: 463.37 secs

    Epoch: 8 Validation Loss 0.24 Time taken: 2.48 secs| Accuracy 90.066

Epoch: 9 Train Loss 0.161 Time taken: 464.28 secs

    Epoch: 9 Validation Loss 0.292 Time taken: 1.87 secs| Accuracy 89.404
T\P      0       1
0       0169        0029        
1       0027        0229        

Accuracy 87.665
Time taken: 2.51 secs
4584.794 secs

Mon Apr 25 18:47:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=402
Mon Apr 25 18:47:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.961 GB
39 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 402,
    "message": "401 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.374 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 684.6 secs

    Epoch: 1 Validation Loss 0.082 Time taken: 24.1 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.106 Time taken: 674.78 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 24.44 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.07 Time taken: 676.4 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 24.34 secs| Accuracy 98.135
T\P      0       1
0       5194        0115        
1       0048        5260        

Accuracy 98.465
Time taken: 36.25 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.56 Time taken: 479.3 secs

    Epoch: 4 Validation Loss 0.436 Time taken: 2.32 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.415 Time taken: 493.37 secs

    Epoch: 5 Validation Loss 0.358 Time taken: 2.37 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.347 Time taken: 471.92 secs

    Epoch: 6 Validation Loss 0.317 Time taken: 2.48 secs| Accuracy 86.755

Epoch: 7 Train Loss 0.269 Time taken: 474.29 secs

    Epoch: 7 Validation Loss 0.276 Time taken: 2.12 secs| Accuracy 87.748

Epoch: 8 Train Loss 0.199 Time taken: 468.65 secs

    Epoch: 8 Validation Loss 0.247 Time taken: 2.35 secs| Accuracy 89.404

Epoch: 9 Train Loss 0.17 Time taken: 467.72 secs

    Epoch: 9 Validation Loss 0.249 Time taken: 2.16 secs| Accuracy 90.066
T\P      0       1
0       0169        0029        
1       0024        0232        

Accuracy 88.326
Time taken: 2.69 secs
5048.913 secs

Mon Apr 25 20:12:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=403
Mon Apr 25 20:12:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.167 GB
35 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 403,
    "message": "401 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.398 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.247 Time taken: 675.36 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 24.55 secs| Accuracy 97.584

Epoch: 2 Train Loss 0.078 Time taken: 684.72 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 24.63 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.05 Time taken: 673.25 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 24.4 secs| Accuracy 98.601
T\P      0       1
0       5230        0079        
1       0036        5272        

Accuracy 98.917
Time taken: 35.89 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.706 Time taken: 490.43 secs

    Epoch: 4 Validation Loss 0.581 Time taken: 2.37 secs| Accuracy 67.55

Epoch: 5 Train Loss 0.469 Time taken: 463.63 secs

    Epoch: 5 Validation Loss 0.38 Time taken: 2.21 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.323 Time taken: 463.63 secs

    Epoch: 6 Validation Loss 0.328 Time taken: 2.29 secs| Accuracy 88.742

Epoch: 7 Train Loss 0.229 Time taken: 487.37 secs

    Epoch: 7 Validation Loss 0.317 Time taken: 2.28 secs| Accuracy 87.748

Epoch: 8 Train Loss 0.179 Time taken: 469.85 secs

    Epoch: 8 Validation Loss 0.284 Time taken: 2.18 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.163 Time taken: 466.8 secs

    Epoch: 9 Validation Loss 0.27 Time taken: 2.23 secs| Accuracy 88.411
T\P      0       1
0       0167        0031        
1       0022        0234        

Accuracy 88.326
Time taken: 2.87 secs
5028.01 secs

Mon Apr 25 21:37:42 2022


======================================================================
----------------------------------------------------------------------
                run_ID=446
Mon Apr 25 21:37:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.189 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 446,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.983 GB
41 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 698.4 secs

    Epoch: 1 Validation Loss 0.082 Time taken: 24.29 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.106 Time taken: 680.77 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 24.58 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.07 Time taken: 680.89 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 24.65 secs| Accuracy 98.135
T\P      0       1
0       5194        0115        
1       0048        5260        

Accuracy 98.465
Time taken: 36.67 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.56 Time taken: 492.68 secs

    Epoch: 4 Validation Loss 0.436 Time taken: 2.59 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.415 Time taken: 478.78 secs

    Epoch: 5 Validation Loss 0.358 Time taken: 2.29 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.347 Time taken: 466.37 secs

    Epoch: 6 Validation Loss 0.317 Time taken: 2.39 secs| Accuracy 86.755

Epoch: 7 Train Loss 0.269 Time taken: 464.63 secs

    Epoch: 7 Validation Loss 0.276 Time taken: 2.76 secs| Accuracy 87.748

Epoch: 8 Train Loss 0.199 Time taken: 466.87 secs

    Epoch: 8 Validation Loss 0.247 Time taken: 2.58 secs| Accuracy 89.404

Epoch: 9 Train Loss 0.17 Time taken: 463.45 secs

    Epoch: 9 Validation Loss 0.249 Time taken: 2.34 secs| Accuracy 90.066
T\P      0       1
0       0169        0029        
1       0024        0232        

Accuracy 88.326
Time taken: 2.74 secs
5052.254 secs

Mon Apr 25 23:03:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=447
Mon Apr 25 23:03:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.074 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 447,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.193 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.247 Time taken: 669.25 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 24.35 secs| Accuracy 97.584

Epoch: 2 Train Loss 0.078 Time taken: 682.13 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 24.41 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.05 Time taken: 675.28 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.77 secs| Accuracy 98.601
T\P      0       1
0       5230        0079        
1       0036        5272        

Accuracy 98.917
Time taken: 35.79 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.706 Time taken: 476.98 secs

    Epoch: 4 Validation Loss 0.581 Time taken: 2.05 secs| Accuracy 67.55

Epoch: 5 Train Loss 0.469 Time taken: 463.13 secs

    Epoch: 5 Validation Loss 0.38 Time taken: 2.38 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.323 Time taken: 465.45 secs

    Epoch: 6 Validation Loss 0.328 Time taken: 2.41 secs| Accuracy 88.742

Epoch: 7 Train Loss 0.229 Time taken: 462.08 secs

    Epoch: 7 Validation Loss 0.317 Time taken: 2.14 secs| Accuracy 87.748

Epoch: 8 Train Loss 0.179 Time taken: 457.66 secs

    Epoch: 8 Validation Loss 0.284 Time taken: 2.43 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.163 Time taken: 465.39 secs

    Epoch: 9 Validation Loss 0.27 Time taken: 2.22 secs| Accuracy 88.411
T\P      0       1
0       0167        0031        
1       0022        0234        

Accuracy 88.326
Time taken: 2.74 secs
4968.216 secs

Tue Apr 26 00:27:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=448
Tue Apr 26 00:27:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.044 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 448,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.298 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.298 Time taken: 679.05 secs

    Epoch: 1 Validation Loss 0.08 Time taken: 23.82 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.111 Time taken: 674.12 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 24.69 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.077 Time taken: 697.15 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 24.78 secs| Accuracy 98.502
T\P      0       1
0       5263        0046        
1       0081        5227        

Accuracy 98.804
Time taken: 36.19 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.554 Time taken: 482.13 secs

    Epoch: 4 Validation Loss 0.473 Time taken: 1.96 secs| Accuracy 79.47

Epoch: 5 Train Loss 0.401 Time taken: 469.97 secs

    Epoch: 5 Validation Loss 0.427 Time taken: 2.47 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.325 Time taken: 470.07 secs

    Epoch: 6 Validation Loss 0.376 Time taken: 2.45 secs| Accuracy 81.126

Epoch: 7 Train Loss 0.254 Time taken: 470.42 secs

    Epoch: 7 Validation Loss 0.346 Time taken: 2.12 secs| Accuracy 85.43

Epoch: 8 Train Loss 0.217 Time taken: 466.93 secs

    Epoch: 8 Validation Loss 0.334 Time taken: 2.03 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.187 Time taken: 464.4 secs

    Epoch: 9 Validation Loss 0.356 Time taken: 2.34 secs| Accuracy 87.748
T\P      0       1
0       0155        0043        
1       0015        0241        

Accuracy 87.225
Time taken: 2.81 secs
5027.073 secs

Tue Apr 26 01:52:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=449
Tue Apr 26 01:52:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.946 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 449,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.268 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.3 Time taken: 668.47 secs

    Epoch: 1 Validation Loss 0.098 Time taken: 24.24 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.101 Time taken: 672.67 secs

    Epoch: 2 Validation Loss 0.067 Time taken: 25.38 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.084 Time taken: 685.2 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 24.12 secs| Accuracy 98.46
T\P      0       1
0       5210        0099        
1       0060        5248        

Accuracy 98.502
Time taken: 35.76 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.634 Time taken: 476.49 secs

    Epoch: 4 Validation Loss 0.501 Time taken: 1.98 secs| Accuracy 78.146

Epoch: 5 Train Loss 0.441 Time taken: 462.94 secs

    Epoch: 5 Validation Loss 0.511 Time taken: 2.21 secs| Accuracy 79.139

Epoch: 6 Train Loss 0.418 Time taken: 456.89 secs

    Epoch: 6 Validation Loss 0.475 Time taken: 1.97 secs| Accuracy 82.781

Epoch: 7 Train Loss 0.335 Time taken: 459.76 secs

    Epoch: 7 Validation Loss 0.348 Time taken: 2.41 secs| Accuracy 84.106

Epoch: 8 Train Loss 0.275 Time taken: 446.14 secs

    Epoch: 8 Validation Loss 0.322 Time taken: 1.95 secs| Accuracy 83.444

Epoch: 9 Train Loss 0.237 Time taken: 383.45 secs

    Epoch: 9 Validation Loss 0.367 Time taken: 1.88 secs| Accuracy 85.43
T\P      0       1
0       0168        0030        
1       0029        0227        

Accuracy 87.004
Time taken: 2.35 secs
4862.546 secs

Tue Apr 26 03:14:55 2022




======================================================================
----------------------------------------------------------------------
                run_ID=450
Tue Apr 26 10:40:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.977 GB
34 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 450,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.183 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.262 Time taken: 693.17 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 24.72 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.071 Time taken: 685.42 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 24.86 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.048 Time taken: 691.45 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 24.63 secs| Accuracy 98.856
T\P      0       1
0       5261        0048        
1       0057        5251        

Accuracy 99.011
Time taken: 36.13 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.666 Time taken: 487.77 secs

    Epoch: 4 Validation Loss 0.516 Time taken: 2.27 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.45 Time taken: 476.66 secs

    Epoch: 5 Validation Loss 0.528 Time taken: 2.72 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.345 Time taken: 475.08 secs

    Epoch: 6 Validation Loss 0.34 Time taken: 2.12 secs| Accuracy 85.099

Epoch: 7 Train Loss 0.241 Time taken: 470.07 secs

    Epoch: 7 Validation Loss 0.407 Time taken: 2.57 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.179 Time taken: 470.97 secs

    Epoch: 8 Validation Loss 0.31 Time taken: 2.06 secs| Accuracy 86.755

Epoch: 9 Train Loss 0.141 Time taken: 474.67 secs

    Epoch: 9 Validation Loss 0.331 Time taken: 2.01 secs| Accuracy 87.086
T\P      0       1
0       0173        0025        
1       0037        0219        

Accuracy 86.344
Time taken: 2.39 secs
5076.47 secs

Tue Apr 26 12:06:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=474
Tue Apr 26 12:06:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.876 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 474,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.288 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.229 Time taken: 684.52 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 24.44 secs| Accuracy 98.432

Epoch: 2 Train Loss 0.07 Time taken: 686.11 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 24.54 secs| Accuracy 98.63

Epoch: 3 Train Loss 0.047 Time taken: 692.47 secs

    Epoch: 3 Validation Loss 0.032 Time taken: 24.78 secs| Accuracy 98.969
T\P      0       1
0       5268        0041        
1       0067        5241        

Accuracy 98.983
Time taken: 36.26 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.559 Time taken: 488.05 secs

    Epoch: 4 Validation Loss 0.452 Time taken: 1.9 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.399 Time taken: 487.68 secs

    Epoch: 5 Validation Loss 0.391 Time taken: 3.08 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.286 Time taken: 473.97 secs

    Epoch: 6 Validation Loss 0.434 Time taken: 2.18 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.194 Time taken: 473.23 secs

    Epoch: 7 Validation Loss 0.326 Time taken: 2.18 secs| Accuracy 84.437

Epoch: 8 Train Loss 0.139 Time taken: 469.14 secs

    Epoch: 8 Validation Loss 0.461 Time taken: 2.48 secs| Accuracy 85.099

Epoch: 9 Train Loss 0.146 Time taken: 471.27 secs

    Epoch: 9 Validation Loss 0.354 Time taken: 2.09 secs| Accuracy 85.762
T\P      0       1
0       0182        0016        
1       0037        0219        

Accuracy 88.326
Time taken: 2.59 secs
5079.58 secs

Tue Apr 26 13:32:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=475
Tue Apr 26 14:52:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.03 GB
42 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 475,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
30 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.278 Time taken: 674.97 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 23.51 secs| Accuracy 97.598

Epoch: 2 Train Loss 0.084 Time taken: 675.22 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 24.13 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.059 Time taken: 690.44 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.7 secs| Accuracy 98.757
T\P      0       1
0       5228        0081        
1       0053        5255        

Accuracy 98.738
Time taken: 34.9 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.593 Time taken: 480.6 secs

    Epoch: 4 Validation Loss 0.437 Time taken: 2.1 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.404 Time taken: 462.51 secs

    Epoch: 5 Validation Loss 0.353 Time taken: 1.81 secs| Accuracy 84.768

Epoch: 6 Train Loss 0.329 Time taken: 461.13 secs

    Epoch: 6 Validation Loss 0.32 Time taken: 2.06 secs| Accuracy 86.755

Epoch: 7 Train Loss 0.25 Time taken: 460.45 secs

    Epoch: 7 Validation Loss 0.264 Time taken: 2.35 secs| Accuracy 88.411

Epoch: 8 Train Loss 0.191 Time taken: 458.81 secs

    Epoch: 8 Validation Loss 0.248 Time taken: 2.22 secs| Accuracy 88.742

Epoch: 9 Train Loss 0.167 Time taken: 459.49 secs

    Epoch: 9 Validation Loss 0.243 Time taken: 1.89 secs| Accuracy 89.073
T\P      0       1
0       0166        0032        
1       0022        0234        

Accuracy 88.106
Time taken: 2.61 secs
4971.558 secs

Tue Apr 26 16:16:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=580
Fri Apr 29 17:25:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.274 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 580,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.536 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 566.52 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.59 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.057 Time taken: 566.11 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.65 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.034 Time taken: 565.83 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.6 secs| Accuracy 98.163
T\P      0       1
0       5224        0085        
1       0060        5248        

Accuracy 98.634
Time taken: 33.38 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.586 Time taken: 374.23 secs

    Epoch: 4 Validation Loss 0.35 Time taken: 1.84 secs| Accuracy 82.45

Epoch: 5 Train Loss 0.273 Time taken: 374.57 secs

    Epoch: 5 Validation Loss 0.285 Time taken: 1.71 secs| Accuracy 85.762

Epoch: 6 Train Loss 0.201 Time taken: 372.41 secs

    Epoch: 6 Validation Loss 0.316 Time taken: 1.69 secs| Accuracy 86.093

Epoch: 7 Train Loss 0.154 Time taken: 372.35 secs

    Epoch: 7 Validation Loss 0.296 Time taken: 1.74 secs| Accuracy 87.417

Epoch: 8 Train Loss 0.082 Time taken: 372.02 secs

    Epoch: 8 Validation Loss 0.358 Time taken: 1.75 secs| Accuracy 88.079

Epoch: 9 Train Loss 0.046 Time taken: 371.84 secs

    Epoch: 9 Validation Loss 0.47 Time taken: 1.75 secs| Accuracy 87.086
T\P      0       1
0       0183        0015        
1       0035        0221        

Accuracy 88.987
Time taken: 2.17 secs
4070.999 secs

Fri Apr 29 18:34:18 2022


======================================================================
----------------------------------------------------------------------
                run_ID=581
Fri Apr 29 18:34:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.877 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 581,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 0.42 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 564.55 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.44 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 566.06 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.42 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.034 Time taken: 566.4 secs

    Epoch: 3 Validation Loss 0.07 Time taken: 22.43 secs| Accuracy 98.036
T\P      0       1
0       5284        0025        
1       0154        5154        

Accuracy 98.314
Time taken: 33.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.496 Time taken: 370.51 secs

    Epoch: 4 Validation Loss 0.276 Time taken: 1.7 secs| Accuracy 86.755

Epoch: 5 Train Loss 0.202 Time taken: 371.34 secs

    Epoch: 5 Validation Loss 0.274 Time taken: 1.67 secs| Accuracy 88.742

Epoch: 6 Train Loss 0.144 Time taken: 371.05 secs

    Epoch: 6 Validation Loss 0.309 Time taken: 1.66 secs| Accuracy 89.073

Epoch: 7 Train Loss 0.071 Time taken: 371.35 secs

    Epoch: 7 Validation Loss 0.331 Time taken: 1.63 secs| Accuracy 88.411

Epoch: 8 Train Loss 0.03 Time taken: 370.97 secs

    Epoch: 8 Validation Loss 0.39 Time taken: 1.62 secs| Accuracy 88.411

Epoch: 9 Train Loss 0.018 Time taken: 370.85 secs

    Epoch: 9 Validation Loss 0.46 Time taken: 1.65 secs| Accuracy 89.073
T\P      0       1
0       0158        0040        
1       0019        0237        

Accuracy 87.004
Time taken: 2.16 secs
4058.242 secs

Fri Apr 29 19:42:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=582
Fri Apr 29 19:43:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.373 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 582,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.726 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 564.52 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.42 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.06 Time taken: 564.93 secs

    Epoch: 2 Validation Loss 0.068 Time taken: 22.4 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.032 Time taken: 563.82 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.4 secs| Accuracy 98.333
T\P      0       1
0       5251        0058        
1       0073        5235        

Accuracy 98.766
Time taken: 33.19 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.549 Time taken: 370.02 secs

    Epoch: 4 Validation Loss 0.325 Time taken: 1.62 secs| Accuracy 84.768

Epoch: 5 Train Loss 0.253 Time taken: 370.61 secs

    Epoch: 5 Validation Loss 0.278 Time taken: 1.64 secs| Accuracy 85.43

Epoch: 6 Train Loss 0.18 Time taken: 370.2 secs

    Epoch: 6 Validation Loss 0.316 Time taken: 1.65 secs| Accuracy 86.424

Epoch: 7 Train Loss 0.125 Time taken: 370.11 secs

    Epoch: 7 Validation Loss 0.363 Time taken: 1.56 secs| Accuracy 86.093

Epoch: 8 Train Loss 0.076 Time taken: 370.45 secs

    Epoch: 8 Validation Loss 0.444 Time taken: 1.65 secs| Accuracy 86.755

Epoch: 9 Train Loss 0.037 Time taken: 370.09 secs

    Epoch: 9 Validation Loss 0.427 Time taken: 1.64 secs| Accuracy 86.093
T\P      0       1
0       0168        0030        
1       0025        0231        

Accuracy 87.885
Time taken: 2.14 secs
4049.64 secs

Fri Apr 29 20:51:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=583
Fri Apr 29 20:51:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.875 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 583,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.976 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 565.02 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.56 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 565.55 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.57 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.033 Time taken: 565.65 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.53 secs| Accuracy 98.29
T\P      0       1
0       5240        0069        
1       0078        5230        

Accuracy 98.615
Time taken: 33.31 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.516 Time taken: 370.5 secs

    Epoch: 4 Validation Loss 0.367 Time taken: 1.78 secs| Accuracy 82.45

Epoch: 5 Train Loss 0.243 Time taken: 370.17 secs

    Epoch: 5 Validation Loss 0.391 Time taken: 1.78 secs| Accuracy 83.444

Epoch: 6 Train Loss 0.18 Time taken: 370.6 secs

    Epoch: 6 Validation Loss 0.378 Time taken: 1.83 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.097 Time taken: 370.63 secs

    Epoch: 7 Validation Loss 0.425 Time taken: 1.8 secs| Accuracy 83.444

Epoch: 8 Train Loss 0.042 Time taken: 370.07 secs

    Epoch: 8 Validation Loss 0.565 Time taken: 1.83 secs| Accuracy 86.093

Epoch: 9 Train Loss 0.048 Time taken: 370.37 secs

    Epoch: 9 Validation Loss 0.491 Time taken: 1.83 secs| Accuracy 86.093
T\P      0       1
0       0183        0015        
1       0032        0224        

Accuracy 89.648
Time taken: 2.19 secs
4054.296 secs

Fri Apr 29 22:00:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=584
Fri Apr 29 22:00:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.854 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 584,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.984 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 565.69 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.55 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.056 Time taken: 566.23 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.59 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.032 Time taken: 566.46 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.54 secs| Accuracy 98.474
T\P      0       1
0       5227        0082        
1       0068        5240        

Accuracy 98.587
Time taken: 33.36 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.495 Time taken: 370.74 secs

    Epoch: 4 Validation Loss 0.341 Time taken: 1.84 secs| Accuracy 86.093

Epoch: 5 Train Loss 0.258 Time taken: 371.1 secs

    Epoch: 5 Validation Loss 0.285 Time taken: 1.9 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.2 Time taken: 370.97 secs

    Epoch: 6 Validation Loss 0.26 Time taken: 1.83 secs| Accuracy 88.079

Epoch: 7 Train Loss 0.141 Time taken: 371.59 secs

    Epoch: 7 Validation Loss 0.333 Time taken: 1.83 secs| Accuracy 86.755

Epoch: 8 Train Loss 0.076 Time taken: 371.35 secs

    Epoch: 8 Validation Loss 0.368 Time taken: 1.78 secs| Accuracy 88.079

Epoch: 9 Train Loss 0.052 Time taken: 371.54 secs

    Epoch: 9 Validation Loss 0.439 Time taken: 1.82 secs| Accuracy 86.424
T\P      0       1
0       0171        0027        
1       0029        0227        

Accuracy 87.665
Time taken: 2.28 secs
4061.17 secs

Fri Apr 29 23:08:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=585
Fri Apr 29 23:08:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.876 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 585,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.981 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 566.71 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.51 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.052 Time taken: 564.43 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.72 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.031 Time taken: 565.39 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 22.77 secs| Accuracy 98.446
T\P      0       1
0       5237        0072        
1       0086        5222        

Accuracy 98.512
Time taken: 33.55 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.636 Time taken: 370.17 secs

    Epoch: 4 Validation Loss 0.316 Time taken: 2.02 secs| Accuracy 85.762

Epoch: 5 Train Loss 0.265 Time taken: 370.67 secs

    Epoch: 5 Validation Loss 0.284 Time taken: 1.97 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.19 Time taken: 370.91 secs

    Epoch: 6 Validation Loss 0.29 Time taken: 1.98 secs| Accuracy 86.424

Epoch: 7 Train Loss 0.11 Time taken: 371.06 secs

    Epoch: 7 Validation Loss 0.404 Time taken: 1.96 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.072 Time taken: 370.82 secs

    Epoch: 8 Validation Loss 0.399 Time taken: 1.96 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.055 Time taken: 370.92 secs

    Epoch: 9 Validation Loss 0.415 Time taken: 2.02 secs| Accuracy 86.424
T\P      0       1
0       0171        0027        
1       0030        0226        

Accuracy 87.445
Time taken: 2.51 secs
4059.677 secs

Sat Apr 30 00:17:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=586
Sat Apr 30 00:17:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.877 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 586,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.979 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 565.91 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 22.6 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.061 Time taken: 566.27 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.56 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.034 Time taken: 566.32 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 22.61 secs| Accuracy 98.375
T\P      0       1
0       5205        0104        
1       0064        5244        

Accuracy 98.418
Time taken: 33.32 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.531 Time taken: 369.96 secs

    Epoch: 4 Validation Loss 0.438 Time taken: 1.79 secs| Accuracy 82.781

Epoch: 5 Train Loss 0.243 Time taken: 370.91 secs

    Epoch: 5 Validation Loss 0.346 Time taken: 1.76 secs| Accuracy 83.444

Epoch: 6 Train Loss 0.172 Time taken: 370.94 secs

    Epoch: 6 Validation Loss 0.425 Time taken: 1.75 secs| Accuracy 84.106

Epoch: 7 Train Loss 0.12 Time taken: 371.13 secs

    Epoch: 7 Validation Loss 0.414 Time taken: 1.82 secs| Accuracy 85.099

Epoch: 8 Train Loss 0.084 Time taken: 370.57 secs

    Epoch: 8 Validation Loss 0.559 Time taken: 1.87 secs| Accuracy 84.768

Epoch: 9 Train Loss 0.053 Time taken: 370.19 secs

    Epoch: 9 Validation Loss 0.507 Time taken: 2.01 secs| Accuracy 86.093
T\P      0       1
0       0178        0020        
1       0033        0223        

Accuracy 88.326
Time taken: 2.48 secs
4058.148 secs

Sat Apr 30 01:25:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=587
Sat Apr 30 01:25:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.877 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 587,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.994 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 563.65 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.35 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.063 Time taken: 564.01 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.31 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.036 Time taken: 564.45 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.3 secs| Accuracy 98.29
T\P      0       1
0       5199        0110        
1       0059        5249        

Accuracy 98.408
Time taken: 33.1 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.514 Time taken: 368.92 secs

    Epoch: 4 Validation Loss 0.321 Time taken: 1.61 secs| Accuracy 85.762

Epoch: 5 Train Loss 0.235 Time taken: 369.64 secs

    Epoch: 5 Validation Loss 0.278 Time taken: 1.58 secs| Accuracy 87.748

Epoch: 6 Train Loss 0.162 Time taken: 369.4 secs

    Epoch: 6 Validation Loss 0.313 Time taken: 1.59 secs| Accuracy 87.417

Epoch: 7 Train Loss 0.12 Time taken: 369.72 secs

    Epoch: 7 Validation Loss 0.363 Time taken: 1.7 secs| Accuracy 87.086

Epoch: 8 Train Loss 0.064 Time taken: 369.6 secs

    Epoch: 8 Validation Loss 0.34 Time taken: 1.61 secs| Accuracy 88.079

Epoch: 9 Train Loss 0.026 Time taken: 369.16 secs

    Epoch: 9 Validation Loss 0.47 Time taken: 1.67 secs| Accuracy 87.417
T\P      0       1
0       0182        0016        
1       0034        0222        

Accuracy 88.987
Time taken: 2.08 secs
4042.154 secs

Sat Apr 30 02:34:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=588
Sat Apr 30 02:34:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.871 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 588,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.977 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.192 Time taken: 565.61 secs

    Epoch: 1 Validation Loss 0.084 Time taken: 22.53 secs| Accuracy 97.415

Epoch: 2 Train Loss 0.058 Time taken: 565.45 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 22.51 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.033 Time taken: 565.44 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.53 secs| Accuracy 98.404
T\P      0       1
0       5255        0054        
1       0088        5220        

Accuracy 98.663
Time taken: 33.45 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.625 Time taken: 369.95 secs

    Epoch: 4 Validation Loss 0.374 Time taken: 1.68 secs| Accuracy 84.106

Epoch: 5 Train Loss 0.274 Time taken: 369.83 secs

    Epoch: 5 Validation Loss 0.295 Time taken: 1.76 secs| Accuracy 86.093

Epoch: 6 Train Loss 0.208 Time taken: 370.11 secs

    Epoch: 6 Validation Loss 0.281 Time taken: 1.75 secs| Accuracy 87.417

Epoch: 7 Train Loss 0.162 Time taken: 369.62 secs

    Epoch: 7 Validation Loss 0.306 Time taken: 1.94 secs| Accuracy 87.417

Epoch: 8 Train Loss 0.1 Time taken: 370.3 secs

    Epoch: 8 Validation Loss 0.36 Time taken: 1.87 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.07 Time taken: 369.98 secs

    Epoch: 9 Validation Loss 0.474 Time taken: 1.83 secs| Accuracy 84.437
T\P      0       1
0       0187        0011        
1       0055        0201        

Accuracy 85.463
Time taken: 2.2 secs
4051.324 secs

Sat Apr 30 03:42:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=589
Sat Apr 30 03:42:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.848 GB
31 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 589,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.982 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 566.1 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.47 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.06 Time taken: 565.73 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.49 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.035 Time taken: 565.23 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 22.49 secs| Accuracy 98.333
T\P      0       1
0       5218        0091        
1       0065        5243        

Accuracy 98.531
Time taken: 33.25 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.446 Time taken: 370.03 secs

    Epoch: 4 Validation Loss 0.296 Time taken: 1.72 secs| Accuracy 87.417

Epoch: 5 Train Loss 0.2 Time taken: 369.62 secs

    Epoch: 5 Validation Loss 0.314 Time taken: 1.78 secs| Accuracy 87.086

Epoch: 6 Train Loss 0.125 Time taken: 369.77 secs

    Epoch: 6 Validation Loss 0.371 Time taken: 1.81 secs| Accuracy 87.417

Epoch: 7 Train Loss 0.073 Time taken: 370.52 secs

    Epoch: 7 Validation Loss 0.352 Time taken: 1.79 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.045 Time taken: 370.06 secs

    Epoch: 8 Validation Loss 0.602 Time taken: 1.78 secs| Accuracy 83.113

Epoch: 9 Train Loss 0.057 Time taken: 369.83 secs

    Epoch: 9 Validation Loss 0.37 Time taken: 1.78 secs| Accuracy 86.755
T\P      0       1
0       0171        0027        
1       0017        0239        

Accuracy 90.308
Time taken: 2.33 secs
4052.797 secs

Sat Apr 30 04:51:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=590
Sat Apr 30 04:51:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.854 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 590,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.991 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=590
Sat Apr 30 08:02:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.867 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 590,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.002 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 577.13 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.49 secs| Accuracy 97.966

Epoch: 2 Train Loss 0.059 Time taken: 577.09 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.46 secs| Accuracy 98.135


======================================================================
----------------------------------------------------------------------
                run_ID=591
Sat Apr 30 08:38:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.082 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 591,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.301 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=591
Sat Apr 30 08:43:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.101 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 591,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.727 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 589.28 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.82 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.063 Time taken: 589.6 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.8 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.032 Time taken: 589.27 secs

    Epoch: 3 Validation Loss 0.072 Time taken: 22.87 secs| Accuracy 98.022
T\P      0       1
0       5283        0026        
1       0141        5167        

Accuracy 98.427
Time taken: 33.76 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.524 Time taken: 378.39 secs

    Epoch: 4 Validation Loss 0.288 Time taken: 1.85 secs| Accuracy 86.424

Epoch: 5 Train Loss 0.21 Time taken: 377.89 secs

    Epoch: 5 Validation Loss 0.282 Time taken: 1.83 secs| Accuracy 88.742

Epoch: 6 Train Loss 0.132 Time taken: 378.19 secs

    Epoch: 6 Validation Loss 0.292 Time taken: 1.77 secs| Accuracy 88.742

Epoch: 7 Train Loss 0.065 Time taken: 377.55 secs

    Epoch: 7 Validation Loss 0.339 Time taken: 1.83 secs| Accuracy 88.411

Epoch: 8 Train Loss 0.032 Time taken: 377.85 secs

    Epoch: 8 Validation Loss 0.376 Time taken: 1.79 secs| Accuracy 88.742

Epoch: 9 Train Loss 0.041 Time taken: 377.71 secs

    Epoch: 9 Validation Loss 0.371 Time taken: 1.82 secs| Accuracy 89.735
T\P      0       1
0       0173        0025        
1       0024        0232        

Accuracy 89.207
Time taken: 2.21 secs
4173.465 secs

Sat Apr 30 09:53:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=590
Sat Apr 30 09:53:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.964 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 590,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.978 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 588.51 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.74 secs| Accuracy 97.966

Epoch: 2 Train Loss 0.059 Time taken: 588.41 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.93 secs| Accuracy 98.135

Epoch: 3 Train Loss 0.033 Time taken: 588.59 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.71 secs| Accuracy 98.319
T\P      0       1
0       5207        0102        
1       0069        5239        

Accuracy 98.389
Time taken: 33.76 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.51 Time taken: 377.63 secs

    Epoch: 4 Validation Loss 0.323 Time taken: 1.81 secs| Accuracy 80.464

Epoch: 5 Train Loss 0.229 Time taken: 377.51 secs

    Epoch: 5 Validation Loss 0.289 Time taken: 1.68 secs| Accuracy 85.762

Epoch: 6 Train Loss 0.159 Time taken: 377.45 secs

    Epoch: 6 Validation Loss 0.297 Time taken: 1.73 secs| Accuracy 87.086

Epoch: 7 Train Loss 0.102 Time taken: 377.63 secs

    Epoch: 7 Validation Loss 0.353 Time taken: 1.75 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.061 Time taken: 377.58 secs

    Epoch: 8 Validation Loss 0.435 Time taken: 1.74 secs| Accuracy 86.093

Epoch: 9 Train Loss 0.044 Time taken: 377.75 secs

    Epoch: 9 Validation Loss 0.428 Time taken: 1.69 secs| Accuracy 88.411
T\P      0       1
0       0181        0017        
1       0021        0235        

Accuracy 91.63
Time taken: 2.11 secs
4170.178 secs

Sat Apr 30 11:04:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=592
Sat Apr 30 11:04:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.869 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 592,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.983 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 589.77 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.76 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.057 Time taken: 587.96 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 22.85 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.031 Time taken: 588.38 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 22.87 secs| Accuracy 98.389
T\P      0       1
0       5237        0072        
1       0061        5247        

Accuracy 98.747
Time taken: 33.66 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.552 Time taken: 378.14 secs

    Epoch: 4 Validation Loss 0.313 Time taken: 1.78 secs| Accuracy 85.099

Epoch: 5 Train Loss 0.24 Time taken: 377.94 secs

    Epoch: 5 Validation Loss 0.289 Time taken: 1.81 secs| Accuracy 84.768

Epoch: 6 Train Loss 0.177 Time taken: 377.88 secs

    Epoch: 6 Validation Loss 0.335 Time taken: 1.77 secs| Accuracy 84.437

Epoch: 7 Train Loss 0.119 Time taken: 377.39 secs

    Epoch: 7 Validation Loss 0.375 Time taken: 1.81 secs| Accuracy 85.762

Epoch: 8 Train Loss 0.085 Time taken: 377.99 secs

    Epoch: 8 Validation Loss 0.474 Time taken: 1.75 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.057 Time taken: 377.59 secs

    Epoch: 9 Validation Loss 0.527 Time taken: 1.77 secs| Accuracy 86.755
T\P      0       1
0       0164        0034        
1       0020        0236        

Accuracy 88.106
Time taken: 2.26 secs
4172.744 secs

Sat Apr 30 12:14:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=593
Sat Apr 30 12:14:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.873 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 593,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.925 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 592.98 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 23.03 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.06 Time taken: 591.65 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 23.04 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.034 Time taken: 590.34 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.8 secs| Accuracy 98.418
T\P      0       1
0       5251        0058        
1       0085        5223        

Accuracy 98.653
Time taken: 33.63 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.469 Time taken: 379.46 secs

    Epoch: 4 Validation Loss 0.376 Time taken: 1.88 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.275 Time taken: 381.7 secs

    Epoch: 5 Validation Loss 0.358 Time taken: 2.17 secs| Accuracy 86.093

Epoch: 6 Train Loss 0.196 Time taken: 482.92 secs

    Epoch: 6 Validation Loss 0.344 Time taken: 2.54 secs| Accuracy 87.086

Epoch: 7 Train Loss 0.129 Time taken: 456.45 secs

    Epoch: 7 Validation Loss 0.361 Time taken: 1.97 secs| Accuracy 85.762

Epoch: 8 Train Loss 0.068 Time taken: 380.82 secs

    Epoch: 8 Validation Loss 0.389 Time taken: 1.77 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.034 Time taken: 377.85 secs

    Epoch: 9 Validation Loss 0.502 Time taken: 1.79 secs| Accuracy 87.086
T\P      0       1
0       0171        0027        
1       0030        0226        

Accuracy 87.445
Time taken: 2.27 secs
4374.239 secs

Sat Apr 30 13:28:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=620
Sun May  1 01:01:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.069 GB
23 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 620,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.863 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.682 Time taken: 377.95 secs

    Epoch: 1 Validation Loss 0.641 Time taken: 1.45 secs| Accuracy 84.106

Epoch: 2 Train Loss 0.576 Time taken: 375.12 secs

    Epoch: 2 Validation Loss 0.516 Time taken: 1.37 secs| Accuracy 87.748

Epoch: 3 Train Loss 0.459 Time taken: 375.02 secs

    Epoch: 3 Validation Loss 0.456 Time taken: 1.4 secs| Accuracy 84.106

Epoch: 4 Train Loss 0.374 Time taken: 374.71 secs

    Epoch: 4 Validation Loss 0.396 Time taken: 1.39 secs| Accuracy 87.417

Epoch: 5 Train Loss 0.306 Time taken: 375.37 secs

    Epoch: 5 Validation Loss 0.348 Time taken: 1.37 secs| Accuracy 88.742
T\P      0       1
0       0176        0022        
1       0032        0224        

Accuracy 88.106
Time taken: 1.82 secs
1887.001 secs

Sun May  1 01:33:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=621
Sun May  1 01:34:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.019 GB
21 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 621,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.822 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.668 Time taken: 377.65 secs

    Epoch: 1 Validation Loss 0.638 Time taken: 1.37 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.589 Time taken: 374.23 secs

    Epoch: 2 Validation Loss 0.535 Time taken: 1.35 secs| Accuracy 85.099

Epoch: 3 Train Loss 0.485 Time taken: 375.9 secs

    Epoch: 3 Validation Loss 0.498 Time taken: 1.33 secs| Accuracy 83.444

Epoch: 4 Train Loss 0.421 Time taken: 374.5 secs

    Epoch: 4 Validation Loss 0.406 Time taken: 1.35 secs| Accuracy 85.762

Epoch: 5 Train Loss 0.331 Time taken: 374.21 secs

    Epoch: 5 Validation Loss 0.378 Time taken: 1.35 secs| Accuracy 86.755
T\P      0       1
0       0182        0016        
1       0055        0201        

Accuracy 84.361
Time taken: 1.79 secs
1885.071 secs

Sun May  1 02:06:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=622
Sun May  1 02:06:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.061 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 622,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.87 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.674 Time taken: 379.7 secs

    Epoch: 1 Validation Loss 0.644 Time taken: 1.36 secs| Accuracy 79.801

Epoch: 2 Train Loss 0.579 Time taken: 374.56 secs

    Epoch: 2 Validation Loss 0.518 Time taken: 1.39 secs| Accuracy 85.099

Epoch: 3 Train Loss 0.45 Time taken: 375.26 secs

    Epoch: 3 Validation Loss 0.46 Time taken: 1.34 secs| Accuracy 85.099

Epoch: 4 Train Loss 0.351 Time taken: 374.55 secs

    Epoch: 4 Validation Loss 0.413 Time taken: 1.36 secs| Accuracy 85.43

Epoch: 5 Train Loss 0.273 Time taken: 376.26 secs

    Epoch: 5 Validation Loss 0.386 Time taken: 1.38 secs| Accuracy 87.086
T\P      0       1
0       0160        0038        
1       0016        0240        

Accuracy 88.106
Time taken: 1.84 secs
1889.021 secs

Sun May  1 02:38:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=623
Sun May  1 02:38:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.066 GB
23 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 623,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.855 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.664 Time taken: 377.4 secs

    Epoch: 1 Validation Loss 0.619 Time taken: 1.34 secs| Accuracy 80.464

Epoch: 2 Train Loss 0.558 Time taken: 375.17 secs

    Epoch: 2 Validation Loss 0.526 Time taken: 1.33 secs| Accuracy 83.444

Epoch: 3 Train Loss 0.449 Time taken: 374.67 secs

    Epoch: 3 Validation Loss 0.446 Time taken: 1.32 secs| Accuracy 85.762

Epoch: 4 Train Loss 0.36 Time taken: 375.28 secs

    Epoch: 4 Validation Loss 0.402 Time taken: 1.29 secs| Accuracy 86.755

Epoch: 5 Train Loss 0.287 Time taken: 375.41 secs

    Epoch: 5 Validation Loss 0.442 Time taken: 1.36 secs| Accuracy 84.106
T\P      0       1
0       0152        0046        
1       0008        0248        

Accuracy 88.106
Time taken: 1.79 secs
1886.383 secs

Sun May  1 03:10:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=624
Sun May  1 03:10:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.065 GB
23 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 624,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.851 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.681 Time taken: 378.6 secs

    Epoch: 1 Validation Loss 0.641 Time taken: 1.42 secs| Accuracy 81.457

Epoch: 2 Train Loss 0.569 Time taken: 375.55 secs

    Epoch: 2 Validation Loss 0.532 Time taken: 1.42 secs| Accuracy 82.781

Epoch: 3 Train Loss 0.476 Time taken: 375.78 secs

    Epoch: 3 Validation Loss 0.469 Time taken: 1.42 secs| Accuracy 84.768

Epoch: 4 Train Loss 0.39 Time taken: 376.01 secs

    Epoch: 4 Validation Loss 0.389 Time taken: 1.45 secs| Accuracy 87.417

Epoch: 5 Train Loss 0.307 Time taken: 376.04 secs

    Epoch: 5 Validation Loss 0.373 Time taken: 1.33 secs| Accuracy 86.755
T\P      0       1
0       0166        0032        
1       0023        0233        

Accuracy 87.885
Time taken: 1.8 secs
1890.836 secs

Sun May  1 03:43:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=625
Sun May  1 03:43:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.065 GB
24 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 625,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.861 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.682 Time taken: 379.91 secs

    Epoch: 1 Validation Loss 0.639 Time taken: 1.47 secs| Accuracy 85.099

Epoch: 2 Train Loss 0.575 Time taken: 377.51 secs

    Epoch: 2 Validation Loss 0.515 Time taken: 1.4 secs| Accuracy 87.086

Epoch: 3 Train Loss 0.461 Time taken: 377.79 secs

    Epoch: 3 Validation Loss 0.425 Time taken: 1.44 secs| Accuracy 88.742

Epoch: 4 Train Loss 0.353 Time taken: 377.77 secs

    Epoch: 4 Validation Loss 0.381 Time taken: 1.41 secs| Accuracy 88.742

Epoch: 5 Train Loss 0.272 Time taken: 377.55 secs

    Epoch: 5 Validation Loss 0.349 Time taken: 1.46 secs| Accuracy 88.079
T\P      0       1
0       0157        0041        
1       0018        0238        

Accuracy 87.004
Time taken: 1.87 secs
1899.596 secs

Sun May  1 04:15:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=626
Sun May  1 04:15:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.062 GB
25 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 626,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.856 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.668 Time taken: 380.2 secs

    Epoch: 1 Validation Loss 0.637 Time taken: 1.43 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.584 Time taken: 378.84 secs

    Epoch: 2 Validation Loss 0.536 Time taken: 1.43 secs| Accuracy 84.437

Epoch: 3 Train Loss 0.479 Time taken: 375.83 secs

    Epoch: 3 Validation Loss 0.496 Time taken: 1.51 secs| Accuracy 83.444

Epoch: 4 Train Loss 0.398 Time taken: 377.59 secs

    Epoch: 4 Validation Loss 0.434 Time taken: 1.38 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.346 Time taken: 377.67 secs

    Epoch: 5 Validation Loss 0.384 Time taken: 1.46 secs| Accuracy 88.411
T\P      0       1
0       0165        0033        
1       0012        0244        

Accuracy 90.088
Time taken: 1.96 secs
1899.342 secs

Sun May  1 04:48:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=627
Sun May  1 04:48:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.063 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 627,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.86 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.673 Time taken: 377.82 secs

    Epoch: 1 Validation Loss 0.642 Time taken: 1.36 secs| Accuracy 78.477

Epoch: 2 Train Loss 0.586 Time taken: 376.6 secs

    Epoch: 2 Validation Loss 0.535 Time taken: 1.36 secs| Accuracy 82.45

Epoch: 3 Train Loss 0.453 Time taken: 377.14 secs

    Epoch: 3 Validation Loss 0.472 Time taken: 1.34 secs| Accuracy 85.099

Epoch: 4 Train Loss 0.349 Time taken: 377.64 secs

    Epoch: 4 Validation Loss 0.392 Time taken: 1.35 secs| Accuracy 87.748

Epoch: 5 Train Loss 0.268 Time taken: 377.89 secs

    Epoch: 5 Validation Loss 0.397 Time taken: 1.33 secs| Accuracy 86.093
T\P      0       1
0       0173        0025        
1       0033        0223        

Accuracy 87.225
Time taken: 1.76 secs
1895.599 secs

Sun May  1 05:20:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=628
Sun May  1 05:20:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.06 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 628,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.859 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.663 Time taken: 380.45 secs

    Epoch: 1 Validation Loss 0.617 Time taken: 1.4 secs| Accuracy 81.788

Epoch: 2 Train Loss 0.555 Time taken: 377.97 secs

    Epoch: 2 Validation Loss 0.525 Time taken: 1.39 secs| Accuracy 84.437

Epoch: 3 Train Loss 0.451 Time taken: 377.16 secs

    Epoch: 3 Validation Loss 0.438 Time taken: 1.37 secs| Accuracy 86.755

Epoch: 4 Train Loss 0.351 Time taken: 377.33 secs

    Epoch: 4 Validation Loss 0.381 Time taken: 1.44 secs| Accuracy 88.079

Epoch: 5 Train Loss 0.278 Time taken: 376.58 secs

    Epoch: 5 Validation Loss 0.378 Time taken: 1.44 secs| Accuracy 87.086
T\P      0       1
0       0176        0022        
1       0032        0224        

Accuracy 88.106
Time taken: 1.9 secs
1898.427 secs

Sun May  1 05:53:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=629
Sun May  1 05:53:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.066 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 629,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.844 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.68 Time taken: 378.58 secs

    Epoch: 1 Validation Loss 0.639 Time taken: 1.29 secs| Accuracy 80.795

Epoch: 2 Train Loss 0.566 Time taken: 377.17 secs

    Epoch: 2 Validation Loss 0.521 Time taken: 1.31 secs| Accuracy 85.762

Epoch: 3 Train Loss 0.459 Time taken: 377.03 secs

    Epoch: 3 Validation Loss 0.456 Time taken: 1.31 secs| Accuracy 85.762

Epoch: 4 Train Loss 0.372 Time taken: 377.5 secs

    Epoch: 4 Validation Loss 0.423 Time taken: 1.32 secs| Accuracy 85.43

Epoch: 5 Train Loss 0.295 Time taken: 377.87 secs

    Epoch: 5 Validation Loss 0.385 Time taken: 1.3 secs| Accuracy 85.43
T\P      0       1
0       0163        0035        
1       0023        0233        

Accuracy 87.225
Time taken: 1.72 secs
1896.422 secs

Sun May  1 06:25:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=630
Sun May  1 06:25:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.872 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 630,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.988 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 575.01 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.61 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 573.58 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.49 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.034 Time taken: 570.63 secs

    Epoch: 3 Validation Loss 0.07 Time taken: 22.59 secs| Accuracy 98.036

Epoch: 4 Train Loss 0.021 Time taken: 570.97 secs

    Epoch: 4 Validation Loss 0.06 Time taken: 22.51 secs| Accuracy 98.389

Epoch: 5 Train Loss 0.013 Time taken: 571.69 secs

    Epoch: 5 Validation Loss 0.07 Time taken: 22.64 secs| Accuracy 97.994
T\P      0       1
0       5166        0143        
1       0040        5268        

Accuracy 98.276
Time taken: 33.59 secs
3008.732 secs

Sun May  1 07:16:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=631
Sun May  1 07:17:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.96 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 631,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.985 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 573.94 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.62 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.06 Time taken: 572.99 secs

    Epoch: 2 Validation Loss 0.068 Time taken: 22.68 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.032 Time taken: 573.28 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.5 secs| Accuracy 98.333

Epoch: 4 Train Loss 0.02 Time taken: 572.7 secs

    Epoch: 4 Validation Loss 0.068 Time taken: 22.35 secs| Accuracy 98.446

Epoch: 5 Train Loss 0.013 Time taken: 570.49 secs

    Epoch: 5 Validation Loss 0.073 Time taken: 22.31 secs| Accuracy 98.432
T\P      0       1
0       5265        0044        
1       0103        5205        

Accuracy 98.615
Time taken: 33.22 secs
3009.311 secs

Sun May  1 08:08:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=635
Sun May  1 09:48:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 635,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.288 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 578.9 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.24 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.063 Time taken: 580.83 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.21 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.032 Time taken: 580.13 secs

    Epoch: 3 Validation Loss 0.072 Time taken: 22.12 secs| Accuracy 98.022

Epoch: 4 Train Loss 0.019 Time taken: 579.88 secs

    Epoch: 4 Validation Loss 0.065 Time taken: 22.09 secs| Accuracy 98.375

Epoch: 5 Train Loss 0.016 Time taken: 579.8 secs

    Epoch: 5 Validation Loss 0.06 Time taken: 22.13 secs| Accuracy 98.517
T\P      0       1
0       5227        0082        
1       0066        5242        

Accuracy 98.606
Time taken: 32.97 secs
3043.719 secs

Sun May  1 10:40:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=650
Sun May  1 10:40:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.209 GB
42 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 650,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-uncased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-uncased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.626 GB
Memory taken on RAM 4.973 GB
31 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 567.49 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.36 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.063 Time taken: 568.38 secs

    Epoch: 2 Validation Loss 0.069 Time taken: 22.28 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.038 Time taken: 567.46 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 22.4 secs| Accuracy 98.206
T\P      0       1
0       5270        0039        
1       0115        5193        

Accuracy 98.549
Time taken: 33.31 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.479 Time taken: 376.01 secs

    Epoch: 4 Validation Loss 0.287 Time taken: 1.46 secs| Accuracy 85.762
T\P      0       1
0       0166        0032        
1       0031        0225        

Accuracy 86.123
Time taken: 1.9 secs
2204.191 secs

Sun May  1 11:18:28 2022


======================================================================
----------------------------------------------------------------------
                run_ID=651
Sun May  1 11:18:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.406 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 651,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-uncased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-uncased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.626 GB
Memory taken on RAM 0.026 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.206 Time taken: 582.53 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 22.58 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.063 Time taken: 571.26 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.39 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.038 Time taken: 580.85 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.5 secs| Accuracy 98.375
T\P      0       1
0       5238        0071        
1       0071        5237        

Accuracy 98.663
Time taken: 33.5 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.446 Time taken: 394.03 secs

    Epoch: 4 Validation Loss 0.334 Time taken: 2.03 secs| Accuracy 85.762
T\P      0       1
0       0145        0053        
1       0019        0237        

Accuracy 84.141
Time taken: 2.45 secs
2261.157 secs

Sun May  1 11:57:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=652
Sun May  1 11:57:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.845 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 652,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-uncased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-uncased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.626 GB
Memory taken on RAM 4.942 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 598.94 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.81 secs| Accuracy 98.22

Epoch: 2 Train Loss 0.06 Time taken: 600.22 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 22.68 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.034 Time taken: 585.75 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 22.55 secs| Accuracy 98.29
T\P      0       1
0       5251        0058        
1       0071        5237        

Accuracy 98.785
Time taken: 33.48 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.528 Time taken: 382.33 secs

    Epoch: 4 Validation Loss 0.334 Time taken: 1.43 secs| Accuracy 84.437
T\P      0       1
0       0170        0028        
1       0042        0214        

Accuracy 84.581
Time taken: 1.84 secs
2294.525 secs

Sun May  1 12:36:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=653
Sun May  1 12:36:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.356 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 653,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-uncased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-uncased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.626 GB
Memory taken on RAM 5.665 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.196 Time taken: 595.21 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.68 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.062 Time taken: 598.47 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 22.72 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.035 Time taken: 596.1 secs

    Epoch: 3 Validation Loss 0.082 Time taken: 22.58 secs| Accuracy 97.824
T\P      0       1
0       5182        0127        
1       0050        5258        

Accuracy 98.333
Time taken: 33.7 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.525 Time taken: 398.28 secs

    Epoch: 4 Validation Loss 0.362 Time taken: 1.5 secs| Accuracy 83.775
T\P      0       1
0       0177        0021        
1       0055        0201        

Accuracy 83.26
Time taken: 2.0 secs
2316.37 secs

Sun May  1 13:16:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=654
Sun May  1 13:16:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.861 GB
34 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 654,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-uncased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-uncased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.626 GB
Memory taken on RAM 5.013 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.233 Time taken: 593.63 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 22.63 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.074 Time taken: 598.42 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.65 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.041 Time taken: 599.59 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 22.67 secs| Accuracy 98.29
T\P      0       1
0       5196        0113        
1       0076        5232        

Accuracy 98.22
Time taken: 33.71 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.527 Time taken: 400.85 secs

    Epoch: 4 Validation Loss 0.367 Time taken: 1.7 secs| Accuracy 83.775
T\P      0       1
0       0185        0013        
1       0055        0201        

Accuracy 85.022
Time taken: 2.06 secs
2319.615 secs

Sun May  1 13:55:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=655
Sun May  1 13:56:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.068 GB
41 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 655,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.133 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.694 Time taken: 502.95 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 22.95 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 514.58 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 22.93 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 514.83 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 22.87 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 34.12 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.694 Time taken: 407.49 secs

    Epoch: 4 Validation Loss 0.693 Time taken: 1.65 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.17 secs
2076.698 secs

Sun May  1 14:31:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=656
Sun May  1 14:31:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.031 GB
40 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 656,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.035 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 510.23 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 22.45 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 510.86 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 22.92 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 511.72 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 22.96 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 34.03 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.692 Time taken: 405.29 secs

    Epoch: 4 Validation Loss 0.691 Time taken: 1.72 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.11 secs
2074.425 secs

Sun May  1 15:07:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=657
Sun May  1 15:07:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.171 GB
41 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 657,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.202 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 513.66 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 23.05 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 503.4 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 23.03 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 513.43 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 23.02 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 34.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.693 Time taken: 400.4 secs

    Epoch: 4 Validation Loss 0.692 Time taken: 1.62 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.13 secs
2067.229 secs

Sun May  1 15:43:28 2022


======================================================================
----------------------------------------------------------------------
                run_ID=658
Sun May  1 15:43:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.242 GB
39 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 658,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.086 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 503.36 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 23.02 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 505.73 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 23.18 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 501.16 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 23.13 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 34.31 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.692 Time taken: 401.79 secs

    Epoch: 4 Validation Loss 0.69 Time taken: 1.83 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.28 secs
2051.272 secs

Sun May  1 16:18:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=659
Sun May  1 16:19:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.833 GB
40 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 659,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.146 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 508.36 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 23.08 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 510.35 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 23.07 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 505.92 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 22.58 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 33.72 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.693 Time taken: 402.17 secs

    Epoch: 4 Validation Loss 0.692 Time taken: 1.77 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.21 secs
2063.483 secs

Sun May  1 16:54:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=660
Sun May  1 16:54:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.046 GB
91 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 660,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-100-1280",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1280
}
myBert(
  (model): XLMModel(
 xlm-mlm-100-1280 )
Memory taken on GPU 2.162 GB
Memory taken on RAM 5.538 GB
31 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.69 Time taken: 1301.69 secs

    Epoch: 1 Validation Loss 0.672 Time taken: 60.5 secs| Accuracy 59.056

Epoch: 2 Train Loss 0.674 Time taken: 1302.47 secs

    Epoch: 2 Validation Loss 0.649 Time taken: 60.39 secs| Accuracy 62.207

Epoch: 3 Train Loss 0.66 Time taken: 1303.35 secs

    Epoch: 3 Validation Loss 0.639 Time taken: 60.35 secs| Accuracy 61.599
T\P      0       1
0       4356        0953        
1       3130        2178        

Accuracy 61.543
Time taken: 90.53 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.689 Time taken: 1007.51 secs

    Epoch: 4 Validation Loss 0.719 Time taken: 3.03 secs| Accuracy 55.629
T\P      0       1
0       0052        0146        
1       0069        0187        

Accuracy 52.643
Time taken: 4.41 secs
5221.182 secs

Sun May  1 18:23:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=661
Sun May  1 18:23:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.432 GB
94 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 661,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-100-1280",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1280
}
myBert(
  (model): XLMModel(
 xlm-mlm-100-1280 )
Memory taken on GPU 2.162 GB
Memory taken on RAM 5.413 GB
31 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.688 Time taken: 1302.96 secs

    Epoch: 1 Validation Loss 0.679 Time taken: 60.04 secs| Accuracy 53.038

Epoch: 2 Train Loss 0.673 Time taken: 1303.37 secs

    Epoch: 2 Validation Loss 0.652 Time taken: 60.52 secs| Accuracy 60.907

Epoch: 3 Train Loss 0.662 Time taken: 1303.48 secs

    Epoch: 3 Validation Loss 0.634 Time taken: 60.52 secs| Accuracy 63.987
T\P      0       1
0       3701        1608        
1       2262        3046        

Accuracy 63.549
Time taken: 90.51 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.695 Time taken: 1008.28 secs

    Epoch: 4 Validation Loss 0.718 Time taken: 3.1 secs| Accuracy 54.967
T\P      0       1
0       0043        0155        
1       0072        0184        

Accuracy 50.0
Time taken: 4.41 secs
5227.743 secs

Sun May  1 19:53:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=662
Sun May  1 19:53:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.425 GB
94 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 662,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-100-1280",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1280
}
myBert(
  (model): XLMModel(
 xlm-mlm-100-1280 )
Memory taken on GPU 2.162 GB
Memory taken on RAM 5.414 GB
32 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.686 Time taken: 1293.87 secs

    Epoch: 1 Validation Loss 0.666 Time taken: 59.92 secs| Accuracy 61.797

Epoch: 2 Train Loss 0.67 Time taken: 1291.69 secs

    Epoch: 2 Validation Loss 0.643 Time taken: 59.85 secs| Accuracy 63.069

Epoch: 3 Train Loss 0.657 Time taken: 1291.51 secs

    Epoch: 3 Validation Loss 0.631 Time taken: 59.81 secs| Accuracy 64.736
T\P      0       1
0       3267        2042        
1       1644        3664        

Accuracy 65.282
Time taken: 89.84 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.692 Time taken: 999.54 secs

    Epoch: 4 Validation Loss 0.715 Time taken: 2.99 secs| Accuracy 55.629
T\P      0       1
0       0032        0166        
1       0063        0193        

Accuracy 49.559
Time taken: 4.27 secs
5181.82 secs

Sun May  1 21:22:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=663
Sun May  1 21:22:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.456 GB
86 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 663,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-100-1280",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1280
}
myBert(
  (model): XLMModel(
 xlm-mlm-100-1280 )
Memory taken on GPU 2.162 GB
Memory taken on RAM 5.447 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.687 Time taken: 1295.02 secs

    Epoch: 1 Validation Loss 0.669 Time taken: 60.09 secs| Accuracy 61.769

Epoch: 2 Train Loss 0.672 Time taken: 1295.16 secs

    Epoch: 2 Validation Loss 0.647 Time taken: 60.27 secs| Accuracy 62.73

Epoch: 3 Train Loss 0.66 Time taken: 1296.53 secs

    Epoch: 3 Validation Loss 0.635 Time taken: 60.09 secs| Accuracy 62.828
T\P      0       1
0       4157        1152        
1       2717        2591        

Accuracy 63.558
Time taken: 90.11 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.683 Time taken: 1003.88 secs

    Epoch: 4 Validation Loss 0.719 Time taken: 3.17 secs| Accuracy 56.623
T\P      0       1
0       0038        0160        
1       0060        0196        

Accuracy 51.542
Time taken: 4.43 secs
5196.88 secs

Sun May  1 22:50:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=664
Sun May  1 22:50:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.301 GB
91 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 664,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-100-1280",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1280
}
myBert(
  (model): XLMModel(
 xlm-mlm-100-1280 )
Memory taken on GPU 2.162 GB
Memory taken on RAM 5.456 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.688 Time taken: 1291.82 secs

    Epoch: 1 Validation Loss 0.67 Time taken: 59.75 secs| Accuracy 60.681

Epoch: 2 Train Loss 0.673 Time taken: 1291.05 secs

    Epoch: 2 Validation Loss 0.646 Time taken: 60.06 secs| Accuracy 63.817

Epoch: 3 Train Loss 0.661 Time taken: 1294.3 secs

    Epoch: 3 Validation Loss 0.632 Time taken: 59.44 secs| Accuracy 64.651
T\P      0       1
0       3476        1833        
1       1963        3345        

Accuracy 64.246
Time taken: 89.2 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.685 Time taken: 1000.7 secs

    Epoch: 4 Validation Loss 0.711 Time taken: 3.05 secs| Accuracy 55.298
T\P      0       1
0       0030        0168        
1       0059        0197        

Accuracy 50.0
Time taken: 4.33 secs
5181.235 secs

Mon May  2 00:19:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=670
Wed May  4 12:00:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 7.473 GB
106 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 670,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 5.75 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.697 Time taken: 973.24 secs

    Epoch: 1 Validation Loss 0.707 Time taken: 31.39 secs| Accuracy 50.014

Epoch: 2 Train Loss 0.693 Time taken: 924.39 secs

    Epoch: 2 Validation Loss 0.691 Time taken: 31.33 secs| Accuracy 49.816

Epoch: 3 Train Loss 0.691 Time taken: 971.05 secs

    Epoch: 3 Validation Loss 0.684 Time taken: 31.44 secs| Accuracy 61.5
T\P      0       1
0       1648        3661        
1       0532        4776        

Accuracy 60.507
Time taken: 46.91 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.695 Time taken: 820.69 secs

    Epoch: 4 Validation Loss 0.687 Time taken: 1.9 secs| Accuracy 59.934
T\P      0       1
0       0009        0189        
1       0030        0226        

Accuracy 51.762
Time taken: 2.48 secs
3863.948 secs

Wed May  4 13:07:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=671
Wed May  4 13:07:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.178 GB
106 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 671,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 5.726 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.704 Time taken: 978.86 secs

    Epoch: 1 Validation Loss 0.691 Time taken: 31.46 secs| Accuracy 51.837

Epoch: 2 Train Loss 0.693 Time taken: 974.2 secs

    Epoch: 2 Validation Loss 0.684 Time taken: 31.51 secs| Accuracy 57.94

Epoch: 3 Train Loss 0.691 Time taken: 973.62 secs

    Epoch: 3 Validation Loss 0.676 Time taken: 31.49 secs| Accuracy 55.326
T\P      0       1
0       3451        1858        
1       2839        2469        

Accuracy 55.76
Time taken: 47.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.705 Time taken: 828.65 secs

    Epoch: 4 Validation Loss 0.724 Time taken: 1.98 secs| Accuracy 53.311
T\P      0       1
0       0011        0187        
1       0055        0201        

Accuracy 46.696
Time taken: 2.61 secs
3929.367 secs

Wed May  4 14:15:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=672
Wed May  4 14:15:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.933 GB
108 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 672,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 10.693 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.697 Time taken: 968.98 secs

    Epoch: 1 Validation Loss 0.683 Time taken: 31.4 secs| Accuracy 55.411

Epoch: 2 Train Loss 0.691 Time taken: 976.59 secs

    Epoch: 2 Validation Loss 0.674 Time taken: 31.45 secs| Accuracy 60.78

Epoch: 3 Train Loss 0.691 Time taken: 928.96 secs

    Epoch: 3 Validation Loss 0.68 Time taken: 31.22 secs| Accuracy 52.543
T\P      0       1
0       4596        0713        
1       4311        0997        

Accuracy 52.68
Time taken: 46.39 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.697 Time taken: 766.67 secs

    Epoch: 4 Validation Loss 0.699 Time taken: 1.95 secs| Accuracy 59.272
T\P      0       1
0       0021        0177        
1       0048        0208        

Accuracy 50.441
Time taken: 2.5 secs
3813.422 secs

Wed May  4 15:21:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=695
Thu May  5 00:35:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -1.054 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 695,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.375 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 207.4 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.85 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.052 Time taken: 210.6 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.94 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.031 Time taken: 210.09 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 23.0 secs| Accuracy 98.446
T\P      0       1
0       5237        0072        
1       0086        5222        

Accuracy 98.512
Time taken: 33.94 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.636 Time taken: 420.43 secs

    Epoch: 4 Validation Loss 0.316 Time taken: 2.04 secs| Accuracy 85.762
T\P      0       1
0       0153        0045        
1       0025        0231        

Accuracy 84.581
Time taken: 2.43 secs
1179.766 secs

Thu May  5 00:56:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=696
Thu May  5 17:49:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.24 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 696,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.786 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.664 Time taken: 401.59 secs

    Epoch: 1 Validation Loss 0.631 Time taken: 1.92 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.575 Time taken: 401.78 secs

    Epoch: 2 Validation Loss 0.516 Time taken: 1.68 secs| Accuracy 86.093

Epoch: 3 Train Loss 0.447 Time taken: 399.95 secs

    Epoch: 3 Validation Loss 0.432 Time taken: 1.89 secs| Accuracy 87.086

Epoch: 4 Train Loss 0.354 Time taken: 397.2 secs

    Epoch: 4 Validation Loss 0.422 Time taken: 1.91 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.306 Time taken: 396.29 secs

    Epoch: 5 Validation Loss 0.343 Time taken: 1.91 secs| Accuracy 89.073
T\P      0       1
0       0178        0020        
1       0032        0224        

Accuracy 88.546
Time taken: 2.41 secs
2008.543 secs

Thu May  5 18:23:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=697
Thu May  5 18:23:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.242 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 697,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.939 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.667 Time taken: 398.28 secs

    Epoch: 1 Validation Loss 0.625 Time taken: 1.73 secs| Accuracy 81.457

Epoch: 2 Train Loss 0.557 Time taken: 395.44 secs

    Epoch: 2 Validation Loss 0.538 Time taken: 1.88 secs| Accuracy 80.132

Epoch: 3 Train Loss 0.456 Time taken: 392.15 secs

    Epoch: 3 Validation Loss 0.469 Time taken: 1.81 secs| Accuracy 82.45

Epoch: 4 Train Loss 0.359 Time taken: 397.41 secs

    Epoch: 4 Validation Loss 0.475 Time taken: 1.71 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.316 Time taken: 397.58 secs

    Epoch: 5 Validation Loss 0.429 Time taken: 1.85 secs| Accuracy 81.788
T\P      0       1
0       0158        0040        
1       0013        0243        

Accuracy 88.326
Time taken: 2.22 secs
1992.094 secs

Thu May  5 18:57:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=698
Thu May  5 18:57:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.134 GB
23 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 698,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.737 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.676 Time taken: 396.85 secs

    Epoch: 1 Validation Loss 0.635 Time taken: 1.87 secs| Accuracy 79.139

Epoch: 2 Train Loss 0.567 Time taken: 396.34 secs

    Epoch: 2 Validation Loss 0.511 Time taken: 1.78 secs| Accuracy 86.424

Epoch: 3 Train Loss 0.443 Time taken: 396.14 secs

    Epoch: 3 Validation Loss 0.414 Time taken: 1.84 secs| Accuracy 87.748

Epoch: 4 Train Loss 0.337 Time taken: 392.7 secs

    Epoch: 4 Validation Loss 0.38 Time taken: 1.83 secs| Accuracy 89.073

Epoch: 5 Train Loss 0.282 Time taken: 394.79 secs

    Epoch: 5 Validation Loss 0.345 Time taken: 1.88 secs| Accuracy 89.735
T\P      0       1
0       0183        0015        
1       0029        0227        

Accuracy 90.308
Time taken: 2.41 secs
1988.453 secs

Thu May  5 19:31:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=699
Thu May  5 19:31:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.08 GB
22 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 699,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.87 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.674 Time taken: 395.29 secs

    Epoch: 1 Validation Loss 0.635 Time taken: 1.89 secs| Accuracy 84.106

Epoch: 2 Train Loss 0.578 Time taken: 397.01 secs

    Epoch: 2 Validation Loss 0.529 Time taken: 1.81 secs| Accuracy 86.424

Epoch: 3 Train Loss 0.462 Time taken: 394.39 secs

    Epoch: 3 Validation Loss 0.439 Time taken: 1.87 secs| Accuracy 88.079

Epoch: 4 Train Loss 0.364 Time taken: 461.81 secs

    Epoch: 4 Validation Loss 0.401 Time taken: 1.71 secs| Accuracy 88.079

Epoch: 5 Train Loss 0.297 Time taken: 494.0 secs

    Epoch: 5 Validation Loss 0.366 Time taken: 1.84 secs| Accuracy 87.417
T\P      0       1
0       0177        0021        
1       0033        0223        

Accuracy 88.106
Time taken: 2.16 secs
2153.814 secs

Thu May  5 20:08:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=700
Thu May  5 20:08:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.147 GB
23 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 700,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.799 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.66 Time taken: 395.88 secs

    Epoch: 1 Validation Loss 0.609 Time taken: 1.8 secs| Accuracy 81.788

Epoch: 2 Train Loss 0.54 Time taken: 397.15 secs

    Epoch: 2 Validation Loss 0.492 Time taken: 1.81 secs| Accuracy 86.093

Epoch: 3 Train Loss 0.432 Time taken: 397.41 secs

    Epoch: 3 Validation Loss 0.42 Time taken: 1.96 secs| Accuracy 87.748

Epoch: 4 Train Loss 0.351 Time taken: 395.87 secs

    Epoch: 4 Validation Loss 0.388 Time taken: 1.84 secs| Accuracy 87.417

Epoch: 5 Train Loss 0.271 Time taken: 396.05 secs

    Epoch: 5 Validation Loss 0.385 Time taken: 1.81 secs| Accuracy 85.43
T\P      0       1
0       0170        0028        
1       0026        0230        

Accuracy 88.106
Time taken: 2.24 secs
1993.825 secs

Thu May  5 20:42:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=701
Thu May  5 20:42:45 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.887 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 701,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.0 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 598.11 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.96 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.057 Time taken: 600.15 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.98 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.034 Time taken: 593.06 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.96 secs| Accuracy 98.163

Epoch: 4 Train Loss 0.019 Time taken: 599.72 secs

    Epoch: 4 Validation Loss 0.078 Time taken: 23.08 secs| Accuracy 98.107

Epoch: 5 Train Loss 0.013 Time taken: 596.12 secs

    Epoch: 5 Validation Loss 0.083 Time taken: 23.12 secs| Accuracy 98.347
T\P      0       1
0       5247        0062        
1       0082        5226        

Accuracy 98.644
Time taken: 34.26 secs
3137.0 secs

Thu May  5 21:36:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=702
Thu May  5 21:36:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.922 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 702,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.918 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 600.17 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.0 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.061 Time taken: 596.73 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.95 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.034 Time taken: 597.89 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 22.92 secs| Accuracy 98.375

Epoch: 4 Train Loss 0.021 Time taken: 601.26 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.16 secs| Accuracy 98.686

Epoch: 5 Train Loss 0.014 Time taken: 598.39 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 22.96 secs| Accuracy 98.615
T\P      0       1
0       5242        0067        
1       0064        5244        

Accuracy 98.766
Time taken: 33.92 secs
3143.789 secs

Thu May  5 22:29:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=703
Thu May  5 22:29:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.619 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 703,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.878 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 602.37 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.86 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.063 Time taken: 601.55 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.98 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.036 Time taken: 600.68 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.96 secs| Accuracy 98.29

Epoch: 4 Train Loss 0.023 Time taken: 600.77 secs

    Epoch: 4 Validation Loss 0.064 Time taken: 23.15 secs| Accuracy 98.375

Epoch: 5 Train Loss 0.017 Time taken: 601.03 secs

    Epoch: 5 Validation Loss 0.063 Time taken: 22.94 secs| Accuracy 98.531
T\P      0       1
0       5211        0098        
1       0061        5247        

Accuracy 98.502
Time taken: 33.85 secs
3155.454 secs

Thu May  5 23:23:13 2022




======================================================================
----------------------------------------------------------------------
                run_ID=704
Thu May  5 23:55:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.905 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 704,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.221 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.192 Time taken: 593.01 secs

    Epoch: 1 Validation Loss 0.084 Time taken: 23.09 secs| Accuracy 97.415

Epoch: 2 Train Loss 0.058 Time taken: 602.06 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 23.0 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.033 Time taken: 599.75 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 23.18 secs| Accuracy 98.404

Epoch: 4 Train Loss 0.02 Time taken: 597.74 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 22.91 secs| Accuracy 98.22

Epoch: 5 Train Loss 0.016 Time taken: 599.46 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 23.12 secs| Accuracy 98.531
T\P      0       1
0       5246        0063        
1       0073        5235        

Accuracy 98.719
Time taken: 33.81 secs
3141.631 secs

Fri May  6 00:48:24 2022


======================================================================
----------------------------------------------------------------------
                run_ID=705
Fri May  6 00:48:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.789 GB
32 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 705,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.094 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 597.78 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 23.27 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.06 Time taken: 600.58 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 23.13 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.035 Time taken: 597.2 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.18 secs| Accuracy 98.333

Epoch: 4 Train Loss 0.023 Time taken: 599.43 secs

    Epoch: 4 Validation Loss 0.06 Time taken: 23.04 secs| Accuracy 98.474

Epoch: 5 Train Loss 0.017 Time taken: 601.96 secs

    Epoch: 5 Validation Loss 0.07 Time taken: 23.04 secs| Accuracy 98.248
T\P      0       1
0       5211        0098        
1       0054        5254        

Accuracy 98.568
Time taken: 34.09 secs
3147.205 secs

Fri May  6 01:41:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=706
Fri May  6 01:42:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.839 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 706,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.016 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.136 Time taken: 599.38 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.82 secs| Accuracy 97.853

Epoch: 2 Train Loss 0.053 Time taken: 597.18 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.1 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.03 Time taken: 600.34 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 23.18 secs| Accuracy 98.361
T\P      0       1
0       5260        0049        
1       0092        5216        

Accuracy 98.672
Time taken: 34.02 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.552 Time taken: 397.05 secs

    Epoch: 4 Validation Loss 0.311 Time taken: 2.0 secs| Accuracy 81.457
T\P      0       1
0       0140        0058        
1       0014        0242        

Accuracy 84.141
Time taken: 2.44 secs
2323.026 secs

Fri May  6 02:21:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=707
Fri May  6 02:21:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.887 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 707,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.984 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.136 Time taken: 602.77 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 23.18 secs| Accuracy 97.528

Epoch: 2 Train Loss 0.055 Time taken: 601.11 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.0 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.031 Time taken: 595.35 secs

    Epoch: 3 Validation Loss 0.059 Time taken: 22.88 secs| Accuracy 98.036
T\P      0       1
0       5210        0099        
1       0058        5250        

Accuracy 98.521
Time taken: 33.85 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.515 Time taken: 399.76 secs

    Epoch: 4 Validation Loss 0.371 Time taken: 1.89 secs| Accuracy 85.099
T\P      0       1
0       0181        0017        
1       0060        0196        

Accuracy 83.04
Time taken: 2.25 secs
2328.726 secs

Fri May  6 03:01:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=708
Fri May  6 03:01:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.853 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 708,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.04 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.136 Time taken: 593.25 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.03 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.053 Time taken: 595.35 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 23.04 secs| Accuracy 98.149

Epoch: 3 Train Loss 0.032 Time taken: 597.57 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.89 secs| Accuracy 98.347
T\P      0       1
0       5207        0102        
1       0065        5243        

Accuracy 98.427
Time taken: 34.07 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.532 Time taken: 393.92 secs

    Epoch: 4 Validation Loss 0.295 Time taken: 2.05 secs| Accuracy 88.411
T\P      0       1
0       0168        0030        
1       0037        0219        

Accuracy 85.242
Time taken: 2.34 secs
2309.173 secs

Fri May  6 03:41:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=709
Fri May  6 03:41:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.817 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 709,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.97 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.148 Time taken: 612.84 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 23.51 secs| Accuracy 97.994

Epoch: 2 Train Loss 0.049 Time taken: 608.36 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.48 secs| Accuracy 97.966

Epoch: 3 Train Loss 0.026 Time taken: 602.1 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 23.34 secs| Accuracy 98.474
T\P      0       1
0       5255        0054        
1       0093        5215        

Accuracy 98.615
Time taken: 34.45 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.543 Time taken: 409.41 secs

    Epoch: 4 Validation Loss 0.303 Time taken: 2.61 secs| Accuracy 86.755
T\P      0       1
0       0160        0038        
1       0029        0227        

Accuracy 85.242
Time taken: 3.05 secs
2365.077 secs

Fri May  6 04:21:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=710
Fri May  6 04:21:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.865 GB
31 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 710,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.046 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.143 Time taken: 594.34 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 22.95 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.05 Time taken: 595.36 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 22.87 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.027 Time taken: 594.11 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.03 secs| Accuracy 98.432
T\P      0       1
0       5240        0069        
1       0073        5235        

Accuracy 98.663
Time taken: 33.89 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.524 Time taken: 392.52 secs

    Epoch: 4 Validation Loss 0.304 Time taken: 1.97 secs| Accuracy 86.424
T\P      0       1
0       0157        0041        
1       0017        0239        

Accuracy 87.225
Time taken: 2.44 secs
2305.863 secs

Fri May  6 05:01:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=741
Fri May  6 05:01:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.944 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 741,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.854 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 593.67 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.82 secs| Accuracy 97.909
T\P      0       1
0       5219        0090        
1       0100        5208        

Accuracy 98.21
Time taken: 33.92 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.494 Time taken: 395.95 secs

    Epoch: 2 Validation Loss 0.34 Time taken: 1.97 secs| Accuracy 85.099
T\P      0       1
0       0172        0026        
1       0047        0209        

Accuracy 83.921
Time taken: 2.33 secs
1071.859 secs

Fri May  6 05:20:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=742
Fri May  6 05:20:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.709 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 742,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.115 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 587.15 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.69 secs| Accuracy 98.008
T\P      0       1
0       5246        0063        
1       0124        5184        

Accuracy 98.239
Time taken: 33.82 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.495 Time taken: 397.75 secs

    Epoch: 2 Validation Loss 0.354 Time taken: 1.85 secs| Accuracy 84.106
T\P      0       1
0       0172        0026        
1       0054        0202        

Accuracy 82.379
Time taken: 2.36 secs
1067.71 secs

Fri May  6 05:39:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=743
Fri May  6 05:39:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.913 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 743,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.142 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 601.32 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.95 secs| Accuracy 97.923
T\P      0       1
0       5255        0054        
1       0114        5194        

Accuracy 98.418
Time taken: 34.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.486 Time taken: 404.26 secs

    Epoch: 2 Validation Loss 0.343 Time taken: 1.76 secs| Accuracy 84.106
T\P      0       1
0       0169        0029        
1       0031        0225        

Accuracy 86.784
Time taken: 2.27 secs
1088.72 secs

Fri May  6 05:58:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=744
Fri May  6 05:58:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.465 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 744,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.054 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 597.63 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.93 secs| Accuracy 97.951
T\P      0       1
0       5252        0057        
1       0112        5196        

Accuracy 98.408
Time taken: 34.09 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.467 Time taken: 401.83 secs

    Epoch: 2 Validation Loss 0.38 Time taken: 1.81 secs| Accuracy 83.444
T\P      0       1
0       0167        0031        
1       0035        0221        

Accuracy 85.463
Time taken: 2.31 secs
1082.146 secs

Fri May  6 06:17:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=745
Fri May  6 06:17:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.415 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 745,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.905 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 597.35 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.89 secs| Accuracy 97.937
T\P      0       1
0       5276        0033        
1       0148        5160        

Accuracy 98.295
Time taken: 33.78 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.426 Time taken: 396.42 secs

    Epoch: 2 Validation Loss 0.349 Time taken: 1.91 secs| Accuracy 82.45
T\P      0       1
0       0147        0051        
1       0014        0242        

Accuracy 85.683
Time taken: 2.28 secs
1076.336 secs

Fri May  6 06:36:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=756
Sat May  7 09:36:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.049 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 756,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.868 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 568.3 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.61 secs| Accuracy 98.05
T\P      0       1
0       5243        0066        
1       0104        5204        

Accuracy 98.399
Time taken: 33.29 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.536 Time taken: 373.72 secs

    Epoch: 2 Validation Loss 0.319 Time taken: 1.71 secs| Accuracy 85.099
T\P      0       1
0       0173        0025        
1       0051        0205        

Accuracy 83.26
Time taken: 2.26 secs
1024.138 secs

Sat May  7 09:53:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=757
Sat May  7 09:54:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.844 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 757,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.005 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 570.25 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 22.61 secs| Accuracy 98.093
T\P      0       1
0       5231        0078        
1       0107        5201        

Accuracy 98.258
Time taken: 33.66 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.442 Time taken: 373.0 secs

    Epoch: 2 Validation Loss 0.365 Time taken: 1.77 secs| Accuracy 82.45
T\P      0       1
0       0167        0031        
1       0024        0232        

Accuracy 87.885
Time taken: 2.2 secs
1026.456 secs

Sat May  7 10:12:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=758
Sat May  7 10:12:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.86 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 758,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.985 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 571.39 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.73 secs| Accuracy 98.079
T\P      0       1
0       5257        0052        
1       0124        5184        

Accuracy 98.342
Time taken: 33.58 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.513 Time taken: 373.07 secs

    Epoch: 2 Validation Loss 0.368 Time taken: 1.82 secs| Accuracy 84.106
T\P      0       1
0       0171        0027        
1       0050        0206        

Accuracy 83.04
Time taken: 2.35 secs
1028.566 secs

Sat May  7 10:30:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=759
Sat May  7 10:30:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.863 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 759,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.985 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.192 Time taken: 568.56 secs

    Epoch: 1 Validation Loss 0.084 Time taken: 22.62 secs| Accuracy 97.415
T\P      0       1
0       5161        0148        
1       0066        5242        

Accuracy 97.984
Time taken: 33.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.508 Time taken: 372.36 secs

    Epoch: 2 Validation Loss 0.313 Time taken: 1.69 secs| Accuracy 85.099
T\P      0       1
0       0166        0032        
1       0040        0216        

Accuracy 84.141
Time taken: 2.2 secs
1023.364 secs

Sat May  7 10:48:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=760
Sat May  7 10:48:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.857 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 760,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 1,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.981 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 569.26 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.72 secs| Accuracy 98.149
T\P      0       1
0       5257        0052        
1       0128        5180        

Accuracy 98.305
Time taken: 33.73 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 2 Train Loss 0.422 Time taken: 372.84 secs

    Epoch: 2 Validation Loss 0.331 Time taken: 1.82 secs| Accuracy 85.099
T\P      0       1
0       0145        0053        
1       0012        0244        

Accuracy 85.683
Time taken: 2.34 secs
1027.509 secs

Sat May  7 11:06:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=781
Fri May 13 11:51:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.99 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 781,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.153 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.152 Time taken: 266.54 secs

    Epoch: 1 Validation Loss 0.045 Time taken: 36.45 secs| Accuracy 98.645

Epoch: 2 Train Loss 0.047 Time taken: 266.37 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 35.96 secs| Accuracy 98.619

Epoch: 3 Train Loss 0.025 Time taken: 266.03 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 35.66 secs| Accuracy 98.804

Epoch: 4 Train Loss 0.015 Time taken: 266.6 secs

    Epoch: 4 Validation Loss 0.049 Time taken: 35.83 secs| Accuracy 98.654
T\P      0       1
0       0488        0012        
1       0010        0390        

Accuracy 97.556
Time taken: 3.52 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1223.688 secs

Fri May 13 12:12:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=782
Fri May 13 12:12:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.95 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 782,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 0.211 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.152 Time taken: 266.81 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 36.37 secs| Accuracy 98.381

Epoch: 2 Train Loss 0.045 Time taken: 265.82 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 35.9 secs| Accuracy 98.812

Epoch: 3 Train Loss 0.026 Time taken: 264.91 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 36.06 secs| Accuracy 98.636

Epoch: 4 Train Loss 0.015 Time taken: 264.53 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 35.83 secs| Accuracy 98.892
T\P      0       1
0       0485        0015        
1       0007        0393        

Accuracy 97.556
Time taken: 3.5 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1220.674 secs

Fri May 13 12:34:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=783
Fri May 13 12:34:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.632 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 783,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.052 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.152 Time taken: 266.68 secs

    Epoch: 1 Validation Loss 0.049 Time taken: 36.23 secs| Accuracy 98.399

Epoch: 2 Train Loss 0.048 Time taken: 266.48 secs

    Epoch: 2 Validation Loss 0.037 Time taken: 35.83 secs| Accuracy 98.874

Epoch: 3 Train Loss 0.026 Time taken: 264.52 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 35.7 secs| Accuracy 98.777

Epoch: 4 Train Loss 0.016 Time taken: 263.87 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 35.75 secs| Accuracy 98.865
T\P      0       1
0       0472        0028        
1       0005        0395        

Accuracy 96.333
Time taken: 3.44 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1219.421 secs

Fri May 13 12:55:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=784
Fri May 13 12:55:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.614 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 784,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.113 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.15 Time taken: 268.01 secs

    Epoch: 1 Validation Loss 0.05 Time taken: 36.15 secs| Accuracy 98.452

Epoch: 2 Train Loss 0.044 Time taken: 266.65 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 35.84 secs| Accuracy 98.557

Epoch: 3 Train Loss 0.025 Time taken: 264.71 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 36.05 secs| Accuracy 98.768

Epoch: 4 Train Loss 0.015 Time taken: 265.09 secs

    Epoch: 4 Validation Loss 0.049 Time taken: 35.79 secs| Accuracy 98.751
T\P      0       1
0       0473        0027        
1       0005        0395        

Accuracy 96.444
Time taken: 3.6 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1222.947 secs

Fri May 13 13:16:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=785
Fri May 13 13:16:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.503 GB
27 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 785,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.896 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.158 Time taken: 267.79 secs

    Epoch: 1 Validation Loss 0.044 Time taken: 36.28 secs| Accuracy 98.777

Epoch: 2 Train Loss 0.046 Time taken: 266.46 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 35.91 secs| Accuracy 98.795

Epoch: 3 Train Loss 0.026 Time taken: 265.21 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 35.87 secs| Accuracy 98.751

Epoch: 4 Train Loss 0.017 Time taken: 265.32 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 35.87 secs| Accuracy 98.636
T\P      0       1
0       0468        0032        
1       0003        0397        

Accuracy 96.111
Time taken: 3.49 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1223.976 secs

Fri May 13 13:38:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=786
Fri May 13 13:38:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.225 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 786,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.26 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.595 Time taken: 21.43 secs

    Epoch: 1 Validation Loss 0.405 Time taken: 2.92 secs| Accuracy 98.4

Epoch: 2 Train Loss 0.284 Time taken: 22.2 secs

    Epoch: 2 Validation Loss 0.201 Time taken: 3.02 secs| Accuracy 98.4

Epoch: 3 Train Loss 0.15 Time taken: 22.46 secs

    Epoch: 3 Validation Loss 0.126 Time taken: 3.04 secs| Accuracy 98.4

Epoch: 4 Train Loss 0.092 Time taken: 21.71 secs

    Epoch: 4 Validation Loss 0.093 Time taken: 2.88 secs| Accuracy 98.4
T\P      0       1
0       0493        0007        
1       0003        0397        

Accuracy 98.889
Time taken: 3.42 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
114.267 secs

Fri May 13 13:40:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=787
Fri May 13 13:40:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.225 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 787,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.243 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.555 Time taken: 21.04 secs

    Epoch: 1 Validation Loss 0.392 Time taken: 2.78 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.289 Time taken: 21.89 secs

    Epoch: 2 Validation Loss 0.222 Time taken: 2.93 secs| Accuracy 98.133

Epoch: 3 Train Loss 0.154 Time taken: 21.77 secs

    Epoch: 3 Validation Loss 0.138 Time taken: 2.86 secs| Accuracy 98.4

Epoch: 4 Train Loss 0.096 Time taken: 21.67 secs

    Epoch: 4 Validation Loss 0.12 Time taken: 2.82 secs| Accuracy 97.733
T\P      0       1
0       0492        0008        
1       0003        0397        

Accuracy 98.778
Time taken: 3.3 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
111.5 secs

Fri May 13 13:43:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=788
Fri May 13 13:43:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.221 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 788,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.251 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.573 Time taken: 21.1 secs

    Epoch: 1 Validation Loss 0.406 Time taken: 2.82 secs| Accuracy 98.133

Epoch: 2 Train Loss 0.295 Time taken: 21.94 secs

    Epoch: 2 Validation Loss 0.217 Time taken: 2.91 secs| Accuracy 98.533

Epoch: 3 Train Loss 0.164 Time taken: 21.86 secs

    Epoch: 3 Validation Loss 0.14 Time taken: 2.84 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.098 Time taken: 21.52 secs

    Epoch: 4 Validation Loss 0.095 Time taken: 2.8 secs| Accuracy 98.8
T\P      0       1
0       0485        0015        
1       0004        0396        

Accuracy 97.889
Time taken: 3.3 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
111.56 secs

Fri May 13 13:46:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=789
Fri May 13 13:46:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.228 GB
23 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 789,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.252 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.537 Time taken: 21.71 secs

    Epoch: 1 Validation Loss 0.364 Time taken: 2.92 secs| Accuracy 98.933

Epoch: 2 Train Loss 0.262 Time taken: 22.59 secs

    Epoch: 2 Validation Loss 0.194 Time taken: 3.01 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.138 Time taken: 22.44 secs

    Epoch: 3 Validation Loss 0.139 Time taken: 3.0 secs| Accuracy 97.733

Epoch: 4 Train Loss 0.085 Time taken: 22.16 secs

    Epoch: 4 Validation Loss 0.096 Time taken: 2.91 secs| Accuracy 98.667
T\P      0       1
0       0484        0016        
1       0002        0398        

Accuracy 98.0
Time taken: 3.42 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
115.267 secs

Fri May 13 13:49:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=790
Fri May 13 13:49:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.228 GB
23 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 790,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.236 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.556 Time taken: 21.42 secs

    Epoch: 1 Validation Loss 0.389 Time taken: 2.81 secs| Accuracy 98.533

Epoch: 2 Train Loss 0.285 Time taken: 22.2 secs

    Epoch: 2 Validation Loss 0.205 Time taken: 2.96 secs| Accuracy 99.067

Epoch: 3 Train Loss 0.154 Time taken: 22.04 secs

    Epoch: 3 Validation Loss 0.131 Time taken: 2.92 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.092 Time taken: 21.91 secs

    Epoch: 4 Validation Loss 0.093 Time taken: 2.78 secs| Accuracy 98.667
T\P      0       1
0       0487        0013        
1       0003        0397        

Accuracy 98.222
Time taken: 3.33 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
112.756 secs

Fri May 13 13:51:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=791
Fri May 13 13:51:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.109 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 791,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.24 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.664 Time taken: 10.97 secs

    Epoch: 1 Validation Loss 0.61 Time taken: 1.79 secs| Accuracy 65.25

Epoch: 2 Train Loss 0.537 Time taken: 11.01 secs

    Epoch: 2 Validation Loss 0.465 Time taken: 1.83 secs| Accuracy 96.5

Epoch: 3 Train Loss 0.385 Time taken: 11.14 secs

    Epoch: 3 Validation Loss 0.338 Time taken: 1.85 secs| Accuracy 96.25
T\P      0       1
0       0194        0006        
1       0009        0191        

Accuracy 96.25
Time taken: 1.79 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.246 Time taken: 22.07 secs

    Epoch: 4 Validation Loss 0.17 Time taken: 3.04 secs| Accuracy 98.8

Epoch: 5 Train Loss 0.134 Time taken: 22.57 secs

    Epoch: 5 Validation Loss 0.114 Time taken: 2.95 secs| Accuracy 98.4
T\P      0       1
0       0486        0014        
1       0004        0396        

Accuracy 98.0
Time taken: 3.4 secs
117.118 secs

Fri May 13 13:54:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=792
Fri May 13 13:54:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.113 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 792,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.229 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.663 Time taken: 11.18 secs

    Epoch: 1 Validation Loss 0.595 Time taken: 1.9 secs| Accuracy 95.25

Epoch: 2 Train Loss 0.511 Time taken: 11.29 secs

    Epoch: 2 Validation Loss 0.451 Time taken: 1.89 secs| Accuracy 94.0

Epoch: 3 Train Loss 0.384 Time taken: 11.41 secs

    Epoch: 3 Validation Loss 0.353 Time taken: 1.91 secs| Accuracy 95.25
T\P      0       1
0       0188        0012        
1       0008        0192        

Accuracy 95.0
Time taken: 1.95 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.245 Time taken: 22.28 secs

    Epoch: 4 Validation Loss 0.187 Time taken: 3.1 secs| Accuracy 98.4

Epoch: 5 Train Loss 0.135 Time taken: 23.0 secs

    Epoch: 5 Validation Loss 0.131 Time taken: 3.03 secs| Accuracy 98.0
T\P      0       1
0       0489        0011        
1       0004        0396        

Accuracy 98.333
Time taken: 3.46 secs
119.048 secs

Fri May 13 13:57:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=793
Fri May 13 13:57:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.023 GB
22 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 793,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.103 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.664 Time taken: 10.87 secs

    Epoch: 1 Validation Loss 0.606 Time taken: 1.72 secs| Accuracy 92.25

Epoch: 2 Train Loss 0.531 Time taken: 11.11 secs

    Epoch: 2 Validation Loss 0.466 Time taken: 1.77 secs| Accuracy 95.0

Epoch: 3 Train Loss 0.396 Time taken: 10.91 secs

    Epoch: 3 Validation Loss 0.357 Time taken: 1.79 secs| Accuracy 95.5
T\P      0       1
0       0191        0009        
1       0010        0190        

Accuracy 95.25
Time taken: 1.77 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.245 Time taken: 22.1 secs

    Epoch: 4 Validation Loss 0.175 Time taken: 2.98 secs| Accuracy 98.4

Epoch: 5 Train Loss 0.124 Time taken: 22.21 secs

    Epoch: 5 Validation Loss 0.118 Time taken: 2.91 secs| Accuracy 98.0
T\P      0       1
0       0468        0032        
1       0003        0397        

Accuracy 96.111
Time taken: 3.35 secs
115.341 secs

Fri May 13 14:00:18 2022


======================================================================
----------------------------------------------------------------------
                run_ID=794
Fri May 13 14:00:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.699 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 794,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.258 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.669 Time taken: 11.05 secs

    Epoch: 1 Validation Loss 0.612 Time taken: 1.91 secs| Accuracy 95.0

Epoch: 2 Train Loss 0.523 Time taken: 11.19 secs

    Epoch: 2 Validation Loss 0.447 Time taken: 1.87 secs| Accuracy 95.25

Epoch: 3 Train Loss 0.369 Time taken: 11.28 secs

    Epoch: 3 Validation Loss 0.35 Time taken: 1.91 secs| Accuracy 94.25
T\P      0       1
0       0193        0007        
1       0011        0189        

Accuracy 95.5
Time taken: 1.94 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.221 Time taken: 22.18 secs

    Epoch: 4 Validation Loss 0.158 Time taken: 3.03 secs| Accuracy 98.933

Epoch: 5 Train Loss 0.128 Time taken: 22.6 secs

    Epoch: 5 Validation Loss 0.104 Time taken: 2.97 secs| Accuracy 98.667
T\P      0       1
0       0488        0012        
1       0009        0391        

Accuracy 97.667
Time taken: 3.43 secs
117.019 secs

Fri May 13 14:03:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=795
Fri May 13 14:03:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.068 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 795,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.298 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.652 Time taken: 11.16 secs

    Epoch: 1 Validation Loss 0.583 Time taken: 1.89 secs| Accuracy 94.75

Epoch: 2 Train Loss 0.515 Time taken: 11.07 secs

    Epoch: 2 Validation Loss 0.451 Time taken: 1.9 secs| Accuracy 96.25

Epoch: 3 Train Loss 0.38 Time taken: 11.2 secs

    Epoch: 3 Validation Loss 0.341 Time taken: 1.94 secs| Accuracy 96.0
T\P      0       1
0       0194        0006        
1       0008        0192        

Accuracy 96.5
Time taken: 1.91 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.237 Time taken: 22.09 secs

    Epoch: 4 Validation Loss 0.174 Time taken: 3.09 secs| Accuracy 98.533

Epoch: 5 Train Loss 0.124 Time taken: 22.8 secs

    Epoch: 5 Validation Loss 0.104 Time taken: 2.99 secs| Accuracy 98.933
T\P      0       1
0       0486        0014        
1       0004        0396        

Accuracy 98.0
Time taken: 3.48 secs
119.584 secs

Fri May 13 14:05:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=796
Fri May 13 14:06:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.76 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 796,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.157 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 208.34 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.87 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.057 Time taken: 207.25 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.58 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.034 Time taken: 206.08 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.55 secs| Accuracy 98.163

Epoch: 4 Train Loss 0.019 Time taken: 206.04 secs

    Epoch: 4 Validation Loss 0.078 Time taken: 22.66 secs| Accuracy 98.107
T\P      0       1
0       5221        0088        
1       0068        5240        

Accuracy 98.531
Time taken: 33.7 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
962.904 secs

Fri May 13 14:23:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=797
Fri May 13 14:23:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.559 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 797,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.404 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 209.02 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.74 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 207.99 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.39 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.034 Time taken: 206.68 secs

    Epoch: 3 Validation Loss 0.07 Time taken: 22.45 secs| Accuracy 98.036

Epoch: 4 Train Loss 0.021 Time taken: 206.74 secs

    Epoch: 4 Validation Loss 0.06 Time taken: 22.45 secs| Accuracy 98.389
T\P      0       1
0       5231        0078        
1       0073        5235        

Accuracy 98.578
Time taken: 33.49 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
964.725 secs

Fri May 13 14:40:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=798
Fri May 13 14:40:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.768 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 798,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 7.106 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 208.62 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.74 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.06 Time taken: 207.35 secs

    Epoch: 2 Validation Loss 0.068 Time taken: 22.38 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.032 Time taken: 206.61 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.42 secs| Accuracy 98.333

Epoch: 4 Train Loss 0.02 Time taken: 205.89 secs

    Epoch: 4 Validation Loss 0.068 Time taken: 22.42 secs| Accuracy 98.446
T\P      0       1
0       5209        0100        
1       0053        5255        

Accuracy 98.559
Time taken: 33.38 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
962.475 secs

Fri May 13 14:57:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=799
Fri May 13 14:57:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.098 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 799,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.379 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 208.46 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.67 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 207.64 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.41 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.033 Time taken: 206.55 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.36 secs| Accuracy 98.29

Epoch: 4 Train Loss 0.02 Time taken: 205.98 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 22.46 secs| Accuracy 98.276
T\P      0       1
0       5249        0060        
1       0078        5230        

Accuracy 98.7
Time taken: 33.43 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
962.947 secs

Fri May 13 15:13:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=800
Fri May 13 15:14:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.048 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 800,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.988 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 208.82 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.68 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.056 Time taken: 208.02 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.4 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.032 Time taken: 206.36 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.43 secs| Accuracy 98.474

Epoch: 4 Train Loss 0.02 Time taken: 207.49 secs

    Epoch: 4 Validation Loss 0.056 Time taken: 22.44 secs| Accuracy 98.46
T\P      0       1
0       5257        0052        
1       0085        5223        

Accuracy 98.71
Time taken: 33.34 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
964.673 secs

Fri May 13 15:31:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=821
Fri May 13 15:55:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.879 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 821,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.388 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 362.85 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.37 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.057 Time taken: 208.74 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.72 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.034 Time taken: 207.07 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 23.06 secs| Accuracy 98.163
T\P      0       1
0       5224        0085        
1       0060        5248        

Accuracy 98.634
Time taken: 33.72 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.065 Time taken: 21.86 secs

    Epoch: 4 Validation Loss 0.054 Time taken: 2.95 secs| Accuracy 98.0

Epoch: 5 Train Loss 0.025 Time taken: 22.78 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 2.96 secs| Accuracy 98.533
T\P      0       1
0       0493        0007        
1       0007        0393        

Accuracy 98.444
Time taken: 3.45 secs
956.124 secs

Fri May 13 16:12:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=822
Fri May 13 16:12:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 822,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.986 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 208.54 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.77 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 207.57 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.49 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.034 Time taken: 207.87 secs

    Epoch: 3 Validation Loss 0.07 Time taken: 22.41 secs| Accuracy 98.036
T\P      0       1
0       5284        0025        
1       0154        5154        

Accuracy 98.314
Time taken: 33.38 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.05 Time taken: 21.61 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 2.96 secs| Accuracy 98.4

Epoch: 5 Train Loss 0.016 Time taken: 22.79 secs

    Epoch: 5 Validation Loss 0.062 Time taken: 2.96 secs| Accuracy 98.4
T\P      0       1
0       0490        0010        
1       0005        0395        

Accuracy 98.333
Time taken: 3.4 secs
800.481 secs

Fri May 13 16:26:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=823
Fri May 13 16:26:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.871 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 823,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.952 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 208.34 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.7 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.06 Time taken: 206.9 secs

    Epoch: 2 Validation Loss 0.068 Time taken: 22.35 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.032 Time taken: 206.23 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.4 secs| Accuracy 98.333
T\P      0       1
0       5251        0058        
1       0073        5235        

Accuracy 98.766
Time taken: 33.51 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.057 Time taken: 21.65 secs

    Epoch: 4 Validation Loss 0.056 Time taken: 3.01 secs| Accuracy 98.267

Epoch: 5 Train Loss 0.027 Time taken: 22.96 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 2.99 secs| Accuracy 98.667
T\P      0       1
0       0484        0016        
1       0004        0396        

Accuracy 97.778
Time taken: 3.41 secs
798.308 secs

Fri May 13 16:40:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=824
Fri May 13 16:40:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.842 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 824,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.995 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 208.74 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.97 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 208.07 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.78 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.033 Time taken: 207.42 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.65 secs| Accuracy 98.29
T\P      0       1
0       5240        0069        
1       0078        5230        

Accuracy 98.615
Time taken: 33.61 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.051 Time taken: 22.45 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 3.13 secs| Accuracy 98.533

Epoch: 5 Train Loss 0.013 Time taken: 23.48 secs

    Epoch: 5 Validation Loss 0.046 Time taken: 3.14 secs| Accuracy 98.533
T\P      0       1
0       0490        0010        
1       0004        0396        

Accuracy 98.444
Time taken: 3.63 secs
804.59 secs

Fri May 13 16:55:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=825
Fri May 13 16:55:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.861 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 825,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.993 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 208.18 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.76 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.056 Time taken: 207.84 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.41 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.032 Time taken: 207.09 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.41 secs| Accuracy 98.474
T\P      0       1
0       5227        0082        
1       0068        5240        

Accuracy 98.587
Time taken: 33.27 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.058 Time taken: 21.8 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 2.93 secs| Accuracy 98.4

Epoch: 5 Train Loss 0.021 Time taken: 22.64 secs

    Epoch: 5 Validation Loss 0.052 Time taken: 2.94 secs| Accuracy 98.533
T\P      0       1
0       0491        0009        
1       0005        0395        

Accuracy 98.444
Time taken: 3.39 secs
800.772 secs

Fri May 13 17:09:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=831
Fri May 13 17:10:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.029 GB
23 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 831,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.948 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.594 Time taken: 22.86 secs

    Epoch: 1 Validation Loss 0.405 Time taken: 3.03 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.283 Time taken: 23.25 secs

    Epoch: 2 Validation Loss 0.198 Time taken: 3.09 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.146 Time taken: 24.23 secs

    Epoch: 3 Validation Loss 0.129 Time taken: 2.94 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.093 Time taken: 23.62 secs

    Epoch: 4 Validation Loss 0.099 Time taken: 3.27 secs| Accuracy 98.267
T\P      0       1
0       0482        0018        
1       0002        0398        

Accuracy 97.778
Time taken: 3.54 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
127.252 secs

Fri May 13 17:13:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=832
Fri May 13 17:13:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.014 GB
23 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 832,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.497 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.555 Time taken: 23.27 secs

    Epoch: 1 Validation Loss 0.391 Time taken: 3.01 secs| Accuracy 98.533

Epoch: 2 Train Loss 0.286 Time taken: 23.56 secs

    Epoch: 2 Validation Loss 0.227 Time taken: 2.86 secs| Accuracy 98.0

Epoch: 3 Train Loss 0.153 Time taken: 23.1 secs

    Epoch: 3 Validation Loss 0.136 Time taken: 3.04 secs| Accuracy 98.533

Epoch: 4 Train Loss 0.095 Time taken: 23.09 secs

    Epoch: 4 Validation Loss 0.126 Time taken: 2.87 secs| Accuracy 97.6
T\P      0       1
0       0476        0024        
1       0002        0398        

Accuracy 97.111
Time taken: 3.26 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.468 secs

Fri May 13 17:16:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=833
Fri May 13 17:16:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.083 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 833,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.86 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.572 Time taken: 23.19 secs

    Epoch: 1 Validation Loss 0.406 Time taken: 2.89 secs| Accuracy 98.133

Epoch: 2 Train Loss 0.296 Time taken: 23.95 secs

    Epoch: 2 Validation Loss 0.221 Time taken: 2.91 secs| Accuracy 98.267

Epoch: 3 Train Loss 0.163 Time taken: 23.33 secs

    Epoch: 3 Validation Loss 0.147 Time taken: 2.89 secs| Accuracy 97.733

Epoch: 4 Train Loss 0.102 Time taken: 23.21 secs

    Epoch: 4 Validation Loss 0.104 Time taken: 2.91 secs| Accuracy 98.267
T\P      0       1
0       0483        0017        
1       0004        0396        

Accuracy 97.667
Time taken: 3.31 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
119.614 secs

Fri May 13 17:19:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=834
Fri May 13 17:19:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.045 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 834,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.842 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.536 Time taken: 22.81 secs

    Epoch: 1 Validation Loss 0.367 Time taken: 2.81 secs| Accuracy 98.667

Epoch: 2 Train Loss 0.264 Time taken: 23.71 secs

    Epoch: 2 Validation Loss 0.209 Time taken: 2.86 secs| Accuracy 98.0

Epoch: 3 Train Loss 0.148 Time taken: 22.98 secs

    Epoch: 3 Validation Loss 0.126 Time taken: 2.8 secs| Accuracy 98.4

Epoch: 4 Train Loss 0.099 Time taken: 22.91 secs

    Epoch: 4 Validation Loss 0.088 Time taken: 2.8 secs| Accuracy 98.4
T\P      0       1
0       0489        0011        
1       0005        0395        

Accuracy 98.222
Time taken: 3.42 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
117.634 secs

Fri May 13 17:22:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=835
Fri May 13 17:22:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.111 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 835,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.944 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.556 Time taken: 22.89 secs

    Epoch: 1 Validation Loss 0.39 Time taken: 2.84 secs| Accuracy 98.533

Epoch: 2 Train Loss 0.286 Time taken: 23.91 secs

    Epoch: 2 Validation Loss 0.208 Time taken: 2.89 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.156 Time taken: 23.16 secs

    Epoch: 3 Validation Loss 0.132 Time taken: 2.91 secs| Accuracy 98.667

Epoch: 4 Train Loss 0.092 Time taken: 23.28 secs

    Epoch: 4 Validation Loss 0.09 Time taken: 2.85 secs| Accuracy 98.8
T\P      0       1
0       0486        0014        
1       0003        0397        

Accuracy 98.111
Time taken: 3.33 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.677 secs

Fri May 13 17:25:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=841
Fri May 13 17:26:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.117 GB
23 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 841,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 1.304 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.599 Time taken: 22.52 secs

    Epoch: 1 Validation Loss 0.391 Time taken: 2.75 secs| Accuracy 98.8

Epoch: 2 Train Loss 0.242 Time taken: 22.86 secs

    Epoch: 2 Validation Loss 0.136 Time taken: 2.86 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.115 Time taken: 24.06 secs

    Epoch: 3 Validation Loss 0.099 Time taken: 2.78 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.072 Time taken: 22.8 secs

    Epoch: 4 Validation Loss 0.081 Time taken: 2.79 secs| Accuracy 98.267
T\P      0       1
0       0489        0011        
1       0002        0398        

Accuracy 98.556
Time taken: 3.3 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.366 secs

Fri May 13 17:29:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=842
Fri May 13 17:29:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.349 GB
23 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 842,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.869 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.572 Time taken: 22.66 secs

    Epoch: 1 Validation Loss 0.31 Time taken: 2.78 secs| Accuracy 98.533

Epoch: 2 Train Loss 0.21 Time taken: 23.6 secs

    Epoch: 2 Validation Loss 0.149 Time taken: 2.88 secs| Accuracy 98.133

Epoch: 3 Train Loss 0.104 Time taken: 23.05 secs

    Epoch: 3 Validation Loss 0.108 Time taken: 2.74 secs| Accuracy 98.133

Epoch: 4 Train Loss 0.069 Time taken: 22.87 secs

    Epoch: 4 Validation Loss 0.081 Time taken: 2.82 secs| Accuracy 98.667
T\P      0       1
0       0494        0006        
1       0004        0396        

Accuracy 98.889
Time taken: 3.3 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
117.016 secs

Fri May 13 17:32:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=843
Fri May 13 17:32:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.086 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 843,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.89 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.554 Time taken: 23.06 secs

    Epoch: 1 Validation Loss 0.284 Time taken: 2.86 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.202 Time taken: 23.84 secs

    Epoch: 2 Validation Loss 0.14 Time taken: 2.86 secs| Accuracy 98.133

Epoch: 3 Train Loss 0.111 Time taken: 23.06 secs

    Epoch: 3 Validation Loss 0.107 Time taken: 2.81 secs| Accuracy 98.133

Epoch: 4 Train Loss 0.074 Time taken: 23.0 secs

    Epoch: 4 Validation Loss 0.108 Time taken: 2.85 secs| Accuracy 97.733
T\P      0       1
0       0466        0034        
1       0002        0398        

Accuracy 96.0
Time taken: 3.31 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.198 secs

Fri May 13 17:34:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=844
Fri May 13 17:35:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.245 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 844,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.863 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.544 Time taken: 22.54 secs

    Epoch: 1 Validation Loss 0.273 Time taken: 2.8 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.185 Time taken: 23.63 secs

    Epoch: 2 Validation Loss 0.122 Time taken: 2.83 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.095 Time taken: 23.09 secs

    Epoch: 3 Validation Loss 0.085 Time taken: 2.85 secs| Accuracy 98.8

Epoch: 4 Train Loss 0.06 Time taken: 23.06 secs

    Epoch: 4 Validation Loss 0.076 Time taken: 2.85 secs| Accuracy 98.4
T\P      0       1
0       0494        0006        
1       0004        0396        

Accuracy 98.889
Time taken: 3.27 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
117.597 secs

Fri May 13 17:37:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=845
Fri May 13 17:37:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.113 GB
23 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 845,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.912 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.556 Time taken: 22.66 secs

    Epoch: 1 Validation Loss 0.289 Time taken: 2.94 secs| Accuracy 98.667

Epoch: 2 Train Loss 0.202 Time taken: 23.77 secs

    Epoch: 2 Validation Loss 0.133 Time taken: 2.85 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.106 Time taken: 23.0 secs

    Epoch: 3 Validation Loss 0.088 Time taken: 2.84 secs| Accuracy 98.933

Epoch: 4 Train Loss 0.07 Time taken: 23.04 secs

    Epoch: 4 Validation Loss 0.077 Time taken: 2.78 secs| Accuracy 98.533
T\P      0       1
0       0484        0016        
1       0002        0398        

Accuracy 98.0
Time taken: 3.35 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
117.878 secs

Fri May 13 17:40:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=846
Fri May 13 17:40:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.112 GB
24 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 846,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.894 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.632 Time taken: 22.8 secs

    Epoch: 1 Validation Loss 0.466 Time taken: 2.93 secs| Accuracy 91.2

Epoch: 2 Train Loss 0.348 Time taken: 23.9 secs

    Epoch: 2 Validation Loss 0.263 Time taken: 2.79 secs| Accuracy 92.267

Epoch: 3 Train Loss 0.197 Time taken: 22.97 secs

    Epoch: 3 Validation Loss 0.249 Time taken: 2.93 secs| Accuracy 91.333

Epoch: 4 Train Loss 0.135 Time taken: 23.03 secs

    Epoch: 4 Validation Loss 0.213 Time taken: 2.79 secs| Accuracy 92.933
T\P      0       1
0       0447        0053        
1       0019        0381        

Accuracy 92.0
Time taken: 3.2 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.93 secs

Fri May 13 17:43:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=847
Fri May 13 17:43:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.116 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 847,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.718 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.61 Time taken: 23.1 secs

    Epoch: 1 Validation Loss 0.389 Time taken: 2.81 secs| Accuracy 92.8

Epoch: 2 Train Loss 0.3 Time taken: 24.04 secs

    Epoch: 2 Validation Loss 0.259 Time taken: 2.89 secs| Accuracy 92.0

Epoch: 3 Train Loss 0.18 Time taken: 23.37 secs

    Epoch: 3 Validation Loss 0.221 Time taken: 2.86 secs| Accuracy 92.8

Epoch: 4 Train Loss 0.154 Time taken: 23.01 secs

    Epoch: 4 Validation Loss 0.268 Time taken: 2.86 secs| Accuracy 91.467
T\P      0       1
0       0412        0088        
1       0006        0394        

Accuracy 89.556
Time taken: 3.31 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.629 secs

Fri May 13 17:46:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=848
Fri May 13 17:46:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -5.242 GB
23 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 848,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.598 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.593 Time taken: 23.04 secs

    Epoch: 1 Validation Loss 0.375 Time taken: 2.92 secs| Accuracy 91.2

Epoch: 2 Train Loss 0.302 Time taken: 23.83 secs

    Epoch: 2 Validation Loss 0.269 Time taken: 2.97 secs| Accuracy 91.067

Epoch: 3 Train Loss 0.195 Time taken: 23.24 secs

    Epoch: 3 Validation Loss 0.239 Time taken: 2.99 secs| Accuracy 91.467

Epoch: 4 Train Loss 0.137 Time taken: 23.69 secs

    Epoch: 4 Validation Loss 0.239 Time taken: 3.03 secs| Accuracy 91.333
T\P      0       1
0       0448        0052        
1       0014        0386        

Accuracy 92.667
Time taken: 3.44 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
119.634 secs

Fri May 13 17:49:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=849
Fri May 13 17:49:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.297 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 849,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.074 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.588 Time taken: 22.8 secs

    Epoch: 1 Validation Loss 0.373 Time taken: 2.79 secs| Accuracy 92.133

Epoch: 2 Train Loss 0.294 Time taken: 23.77 secs

    Epoch: 2 Validation Loss 0.238 Time taken: 2.83 secs| Accuracy 92.533

Epoch: 3 Train Loss 0.186 Time taken: 22.92 secs

    Epoch: 3 Validation Loss 0.212 Time taken: 2.76 secs| Accuracy 92.8

Epoch: 4 Train Loss 0.124 Time taken: 22.82 secs

    Epoch: 4 Validation Loss 0.29 Time taken: 2.79 secs| Accuracy 91.2
T\P      0       1
0       0467        0033        
1       0032        0368        

Accuracy 92.778
Time taken: 3.26 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.041 secs

Fri May 13 17:52:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=850
Fri May 13 17:52:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.287 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 850,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.998 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.605 Time taken: 22.78 secs

    Epoch: 1 Validation Loss 0.402 Time taken: 2.91 secs| Accuracy 91.467

Epoch: 2 Train Loss 0.308 Time taken: 23.81 secs

    Epoch: 2 Validation Loss 0.252 Time taken: 2.83 secs| Accuracy 92.533

Epoch: 3 Train Loss 0.189 Time taken: 23.05 secs

    Epoch: 3 Validation Loss 0.215 Time taken: 2.81 secs| Accuracy 93.067

Epoch: 4 Train Loss 0.124 Time taken: 23.0 secs

    Epoch: 4 Validation Loss 0.228 Time taken: 2.84 secs| Accuracy 92.533
T\P      0       1
0       0445        0055        
1       0013        0387        

Accuracy 92.444
Time taken: 3.33 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
118.17 secs

Fri May 13 17:54:53 2022



======================================================================
----------------------------------------------------------------------
                run_ID=901
Mon May 16 00:21:04 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.058 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 901,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.107 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=901
Mon May 16 00:39:10 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.042 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 901,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.909 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.122 Time taken: 654.51 secs

    Epoch: 1 Validation Loss 0.051 Time taken: 92.16 secs| Accuracy 98.372

Epoch: 2 Train Loss 0.043 Time taken: 666.53 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 89.49 secs| Accuracy 98.619

Epoch: 3 Train Loss 0.021 Time taken: 665.75 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 89.69 secs| Accuracy 98.865

Epoch: 4 Train Loss 0.012 Time taken: 664.55 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 91.4 secs| Accuracy 98.936
T\P      0       1
0       0313        0187        
1       0006        0394        

Accuracy 78.556
Time taken: 7.54 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
3035.986 secs

Mon May 16 01:30:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=902
Mon May 16 01:30:41 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.547 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 902,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.853 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.119 Time taken: 663.39 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 90.98 secs| Accuracy 98.364

Epoch: 2 Train Loss 0.04 Time taken: 667.05 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 89.42 secs| Accuracy 98.724

Epoch: 3 Train Loss 0.021 Time taken: 670.74 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 89.63 secs| Accuracy 98.821

Epoch: 4 Train Loss 0.015 Time taken: 671.95 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 89.58 secs| Accuracy 98.654
T\P      0       1
0       0282        0218        
1       0005        0395        

Accuracy 75.222
Time taken: 7.35 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
3054.633 secs

Mon May 16 02:22:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=903
Mon May 16 02:22:33 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.14 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 903,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.914 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.121 Time taken: 666.7 secs

    Epoch: 1 Validation Loss 0.045 Time taken: 89.37 secs| Accuracy 98.584

Epoch: 2 Train Loss 0.04 Time taken: 670.49 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 89.35 secs| Accuracy 98.733

Epoch: 3 Train Loss 0.023 Time taken: 668.7 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 89.36 secs| Accuracy 98.575

Epoch: 4 Train Loss 0.014 Time taken: 667.65 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 89.29 secs| Accuracy 98.768
T\P      0       1
0       0283        0217        
1       0005        0395        

Accuracy 75.333
Time taken: 7.43 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
3052.91 secs

Mon May 16 03:14:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=904
Mon May 16 03:14:22 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.155 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 904,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.774 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.12 Time taken: 667.99 secs

    Epoch: 1 Validation Loss 0.044 Time taken: 89.55 secs| Accuracy 98.592

Epoch: 2 Train Loss 0.041 Time taken: 672.7 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 89.17 secs| Accuracy 98.724

Epoch: 3 Train Loss 0.022 Time taken: 670.6 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 89.51 secs| Accuracy 98.707

Epoch: 4 Train Loss 0.012 Time taken: 670.48 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 89.68 secs| Accuracy 98.856
T\P      0       1
0       0343        0157        
1       0009        0391        

Accuracy 81.556
Time taken: 7.68 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
3062.154 secs

Mon May 16 04:06:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=905
Mon May 16 04:06:21 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.165 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 905,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.864 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.121 Time taken: 672.56 secs

    Epoch: 1 Validation Loss 0.041 Time taken: 90.1 secs| Accuracy 98.707

Epoch: 2 Train Loss 0.039 Time taken: 675.36 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 89.88 secs| Accuracy 98.68

Epoch: 3 Train Loss 0.02 Time taken: 674.02 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 89.89 secs| Accuracy 98.874

Epoch: 4 Train Loss 0.014 Time taken: 671.96 secs

    Epoch: 4 Validation Loss 0.056 Time taken: 88.51 secs| Accuracy 98.689
T\P      0       1
0       0267        0233        
1       0004        0396        

Accuracy 73.667
Time taken: 7.63 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
3074.564 secs

Mon May 16 04:58:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=906
Mon May 16 04:58:30 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.082 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 906,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.966 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.262 Time taken: 641.48 secs

    Epoch: 1 Validation Loss 0.219 Time taken: 89.92 secs| Accuracy 91.238

Epoch: 2 Train Loss 0.101 Time taken: 646.9 secs

    Epoch: 2 Validation Loss 0.235 Time taken: 89.42 secs| Accuracy 91.783

Epoch: 3 Train Loss 0.055 Time taken: 642.99 secs

    Epoch: 3 Validation Loss 0.233 Time taken: 89.16 secs| Accuracy 91.563

Epoch: 4 Train Loss 0.037 Time taken: 641.99 secs

    Epoch: 4 Validation Loss 0.254 Time taken: 88.98 secs| Accuracy 91.801
T\P      0       1
0       0334        0166        
1       0071        0329        

Accuracy 73.667
Time taken: 7.63 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2953.19 secs

Mon May 16 05:48:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=907
Mon May 16 05:48:39 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.498 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 907,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.796 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.267 Time taken: 641.0 secs

    Epoch: 1 Validation Loss 0.227 Time taken: 88.96 secs| Accuracy 90.965

Epoch: 2 Train Loss 0.104 Time taken: 642.21 secs

    Epoch: 2 Validation Loss 0.2 Time taken: 88.95 secs| Accuracy 92.302

Epoch: 3 Train Loss 0.06 Time taken: 643.34 secs

    Epoch: 3 Validation Loss 0.262 Time taken: 88.47 secs| Accuracy 91.519

Epoch: 4 Train Loss 0.037 Time taken: 636.18 secs

    Epoch: 4 Validation Loss 0.228 Time taken: 87.73 secs| Accuracy 92.716
T\P      0       1
0       0422        0078        
1       0078        0322        

Accuracy 82.667
Time taken: 7.56 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2939.033 secs

Mon May 16 06:38:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=908
Mon May 16 06:38:32 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.066 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 908,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.176 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.272 Time taken: 627.5 secs

    Epoch: 1 Validation Loss 0.226 Time taken: 87.14 secs| Accuracy 90.983

Epoch: 2 Train Loss 0.106 Time taken: 626.65 secs

    Epoch: 2 Validation Loss 0.187 Time taken: 86.51 secs| Accuracy 92.918

Epoch: 3 Train Loss 0.06 Time taken: 623.72 secs

    Epoch: 3 Validation Loss 0.233 Time taken: 86.09 secs| Accuracy 92.03

Epoch: 4 Train Loss 0.039 Time taken: 621.64 secs

    Epoch: 4 Validation Loss 0.255 Time taken: 85.77 secs| Accuracy 91.95
T\P      0       1
0       0361        0139        
1       0088        0312        

Accuracy 74.778
Time taken: 7.41 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2867.203 secs

Mon May 16 07:27:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=909
Mon May 16 07:27:18 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.318 GB
32 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 909,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.025 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.271 Time taken: 616.73 secs

    Epoch: 1 Validation Loss 0.246 Time taken: 85.68 secs| Accuracy 90.314

Epoch: 2 Train Loss 0.109 Time taken: 618.79 secs

    Epoch: 2 Validation Loss 0.256 Time taken: 86.3 secs| Accuracy 89.945

Epoch: 3 Train Loss 0.064 Time taken: 619.79 secs

    Epoch: 3 Validation Loss 0.334 Time taken: 85.45 secs| Accuracy 90.27

Epoch: 4 Train Loss 0.04 Time taken: 616.72 secs

    Epoch: 4 Validation Loss 0.312 Time taken: 85.25 secs| Accuracy 90.428
T\P      0       1
0       0284        0216        
1       0045        0355        

Accuracy 71.0
Time taken: 7.37 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2836.439 secs

Mon May 16 08:15:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=910
Mon May 16 08:15:35 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 1658 | Validation_batches: 711 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.035 GB
27 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 910,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.901 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.262 Time taken: 614.28 secs

    Epoch: 1 Validation Loss 0.265 Time taken: 85.2 secs| Accuracy 89.346

Epoch: 2 Train Loss 0.105 Time taken: 615.66 secs

    Epoch: 2 Validation Loss 0.264 Time taken: 84.84 secs| Accuracy 90.015

Epoch: 3 Train Loss 0.061 Time taken: 616.32 secs

    Epoch: 3 Validation Loss 0.262 Time taken: 85.05 secs| Accuracy 91.818

Epoch: 4 Train Loss 0.038 Time taken: 616.82 secs

    Epoch: 4 Validation Loss 0.314 Time taken: 85.55 secs| Accuracy 90.851
T\P      0       1
0       0332        0168        
1       0093        0307        

Accuracy 71.0
Time taken: 7.43 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2825.834 secs

Mon May 16 09:03:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=911
Mon May 16 09:03:35 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.922 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 911,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.838 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.32 Time taken: 480.0 secs

    Epoch: 1 Validation Loss 0.175 Time taken: 53.22 secs| Accuracy 93.642

Epoch: 2 Train Loss 0.123 Time taken: 482.38 secs

    Epoch: 2 Validation Loss 0.191 Time taken: 53.03 secs| Accuracy 92.978

Epoch: 3 Train Loss 0.067 Time taken: 480.54 secs

    Epoch: 3 Validation Loss 0.144 Time taken: 52.9 secs| Accuracy 95.112

Epoch: 4 Train Loss 0.046 Time taken: 481.17 secs

    Epoch: 4 Validation Loss 0.177 Time taken: 52.9 secs| Accuracy 94.688
T\P      0       1
0       5163        0146        
1       0408        4900        

Accuracy 94.782
Time taken: 79.03 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2229.908 secs

Mon May 16 09:41:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=912
Mon May 16 09:41:42 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.978 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 912,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.03 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.323 Time taken: 476.65 secs

    Epoch: 1 Validation Loss 0.172 Time taken: 52.93 secs| Accuracy 93.303

Epoch: 2 Train Loss 0.12 Time taken: 480.55 secs

    Epoch: 2 Validation Loss 0.163 Time taken: 52.92 secs| Accuracy 93.444

Epoch: 3 Train Loss 0.064 Time taken: 480.53 secs

    Epoch: 3 Validation Loss 0.148 Time taken: 52.87 secs| Accuracy 94.589

Epoch: 4 Train Loss 0.041 Time taken: 482.22 secs

    Epoch: 4 Validation Loss 0.161 Time taken: 53.21 secs| Accuracy 95.168
T\P      0       1
0       5094        0215        
1       0311        4997        

Accuracy 95.046
Time taken: 79.47 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2226.573 secs

Mon May 16 10:19:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=913
Mon May 16 10:19:45 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.851 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 913,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.978 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.329 Time taken: 478.65 secs

    Epoch: 1 Validation Loss 0.179 Time taken: 53.09 secs| Accuracy 93.218

Epoch: 2 Train Loss 0.127 Time taken: 482.23 secs

    Epoch: 2 Validation Loss 0.14 Time taken: 52.93 secs| Accuracy 94.716

Epoch: 3 Train Loss 0.068 Time taken: 480.82 secs

    Epoch: 3 Validation Loss 0.131 Time taken: 52.92 secs| Accuracy 95.267

Epoch: 4 Train Loss 0.045 Time taken: 484.2 secs

    Epoch: 4 Validation Loss 0.183 Time taken: 53.79 secs| Accuracy 94.363
T\P      0       1
0       5071        0238        
1       0335        4973        

Accuracy 94.603
Time taken: 80.37 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2233.513 secs

Mon May 16 10:57:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=914
Mon May 16 10:57:53 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.846 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 914,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.032 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 486.42 secs

    Epoch: 1 Validation Loss 0.183 Time taken: 54.31 secs| Accuracy 92.724

Epoch: 2 Train Loss 0.123 Time taken: 491.94 secs

    Epoch: 2 Validation Loss 0.138 Time taken: 54.41 secs| Accuracy 94.702

Epoch: 3 Train Loss 0.066 Time taken: 495.34 secs

    Epoch: 3 Validation Loss 0.184 Time taken: 54.73 secs| Accuracy 93.981

Epoch: 4 Train Loss 0.047 Time taken: 495.43 secs

    Epoch: 4 Validation Loss 0.168 Time taken: 54.86 secs| Accuracy 95.126
T\P      0       1
0       5059        0250        
1       0264        5044        

Accuracy 95.159
Time taken: 81.9 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2284.071 secs

Mon May 16 11:36:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=915
Mon May 16 11:36:52 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.739 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 915,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.101 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.307 Time taken: 496.21 secs

    Epoch: 1 Validation Loss 0.178 Time taken: 55.37 secs| Accuracy 92.795

Epoch: 2 Train Loss 0.117 Time taken: 501.73 secs

    Epoch: 2 Validation Loss 0.133 Time taken: 55.53 secs| Accuracy 94.843

Epoch: 3 Train Loss 0.066 Time taken: 503.02 secs

    Epoch: 3 Validation Loss 0.152 Time taken: 55.53 secs| Accuracy 94.645

Epoch: 4 Train Loss 0.041 Time taken: 502.58 secs

    Epoch: 4 Validation Loss 0.137 Time taken: 55.56 secs| Accuracy 95.196
T\P      0       1
0       5033        0276        
1       0196        5112        

Accuracy 95.554
Time taken: 83.04 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2323.175 secs

Mon May 16 12:16:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=916
Mon May 16 12:16:31 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.986 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 916,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.111 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.32 Time taken: 514.99 secs

    Epoch: 1 Validation Loss 0.175 Time taken: 55.94 secs| Accuracy 93.642

Epoch: 2 Train Loss 0.123 Time taken: 507.14 secs

    Epoch: 2 Validation Loss 0.191 Time taken: 56.0 secs| Accuracy 92.978

Epoch: 3 Train Loss 0.067 Time taken: 506.94 secs

    Epoch: 3 Validation Loss 0.144 Time taken: 55.98 secs| Accuracy 95.112

Epoch: 4 Train Loss 0.046 Time taken: 504.5 secs

    Epoch: 4 Validation Loss 0.177 Time taken: 55.75 secs| Accuracy 94.688
T\P      0       1
0       5163        0146        
1       0408        4900        

Accuracy 94.782
Time taken: 83.29 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.251 Time taken: 48.99 secs

    Epoch: 5 Validation Loss 0.195 Time taken: 6.39 secs| Accuracy 92.0

Epoch: 6 Train Loss 0.114 Time taken: 51.33 secs

    Epoch: 6 Validation Loss 0.224 Time taken: 6.43 secs| Accuracy 91.6
T\P      0       1
0       0440        0060        
1       0022        0378        

Accuracy 90.889
Time taken: 7.72 secs
2483.509 secs

Mon May 16 12:58:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=917
Mon May 16 12:58:49 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.163 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 917,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.573 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.323 Time taken: 503.76 secs

    Epoch: 1 Validation Loss 0.172 Time taken: 56.07 secs| Accuracy 93.303

Epoch: 2 Train Loss 0.12 Time taken: 505.97 secs

    Epoch: 2 Validation Loss 0.163 Time taken: 55.92 secs| Accuracy 93.444

Epoch: 3 Train Loss 0.064 Time taken: 504.91 secs

    Epoch: 3 Validation Loss 0.148 Time taken: 55.91 secs| Accuracy 94.589

Epoch: 4 Train Loss 0.041 Time taken: 504.71 secs

    Epoch: 4 Validation Loss 0.161 Time taken: 55.77 secs| Accuracy 95.168
T\P      0       1
0       5094        0215        
1       0311        4997        

Accuracy 95.046
Time taken: 83.21 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.24 Time taken: 48.83 secs

    Epoch: 5 Validation Loss 0.19 Time taken: 6.52 secs| Accuracy 92.667

Epoch: 6 Train Loss 0.114 Time taken: 51.32 secs

    Epoch: 6 Validation Loss 0.241 Time taken: 6.57 secs| Accuracy 92.4
T\P      0       1
0       0462        0038        
1       0032        0368        

Accuracy 92.222
Time taken: 7.78 secs
2469.197 secs

Mon May 16 13:40:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=918
Mon May 16 13:40:52 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.904 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 918,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.037 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.329 Time taken: 500.29 secs

    Epoch: 1 Validation Loss 0.179 Time taken: 55.63 secs| Accuracy 93.218

Epoch: 2 Train Loss 0.127 Time taken: 501.21 secs

    Epoch: 2 Validation Loss 0.14 Time taken: 55.4 secs| Accuracy 94.716

Epoch: 3 Train Loss 0.068 Time taken: 500.78 secs

    Epoch: 3 Validation Loss 0.131 Time taken: 55.45 secs| Accuracy 95.267

Epoch: 4 Train Loss 0.045 Time taken: 502.81 secs

    Epoch: 4 Validation Loss 0.183 Time taken: 55.74 secs| Accuracy 94.363
T\P      0       1
0       5071        0238        
1       0335        4973        

Accuracy 94.603
Time taken: 83.24 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.235 Time taken: 48.6 secs

    Epoch: 5 Validation Loss 0.207 Time taken: 6.44 secs| Accuracy 92.267

Epoch: 6 Train Loss 0.114 Time taken: 51.17 secs

    Epoch: 6 Validation Loss 0.231 Time taken: 6.53 secs| Accuracy 91.333
T\P      0       1
0       0449        0051        
1       0036        0364        

Accuracy 90.333
Time taken: 7.7 secs
2453.062 secs

Mon May 16 14:22:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=919
Mon May 16 14:22:40 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.862 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 919,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.961 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 502.34 secs

    Epoch: 1 Validation Loss 0.183 Time taken: 55.63 secs| Accuracy 92.724

Epoch: 2 Train Loss 0.123 Time taken: 504.28 secs

    Epoch: 2 Validation Loss 0.138 Time taken: 55.54 secs| Accuracy 94.702

Epoch: 3 Train Loss 0.066 Time taken: 505.29 secs

    Epoch: 3 Validation Loss 0.184 Time taken: 55.57 secs| Accuracy 93.981

Epoch: 4 Train Loss 0.047 Time taken: 503.39 secs

    Epoch: 4 Validation Loss 0.168 Time taken: 55.57 secs| Accuracy 95.126
T\P      0       1
0       5059        0250        
1       0264        5044        

Accuracy 95.159
Time taken: 82.87 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.288 Time taken: 48.4 secs

    Epoch: 5 Validation Loss 0.211 Time taken: 6.39 secs| Accuracy 92.0

Epoch: 6 Train Loss 0.149 Time taken: 51.17 secs

    Epoch: 6 Validation Loss 0.205 Time taken: 6.41 secs| Accuracy 92.667
T\P      0       1
0       0442        0058        
1       0020        0380        

Accuracy 91.333
Time taken: 7.53 secs
2462.136 secs

Mon May 16 15:04:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=920
Mon May 16 15:04:39 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.859 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 920,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.069 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.307 Time taken: 500.16 secs

    Epoch: 1 Validation Loss 0.178 Time taken: 55.37 secs| Accuracy 92.795

Epoch: 2 Train Loss 0.117 Time taken: 501.66 secs

    Epoch: 2 Validation Loss 0.133 Time taken: 55.33 secs| Accuracy 94.843

Epoch: 3 Train Loss 0.066 Time taken: 501.87 secs

    Epoch: 3 Validation Loss 0.152 Time taken: 55.41 secs| Accuracy 94.645

Epoch: 4 Train Loss 0.041 Time taken: 501.49 secs

    Epoch: 4 Validation Loss 0.137 Time taken: 55.35 secs| Accuracy 95.196
T\P      0       1
0       5033        0276        
1       0196        5112        

Accuracy 95.554
Time taken: 82.8 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.244 Time taken: 47.84 secs

    Epoch: 5 Validation Loss 0.199 Time taken: 6.18 secs| Accuracy 91.467

Epoch: 6 Train Loss 0.121 Time taken: 50.49 secs

    Epoch: 6 Validation Loss 0.216 Time taken: 6.22 secs| Accuracy 91.867
T\P      0       1
0       0442        0058        
1       0023        0377        

Accuracy 91.0
Time taken: 7.45 secs
2449.578 secs

Mon May 16 15:46:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=900
Mon May 16 17:49:34 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 270
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 17
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.913 GB
25 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 900,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM -1.065 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=900
Mon May 16 17:54:50 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.757 GB
25 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 900,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.023 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.15 Time taken: 516.9 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 69.89 secs| Accuracy 98.22


======================================================================
----------------------------------------------------------------------
                run_ID=900
Mon May 16 18:09:08 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.696 GB
25 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 900,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 6.209 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.15 Time taken: 523.72 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 70.27 secs| Accuracy 98.22

Epoch: 2 Train Loss 0.051 Time taken: 540.13 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 68.26 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.026 Time taken: 545.29 secs

    Epoch: 3 Validation Loss 0.068 Time taken: 70.96 secs| Accuracy 98.079

Epoch: 4 Train Loss 0.018 Time taken: 544.3 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 70.12 secs| Accuracy 98.248
T\P      0       1
0       0278        0222        
1       0004        0396        

Accuracy 74.889
Time taken: 9.19 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2457.827 secs

Mon May 16 18:50:56 2022



======================================================================
----------------------------------------------------------------------
                run_ID=188
Thu Apr 14 20:18:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.561 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 188,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.211 Time taken: 626.85 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.97 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.066 Time taken: 635.69 secs

    Epoch: 2 Validation Loss 0.034 Time taken: 23.64 secs| Accuracy 98.997

Epoch: 3 Train Loss 0.044 Time taken: 627.98 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 24.1 secs| Accuracy 98.87

Epoch: 4 Train Loss 0.032 Time taken: 624.67 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 24.23 secs| Accuracy 98.714

Epoch: 5 Train Loss 0.025 Time taken: 626.46 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 24.23 secs| Accuracy 98.969
T\P      0       1
0       5242        0067        
1       0041        5267        

Accuracy 98.9828
Time taken: 35.54 secs
3320.831 secs


On the best model
T\P      0       1
0       5264        0045        
1       0063        5245        

Accuracy 98.9828
Time taken: 36.61 secs
Thu Apr 14 21:15:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=189
Thu Apr 14 21:15:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 189,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.316 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.268 Time taken: 629.09 secs

    Epoch: 1 Validation Loss 0.101 Time taken: 24.29 secs| Accuracy 97.061

Epoch: 2 Train Loss 0.133 Time taken: 633.81 secs

    Epoch: 2 Validation Loss 0.093 Time taken: 24.15 secs| Accuracy 97.118

Epoch: 3 Train Loss 0.09 Time taken: 635.14 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 24.03 secs| Accuracy 98.163

Epoch: 4 Train Loss 0.062 Time taken: 628.08 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 24.35 secs| Accuracy 98.63

Epoch: 5 Train Loss 0.047 Time taken: 627.57 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 24.45 secs| Accuracy 98.63
T\P      0       1
0       5235        0074        
1       0043        5265        

Accuracy 98.898
Time taken: 35.68 secs
3349.756 secs


On the best model
T\P      0       1
0       5276        0033        
1       0087        5221        

Accuracy 98.8697
Time taken: 36.67 secs
Thu Apr 14 22:12:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=190
Thu Apr 14 22:12:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.044 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 190,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.301 Time taken: 631.32 secs

    Epoch: 1 Validation Loss 0.117 Time taken: 23.95 secs| Accuracy 97.429

Epoch: 2 Train Loss 0.129 Time taken: 635.36 secs

    Epoch: 2 Validation Loss 0.065 Time taken: 24.13 secs| Accuracy 98.05

Epoch: 3 Train Loss 0.075 Time taken: 633.82 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.86 secs| Accuracy 98.672

Epoch: 4 Train Loss 0.055 Time taken: 630.07 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 24.04 secs| Accuracy 98.813

Epoch: 5 Train Loss 0.041 Time taken: 627.3 secs

    Epoch: 5 Validation Loss 0.047 Time taken: 24.34 secs| Accuracy 98.46
T\P      0       1
0       5196        0113        
1       0037        5271        

Accuracy 98.5872
Time taken: 35.83 secs
3353.711 secs


On the best model
T\P      0       1
0       5243        0066        
1       0055        5253        

Accuracy 98.8603
Time taken: 36.23 secs
Thu Apr 14 23:10:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=191
Thu Apr 14 23:10:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.859 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 191,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.722 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.334 Time taken: 629.06 secs

    Epoch: 1 Validation Loss 0.152 Time taken: 24.04 secs| Accuracy 96.708

Epoch: 2 Train Loss 0.127 Time taken: 634.83 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 24.17 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.077 Time taken: 625.79 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 24.14 secs| Accuracy 97.669

Epoch: 4 Train Loss 0.053 Time taken: 631.17 secs

    Epoch: 4 Validation Loss 0.05 Time taken: 24.11 secs| Accuracy 98.827

Epoch: 5 Train Loss 0.042 Time taken: 625.59 secs

    Epoch: 5 Validation Loss 0.058 Time taken: 24.19 secs| Accuracy 98.587
T\P      0       1
0       5207        0102        
1       0040        5268        

Accuracy 98.6625
Time taken: 35.61 secs
3335.602 secs


On the best model
T\P      0       1
0       5250        0059        
1       0057        5251        

Accuracy 98.9074
Time taken: 36.4 secs
Fri Apr 15 00:07:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=192
Fri Apr 15 00:07:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.04 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 192,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.365 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.224 Time taken: 628.87 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 24.02 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.08 Time taken: 633.28 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 24.19 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.054 Time taken: 626.85 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 24.26 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.039 Time taken: 624.91 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 24.26 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.031 Time taken: 632.49 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.85 secs| Accuracy 98.969
T\P      0       1
0       5278        0031        
1       0074        5234        

Accuracy 99.011
Time taken: 36.63 secs
3350.119 secs


On the best model
T\P      0       1
0       5278        0031        
1       0074        5234        

Accuracy 99.011
Time taken: 36.42 secs
Fri Apr 15 01:05:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=193
Fri Apr 15 01:05:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.436 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 193,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 4.103 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.214 Time taken: 628.2 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 24.05 secs| Accuracy 98.333

Epoch: 2 Train Loss 0.076 Time taken: 625.99 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 24.07 secs| Accuracy 97.853

Epoch: 3 Train Loss 0.052 Time taken: 625.08 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 24.22 secs| Accuracy 98.206

Epoch: 4 Train Loss 0.039 Time taken: 625.87 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 24.48 secs| Accuracy 98.573

Epoch: 5 Train Loss 0.031 Time taken: 624.74 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 24.3 secs| Accuracy 98.955
T\P      0       1
0       5261        0048        
1       0048        5260        

Accuracy 99.0958
Time taken: 36.98 secs
3319.923 secs


On the best model
T\P      0       1
0       5261        0048        
1       0048        5260        

Accuracy 99.0958
Time taken: 36.15 secs
Fri Apr 15 02:02:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=194
Fri Apr 15 02:02:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.051 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 194,
    "message": "121 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.358 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.246 Time taken: 628.51 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 24.16 secs| Accuracy 97.711

Epoch: 2 Train Loss 0.086 Time taken: 632.81 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 24.07 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.055 Time taken: 632.42 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 24.05 secs| Accuracy 98.728

Epoch: 4 Train Loss 0.039 Time taken: 625.56 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 24.12 secs| Accuracy 98.545

Epoch: 5 Train Loss 0.03 Time taken: 620.5 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 24.08 secs| Accuracy 98.955
T\P      0       1
0       5247        0062        
1       0029        5279        

Accuracy 99.1429
Time taken: 36.21 secs
3337.45 secs


On the best model
T\P      0       1
0       5247        0062        
1       0029        5279        

Accuracy 99.1429
Time taken: 36.82 secs
Fri Apr 15 02:59:34 2022



======================================================================
----------------------------------------------------------------------
                run_ID=208
Fri Apr 15 17:19:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.822 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 208,
    "message": "136 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.649 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.406 Time taken: 610.78 secs

    Epoch: 1 Validation Loss 0.192 Time taken: 23.07 secs| Accuracy 92.47

Epoch: 2 Train Loss 0.157 Time taken: 602.82 secs

    Epoch: 2 Validation Loss 0.137 Time taken: 23.31 secs| Accuracy 94.73

Epoch: 3 Train Loss 0.098 Time taken: 600.75 secs

    Epoch: 3 Validation Loss 0.126 Time taken: 23.12 secs| Accuracy 95.281

Epoch: 4 Train Loss 0.075 Time taken: 602.02 secs

    Epoch: 4 Validation Loss 0.123 Time taken: 23.27 secs| Accuracy 95.762

Epoch: 5 Train Loss 0.047 Time taken: 597.03 secs

    Epoch: 5 Validation Loss 0.129 Time taken: 23.3 secs| Accuracy 95.875
T\P      0       1
0       5021        0288        
1       0140        5168        

Accuracy 95.969
Time taken: 33.87 secs
3205.296 secs


On the best model
T\P      0       1
0       5021        0288        
1       0140        5168        

Accuracy 95.969
Time taken: 35.85 secs
Fri Apr 15 18:14:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=209
Fri Apr 15 18:14:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26543 | Validation_set: 10616 | Test_set: 15926
Train_batches: 830 | Validation_batches: 332 | Test_batches: 498
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.411 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 209,
    "message": "188 with repeated negatives once",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.373 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.18 Time taken: 714.46 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 34.37 secs| Macro F: 0.981

Epoch: 2 Train Loss 0.067 Time taken: 713.02 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 33.87 secs| Macro F: 0.988

Epoch: 3 Train Loss 0.049 Time taken: 712.43 secs

    Epoch: 3 Validation Loss 0.029 Time taken: 34.18 secs| Macro F: 0.991

Epoch: 4 Train Loss 0.032 Time taken: 708.16 secs

    Epoch: 4 Validation Loss 0.027 Time taken: 34.36 secs| Macro F: 0.991

Epoch: 5 Train Loss 0.026 Time taken: 706.53 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 34.03 secs| Macro F: 0.991
T\P      0       1
0       10523       0095        
1       0044        5264        

Macro F: 0.99
Time taken: 50.2 secs
3811.548 secs


On the best model
T\P      0       1
0       10585       0033        
1       0079        5229        

Macro F: 0.992
Time taken: 51.76 secs
Fri Apr 15 19:20:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=210
Fri Apr 15 19:20:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26543 | Validation_set: 10616 | Test_set: 15926
Train_batches: 830 | Validation_batches: 332 | Test_batches: 498
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.217 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 210,
    "message": "209 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.372 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 714.32 secs

    Epoch: 1 Validation Loss 0.046 Time taken: 34.28 secs| Macro F: 0.987

Epoch: 2 Train Loss 0.07 Time taken: 707.0 secs

    Epoch: 2 Validation Loss 0.037 Time taken: 34.25 secs| Macro F: 0.988

Epoch: 3 Train Loss 0.053 Time taken: 711.39 secs

    Epoch: 3 Validation Loss 0.03 Time taken: 35.03 secs| Macro F: 0.991

Epoch: 4 Train Loss 0.033 Time taken: 712.67 secs

    Epoch: 4 Validation Loss 0.032 Time taken: 34.54 secs| Macro F: 0.989

Epoch: 5 Train Loss 0.028 Time taken: 711.54 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 34.71 secs| Macro F: 0.99
T\P      0       1
0       10520       0098        
1       0046        5262        

Macro F: 0.99
Time taken: 51.03 secs
3813.085 secs


On the best model
T\P      0       1
0       10582       0036        
1       0080        5228        

Macro F: 0.992
Time taken: 52.38 secs
Fri Apr 15 20:26:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=211
Fri Apr 15 20:26:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26543 | Validation_set: 10616 | Test_set: 15926
Train_batches: 830 | Validation_batches: 332 | Test_batches: 498
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.472 GB
31 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 211,
    "message": "209 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.403 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.166 Time taken: 717.0 secs

    Epoch: 1 Validation Loss 0.051 Time taken: 34.36 secs| Macro F: 0.984

Epoch: 2 Train Loss 0.063 Time taken: 718.58 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 34.36 secs| Macro F: 0.988

Epoch: 3 Train Loss 0.046 Time taken: 719.17 secs

    Epoch: 3 Validation Loss 0.028 Time taken: 34.34 secs| Macro F: 0.991

Epoch: 4 Train Loss 0.033 Time taken: 710.34 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 34.09 secs| Macro F: 0.99

Epoch: 5 Train Loss 0.027 Time taken: 718.29 secs

    Epoch: 5 Validation Loss 0.027 Time taken: 33.97 secs| Macro F: 0.991
T\P      0       1
0       10522       0096        
1       0049        5259        

Macro F: 0.99
Time taken: 54.88 secs
3852.008 secs


On the best model
T\P      0       1
0       10522       0096        
1       0049        5259        

Macro F: 0.99
Time taken: 52.84 secs
Fri Apr 15 21:32:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=212
Fri Apr 15 21:32:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 35391 | Validation_set: 14154 | Test_set: 21235
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.857 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 212,
    "message": "188 with repeated negatives twice",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 2,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.269 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.162 Time taken: 836.59 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 45.79 secs| Macro F: 0.966

Epoch: 2 Train Loss 0.061 Time taken: 834.17 secs

    Epoch: 2 Validation Loss 0.034 Time taken: 45.98 secs| Macro F: 0.987

Epoch: 3 Train Loss 0.041 Time taken: 829.86 secs

    Epoch: 3 Validation Loss 0.028 Time taken: 46.15 secs| Macro F: 0.99

Epoch: 4 Train Loss 0.03 Time taken: 823.33 secs

    Epoch: 4 Validation Loss 0.028 Time taken: 45.9 secs| Macro F: 0.989

Epoch: 5 Train Loss 0.025 Time taken: 819.23 secs

    Epoch: 5 Validation Loss 0.028 Time taken: 45.87 secs| Macro F: 0.99
T\P      0       1
0       15843       0084        
1       0062        5246        

Macro F: 0.991
Time taken: 69.26 secs
4478.779 secs


On the best model
T\P      0       1
0       15843       0084        
1       0062        5246        

Macro F: 0.991
Time taken: 72.79 secs
Fri Apr 15 22:49:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=213
Fri Apr 15 22:49:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 35391 | Validation_set: 14154 | Test_set: 21235
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.794 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 213,
    "message": "212 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 2,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.345 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.156 Time taken: 828.91 secs

    Epoch: 1 Validation Loss 0.043 Time taken: 45.92 secs| Macro F: 0.984

Epoch: 2 Train Loss 0.056 Time taken: 826.6 secs

    Epoch: 2 Validation Loss 0.032 Time taken: 46.23 secs| Macro F: 0.988

Epoch: 3 Train Loss 0.039 Time taken: 822.88 secs

    Epoch: 3 Validation Loss 0.027 Time taken: 46.3 secs| Macro F: 0.989

Epoch: 4 Train Loss 0.026 Time taken: 824.0 secs

    Epoch: 4 Validation Loss 0.025 Time taken: 46.21 secs| Macro F: 0.99

Epoch: 5 Train Loss 0.019 Time taken: 824.19 secs

    Epoch: 5 Validation Loss 0.027 Time taken: 45.89 secs| Macro F: 0.99
T\P      0       1
0       15791       0136        
1       0046        5262        

Macro F: 0.989
Time taken: 67.88 secs
4465.656 secs


On the best model
T\P      0       1
0       15822       0105        
1       0055        5253        

Macro F: 0.99
Time taken: 70.15 secs
Sat Apr 16 00:06:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=231
Sat Apr 16 01:28:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.679 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 231,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 132,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -1.506 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.232 Time taken: 550.4 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.1 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.083 Time taken: 551.28 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 23.03 secs| Accuracy 98.573

Epoch: 3 Train Loss 0.065 Time taken: 547.19 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.29 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.046 Time taken: 539.05 secs

    Epoch: 4 Validation Loss 0.029 Time taken: 23.24 secs| Accuracy 99.166

Epoch: 5 Train Loss 0.038 Time taken: 538.08 secs

    Epoch: 5 Validation Loss 0.028 Time taken: 23.18 secs| Accuracy 99.096
T\P      0       1
0       5253        0056        
1       0047        5261        

Accuracy 99.03
Time taken: 34.03 secs
2916.072 secs


On the best model
T\P      0       1
0       5275        0034        
1       0065        5243        

Accuracy 99.068
Time taken: 34.37 secs
Sat Apr 16 02:18:28 2022


======================================================================
----------------------------------------------------------------------
                run_ID=232
Sat Apr 16 02:18:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 232,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 133,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.354 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.244 Time taken: 546.46 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.3 secs| Accuracy 97.697

Epoch: 2 Train Loss 0.094 Time taken: 545.11 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.38 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.069 Time taken: 536.58 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.24 secs| Accuracy 98.573

Epoch: 4 Train Loss 0.052 Time taken: 542.21 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.29 secs| Accuracy 98.799

Epoch: 5 Train Loss 0.041 Time taken: 540.25 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 23.23 secs| Accuracy 99.025
T\P      0       1
0       5274        0035        
1       0054        5254        

Accuracy 99.162
Time taken: 33.98 secs
2901.667 secs


On the best model
T\P      0       1
0       5274        0035        
1       0054        5254        

Accuracy 99.162
Time taken: 35.51 secs
Sat Apr 16 03:08:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=233
Sat Apr 16 03:08:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.051 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 233,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 134,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.353 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.247 Time taken: 545.68 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.1 secs| Accuracy 98.121

Epoch: 2 Train Loss 0.095 Time taken: 542.52 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 23.33 secs| Accuracy 98.46

Epoch: 3 Train Loss 0.069 Time taken: 541.62 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.17 secs| Accuracy 98.757

Epoch: 4 Train Loss 0.055 Time taken: 542.9 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 23.28 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.042 Time taken: 538.58 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.18 secs| Accuracy 99.025
T\P      0       1
0       5277        0032        
1       0068        5240        

Accuracy 99.058
Time taken: 34.0 secs
2902.474 secs


On the best model
T\P      0       1
0       5277        0032        
1       0068        5240        

Accuracy 99.058
Time taken: 35.9 secs
Sat Apr 16 03:58:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=234
Sat Apr 16 03:58:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.86 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 234,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 135,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.368 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.256 Time taken: 545.65 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 23.15 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.085 Time taken: 548.81 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 23.19 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.062 Time taken: 539.96 secs

    Epoch: 3 Validation Loss 0.033 Time taken: 23.16 secs| Accuracy 98.912

Epoch: 4 Train Loss 0.047 Time taken: 544.39 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 23.19 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.037 Time taken: 544.11 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.22 secs| Accuracy 98.686
T\P      0       1
0       5208        0101        
1       0039        5269        

Accuracy 98.681
Time taken: 34.14 secs
2908.058 secs


On the best model
T\P      0       1
0       5276        0033        
1       0071        5237        

Accuracy 99.02
Time taken: 34.44 secs
Sat Apr 16 04:48:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=235
Sat Apr 16 04:48:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 235,
    "message": "221 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 136,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.264 Time taken: 547.9 secs

    Epoch: 1 Validation Loss 0.078 Time taken: 23.18 secs| Accuracy 97.372

Epoch: 2 Train Loss 0.108 Time taken: 542.17 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 23.25 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.075 Time taken: 536.83 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.19 secs| Accuracy 98.771

Epoch: 4 Train Loss 0.058 Time taken: 543.83 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 23.4 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.044 Time taken: 538.86 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.1 secs| Accuracy 98.757
T\P      0       1
0       5197        0112        
1       0035        5273        

Accuracy 98.615
Time taken: 34.09 secs
2898.742 secs


On the best model
T\P      0       1
0       5281        0028        
1       0089        5219        

Accuracy 98.898
Time taken: 34.59 secs
Sat Apr 16 05:38:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=236
Sat Apr 16 05:38:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.866 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 236,
    "message": "fc2 with first half of bert frozen with random sents off",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.481 Time taken: 548.64 secs

    Epoch: 1 Validation Loss 0.228 Time taken: 23.36 secs| Accuracy 90.633

Epoch: 2 Train Loss 0.209 Time taken: 549.14 secs

    Epoch: 2 Validation Loss 0.15 Time taken: 23.32 secs| Accuracy 94.038

Epoch: 3 Train Loss 0.145 Time taken: 541.42 secs

    Epoch: 3 Validation Loss 0.137 Time taken: 23.35 secs| Accuracy 95.013

Epoch: 4 Train Loss 0.115 Time taken: 543.4 secs

    Epoch: 4 Validation Loss 0.122 Time taken: 23.45 secs| Accuracy 95.352

Epoch: 5 Train Loss 0.09 Time taken: 551.27 secs

    Epoch: 5 Validation Loss 0.209 Time taken: 23.2 secs| Accuracy 92.992
T\P      0       1
0       4630        0679        
1       0067        5241        

Accuracy 92.974
Time taken: 34.91 secs
2922.733 secs


On the best model
T\P      0       1
0       5084        0225        
1       0261        5047        

Accuracy 95.422
Time taken: 36.04 secs
Sat Apr 16 06:29:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=237
Sat Apr 16 06:29:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.046 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 237,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.365 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.435 Time taken: 548.68 secs

    Epoch: 1 Validation Loss 0.197 Time taken: 23.37 secs| Accuracy 92.244

Epoch: 2 Train Loss 0.191 Time taken: 543.32 secs

    Epoch: 2 Validation Loss 0.147 Time taken: 23.58 secs| Accuracy 94.207

Epoch: 3 Train Loss 0.136 Time taken: 538.04 secs

    Epoch: 3 Validation Loss 0.155 Time taken: 23.53 secs| Accuracy 94.617

Epoch: 4 Train Loss 0.103 Time taken: 539.77 secs

    Epoch: 4 Validation Loss 0.121 Time taken: 23.6 secs| Accuracy 95.38

Epoch: 5 Train Loss 0.083 Time taken: 539.7 secs

    Epoch: 5 Validation Loss 0.132 Time taken: 23.42 secs| Accuracy 95.097
T\P      0       1
0       5148        0161        
1       0321        4987        

Accuracy 95.46
Time taken: 34.36 secs
2898.626 secs


On the best model
T\P      0       1
0       5035        0274        
1       0199        5109        

Accuracy 95.545
Time taken: 35.27 secs
Sat Apr 16 07:19:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=238
Sat Apr 16 07:19:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 238,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.481 Time taken: 546.52 secs

    Epoch: 1 Validation Loss 0.249 Time taken: 23.39 secs| Accuracy 89.842

Epoch: 2 Train Loss 0.206 Time taken: 542.94 secs

    Epoch: 2 Validation Loss 0.154 Time taken: 23.72 secs| Accuracy 93.967

Epoch: 3 Train Loss 0.142 Time taken: 537.53 secs

    Epoch: 3 Validation Loss 0.142 Time taken: 23.55 secs| Accuracy 94.631

Epoch: 4 Train Loss 0.107 Time taken: 542.93 secs

    Epoch: 4 Validation Loss 0.13 Time taken: 23.67 secs| Accuracy 95.069

Epoch: 5 Train Loss 0.082 Time taken: 543.33 secs

    Epoch: 5 Validation Loss 0.135 Time taken: 23.54 secs| Accuracy 95.324
T\P      0       1
0       5073        0236        
1       0215        5093        

Accuracy 95.752
Time taken: 34.54 secs
2911.319 secs


On the best model
T\P      0       1
0       5073        0236        
1       0215        5093        

Accuracy 95.752
Time taken: 36.85 secs
Sat Apr 16 08:09:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=239
Sat Apr 16 08:09:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.046 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 239,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.415 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.455 Time taken: 546.84 secs

    Epoch: 1 Validation Loss 0.217 Time taken: 23.13 secs| Accuracy 91.438

Epoch: 2 Train Loss 0.199 Time taken: 543.45 secs

    Epoch: 2 Validation Loss 0.165 Time taken: 23.42 secs| Accuracy 93.812

Epoch: 3 Train Loss 0.139 Time taken: 542.39 secs

    Epoch: 3 Validation Loss 0.144 Time taken: 23.27 secs| Accuracy 94.575

Epoch: 4 Train Loss 0.106 Time taken: 542.74 secs

    Epoch: 4 Validation Loss 0.119 Time taken: 23.21 secs| Accuracy 95.648

Epoch: 5 Train Loss 0.08 Time taken: 538.02 secs

    Epoch: 5 Validation Loss 0.121 Time taken: 23.19 secs| Accuracy 95.55
T\P      0       1
0       5071        0238        
1       0204        5104        

Accuracy 95.837
Time taken: 34.14 secs
2903.714 secs


On the best model
T\P      0       1
0       5100        0209        
1       0234        5074        

Accuracy 95.827
Time taken: 35.07 secs
Sat Apr 16 08:59:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=240
Sat Apr 16 08:59:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.055 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 240,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.385 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.452 Time taken: 547.85 secs

    Epoch: 1 Validation Loss 0.211 Time taken: 23.14 secs| Accuracy 91.382

Epoch: 2 Train Loss 0.199 Time taken: 543.36 secs

    Epoch: 2 Validation Loss 0.156 Time taken: 23.33 secs| Accuracy 93.699

Epoch: 3 Train Loss 0.135 Time taken: 539.96 secs

    Epoch: 3 Validation Loss 0.138 Time taken: 23.28 secs| Accuracy 94.49

Epoch: 4 Train Loss 0.101 Time taken: 549.78 secs

    Epoch: 4 Validation Loss 0.14 Time taken: 23.16 secs| Accuracy 94.871

Epoch: 5 Train Loss 0.081 Time taken: 540.69 secs

    Epoch: 5 Validation Loss 0.176 Time taken: 23.3 secs| Accuracy 93.798
T\P      0       1
0       4755        0554        
1       0087        5221        

Accuracy 93.963
Time taken: 34.42 secs
2911.048 secs


On the best model
T\P      0       1
0       5068        0241        
1       0286        5022        

Accuracy 95.036
Time taken: 34.38 secs
Sat Apr 16 09:49:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=254
Sat Apr 16 10:15:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 61935 | Validation_set: 24768 | Test_set: 37162
Train_batches: 1936 | Validation_batches: 774 | Test_batches: 1162
Memory taken on GPU 0.0 GB
Memory taken on RAM 8.222 GB
40 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 254,
    "message": "188 with repeated negatives five times",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 5,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.27 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.092 Time taken: 1168.79 secs

    Epoch: 1 Validation Loss 0.028 Time taken: 78.29 secs| Macro F: 0.985

Epoch: 2 Train Loss 0.035 Time taken: 1161.21 secs

    Epoch: 2 Validation Loss 0.026 Time taken: 78.53 secs| Macro F: 0.986

Epoch: 3 Train Loss 0.024 Time taken: 1160.52 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 78.37 secs| Macro F: 0.981

Epoch: 4 Train Loss 0.017 Time taken: 1154.59 secs

    Epoch: 4 Validation Loss 0.022 Time taken: 78.25 secs| Macro F: 0.989

Epoch: 5 Train Loss 0.012 Time taken: 1159.23 secs

    Epoch: 5 Validation Loss 0.022 Time taken: 78.55 secs| Macro F: 0.99
T\P      0       1
0       31771       0083        
1       0079        5229        

Macro F: 0.991
Time taken: 121.19 secs
6357.179 secs


On the best model
T\P      0       1
0       31771       0083        
1       0079        5229        

Macro F: 0.991
Time taken: 121.97 secs
Sat Apr 16 12:04:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=255
Sat Apr 16 12:04:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 61935 | Validation_set: 24768 | Test_set: 37162
Train_batches: 1936 | Validation_batches: 774 | Test_batches: 1162
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.672 GB
49 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 255,
    "message": "254 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 5,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.347 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.112 Time taken: 1167.85 secs

    Epoch: 1 Validation Loss 0.029 Time taken: 79.63 secs| Macro F: 0.983

Epoch: 2 Train Loss 0.038 Time taken: 1167.3 secs

    Epoch: 2 Validation Loss 0.024 Time taken: 80.53 secs| Macro F: 0.988

Epoch: 3 Train Loss 0.027 Time taken: 1170.65 secs

    Epoch: 3 Validation Loss 0.022 Time taken: 80.43 secs| Macro F: 0.988

Epoch: 4 Train Loss 0.018 Time taken: 1174.39 secs

    Epoch: 4 Validation Loss 0.02 Time taken: 80.65 secs| Macro F: 0.989

Epoch: 5 Train Loss 0.014 Time taken: 1166.89 secs

    Epoch: 5 Validation Loss 0.018 Time taken: 79.01 secs| Macro F: 0.989
T\P      0       1
0       31730       0124        
1       0078        5230        

Macro F: 0.989
Time taken: 119.4 secs
6414.546 secs


On the best model
T\P      0       1
0       31730       0124        
1       0078        5230        

Macro F: 0.989
Time taken: 120.38 secs
Sat Apr 16 13:55:14 2022



======================================================================
----------------------------------------------------------------------
                run_ID=259
Sat Apr 16 14:35:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 70783 | Validation_set: 28306 | Test_set: 42471
Train_batches: 2212 | Validation_batches: 885 | Test_batches: 1328
Memory taken on GPU 0.0 GB
Memory taken on RAM 8.349 GB
45 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 259,
    "message": "257 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 6,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.377 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.073 Time taken: 1278.3 secs

    Epoch: 1 Validation Loss 0.022 Time taken: 89.72 secs| Macro F: 0.986

Epoch: 2 Train Loss 0.028 Time taken: 1278.52 secs

    Epoch: 2 Validation Loss 0.019 Time taken: 90.75 secs| Macro F: 0.988

Epoch: 3 Train Loss 0.019 Time taken: 1271.23 secs

    Epoch: 3 Validation Loss 0.017 Time taken: 90.11 secs| Macro F: 0.988

Epoch: 4 Train Loss 0.012 Time taken: 1270.45 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 90.17 secs| Macro F: 0.983

Epoch: 5 Train Loss 0.01 Time taken: 1268.58 secs

    Epoch: 5 Validation Loss 0.024 Time taken: 90.59 secs| Macro F: 0.987
T\P      0       1
0       36968       0195        
1       0047        5261        

Macro F: 0.987
Time taken: 135.02 secs
6976.998 secs


On the best model
T\P      0       1
0       37113       0050        
1       0135        5173        

Macro F: 0.99
Time taken: 136.95 secs
Sat Apr 16 16:35:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=256
Sat Apr 16 16:35:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 61935 | Validation_set: 24768 | Test_set: 37162
Train_batches: 1936 | Validation_batches: 774 | Test_batches: 1162
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.281 GB
41 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 256,
    "message": "254 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 5,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.344 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.084 Time taken: 1176.82 secs

    Epoch: 1 Validation Loss 0.026 Time taken: 81.95 secs| Macro F: 0.986

Epoch: 2 Train Loss 0.033 Time taken: 1164.36 secs

    Epoch: 2 Validation Loss 0.021 Time taken: 79.33 secs| Macro F: 0.989

Epoch: 3 Train Loss 0.022 Time taken: 1166.52 secs

    Epoch: 3 Validation Loss 0.018 Time taken: 79.86 secs| Macro F: 0.99

Epoch: 4 Train Loss 0.017 Time taken: 1165.59 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 79.26 secs| Macro F: 0.978

Epoch: 5 Train Loss 0.012 Time taken: 1164.26 secs

    Epoch: 5 Validation Loss 0.021 Time taken: 79.58 secs| Macro F: 0.99
T\P      0       1
0       31725       0129        
1       0082        5226        

Macro F: 0.988
Time taken: 120.15 secs
6431.185 secs


On the best model
T\P      0       1
0       31749       0105        
1       0083        5225        

Macro F: 0.99
Time taken: 123.96 secs
Sat Apr 16 18:26:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=277
Sat Apr 16 18:26:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.033 GB
36 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 277,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 136,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.423 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.177 Time taken: 608.37 secs

    Epoch: 1 Validation Loss 0.044 Time taken: 23.56 secs| Accuracy 98.63

Epoch: 2 Train Loss 0.058 Time taken: 605.97 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 23.66 secs| Accuracy 98.785

Epoch: 3 Train Loss 0.043 Time taken: 606.71 secs

    Epoch: 3 Validation Loss 0.032 Time taken: 23.69 secs| Accuracy 98.997

Epoch: 4 Train Loss 0.027 Time taken: 607.22 secs

    Epoch: 4 Validation Loss 0.064 Time taken: 23.59 secs| Accuracy 98.248

Epoch: 5 Train Loss 0.02 Time taken: 599.25 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 23.71 secs| Accuracy 98.884
T\P      0       1
0       5214        0095        
1       0027        5281        

Accuracy 98.851
Time taken: 34.34 secs
3215.13 secs


On the best model
T\P      0       1
0       5268        0041        
1       0057        5251        

Accuracy 99.077
Time taken: 35.92 secs
Sat Apr 16 19:21:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=278
Sat Apr 16 19:21:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.135 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 278,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 137,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.371 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.157 Time taken: 607.61 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 23.61 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.056 Time taken: 604.35 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.69 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.039 Time taken: 604.58 secs

    Epoch: 3 Validation Loss 0.054 Time taken: 23.78 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.034 Time taken: 617.9 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.52 secs| Accuracy 98.884

Epoch: 5 Train Loss 0.025 Time taken: 604.48 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 23.69 secs| Accuracy 98.87
T\P      0       1
0       5218        0091        
1       0037        5271        

Accuracy 98.794
Time taken: 34.46 secs
3239.177 secs


On the best model
T\P      0       1
0       5243        0066        
1       0050        5258        

Accuracy 98.907
Time taken: 35.77 secs
Sat Apr 16 20:17:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=279
Sat Apr 16 20:17:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.04 GB
32 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 279,
    "message": "112 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 138,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.365 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.175 Time taken: 605.4 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 23.48 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.069 Time taken: 610.31 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.59 secs| Accuracy 98.276

Epoch: 3 Train Loss 0.047 Time taken: 604.8 secs

    Epoch: 3 Validation Loss 0.033 Time taken: 23.6 secs| Accuracy 98.912

Epoch: 4 Train Loss 0.033 Time taken: 605.72 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.75 secs| Accuracy 98.785

Epoch: 5 Train Loss 0.026 Time taken: 602.57 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.72 secs| Accuracy 98.615
T\P      0       1
0       5208        0101        
1       0032        5276        

Accuracy 98.747
Time taken: 34.53 secs
3206.09 secs


On the best model
T\P      0       1
0       5282        0027        
1       0074        5234        

Accuracy 99.049
Time taken: 35.35 secs
Sat Apr 16 21:12:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=280
Sat Apr 16 21:12:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.953 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 280,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.408 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.28 Time taken: 607.55 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.62 secs| Accuracy 98.333

Epoch: 2 Train Loss 0.081 Time taken: 611.29 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.56 secs| Accuracy 98.672

Epoch: 3 Train Loss 0.053 Time taken: 602.98 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 23.82 secs| Accuracy 98.177

Epoch: 4 Train Loss 0.036 Time taken: 599.3 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 23.62 secs| Accuracy 98.63

Epoch: 5 Train Loss 0.03 Time taken: 600.62 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.51 secs| Accuracy 98.955
T\P      0       1
0       5270        0039        
1       0060        5248        

Accuracy 99.068
Time taken: 34.91 secs
3208.357 secs


On the best model
T\P      0       1
0       5270        0039        
1       0060        5248        

Accuracy 99.068
Time taken: 36.66 secs
Sat Apr 16 22:08:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=281
Sat Apr 16 22:08:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.05 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 281,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.414 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.255 Time taken: 603.68 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.47 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.073 Time taken: 613.1 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 23.53 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.049 Time taken: 606.39 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.44 secs| Accuracy 98.94

Epoch: 4 Train Loss 0.035 Time taken: 607.91 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 23.41 secs| Accuracy 99.011

Epoch: 5 Train Loss 0.026 Time taken: 604.02 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 23.53 secs| Accuracy 98.841
T\P      0       1
0       5223        0086        
1       0037        5271        

Accuracy 98.841
Time taken: 34.57 secs
3231.395 secs


On the best model
T\P      0       1
0       5260        0049        
1       0057        5251        

Accuracy 99.002
Time taken: 35.42 secs
Sat Apr 16 23:03:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=282
Sat Apr 16 23:03:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 282,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.283 Time taken: 604.58 secs

    Epoch: 1 Validation Loss 0.087 Time taken: 23.65 secs| Accuracy 96.736

Epoch: 2 Train Loss 0.093 Time taken: 603.25 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 23.72 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.079 Time taken: 611.42 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.66 secs| Accuracy 98.262

Epoch: 4 Train Loss 0.066 Time taken: 602.68 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.69 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.05 Time taken: 604.45 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.74 secs| Accuracy 98.898
T\P      0       1
0       5259        0050        
1       0060        5248        

Accuracy 98.964
Time taken: 34.84 secs
3229.413 secs


On the best model
T\P      0       1
0       5259        0050        
1       0060        5248        

Accuracy 98.964
Time taken: 37.7 secs
Sat Apr 16 23:59:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=283
Sat Apr 16 23:59:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.028 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 283,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 132,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.376 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.236 Time taken: 606.24 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 23.62 secs| Accuracy 98.163

Epoch: 2 Train Loss 0.072 Time taken: 612.5 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.66 secs| Accuracy 98.432

Epoch: 3 Train Loss 0.05 Time taken: 602.97 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.73 secs| Accuracy 98.983

Epoch: 4 Train Loss 0.037 Time taken: 609.22 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.67 secs| Accuracy 98.799

Epoch: 5 Train Loss 0.027 Time taken: 603.72 secs

    Epoch: 5 Validation Loss 0.068 Time taken: 23.69 secs| Accuracy 98.149
T\P      0       1
0       5182        0127        
1       0038        5270        

Accuracy 98.446
Time taken: 34.64 secs
3224.963 secs


On the best model
T\P      0       1
0       5264        0045        
1       0070        5238        

Accuracy 98.917
Time taken: 35.56 secs
Sun Apr 17 00:55:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=284
Sun Apr 17 00:55:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.06 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 284,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 133,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.273 Time taken: 605.42 secs

    Epoch: 1 Validation Loss 0.087 Time taken: 23.71 secs| Accuracy 97.471

Epoch: 2 Train Loss 0.087 Time taken: 612.72 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.76 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.063 Time taken: 610.84 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.49 secs| Accuracy 98.644

Epoch: 4 Train Loss 0.048 Time taken: 602.22 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 23.83 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.036 Time taken: 604.99 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.83 secs| Accuracy 98.714
T\P      0       1
0       5233        0076        
1       0048        5260        

Accuracy 98.832
Time taken: 34.82 secs
3242.478 secs


On the best model
T\P      0       1
0       5233        0076        
1       0048        5260        

Accuracy 98.832
Time taken: 36.04 secs
Sun Apr 17 01:51:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=285
Sun Apr 17 01:51:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.229 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 285,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 134,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.37 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.312 Time taken: 606.57 secs

    Epoch: 1 Validation Loss 0.113 Time taken: 23.42 secs| Accuracy 97.005

Epoch: 2 Train Loss 0.134 Time taken: 617.5 secs

    Epoch: 2 Validation Loss 0.069 Time taken: 23.32 secs| Accuracy 97.853

Epoch: 3 Train Loss 0.111 Time taken: 603.92 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 23.48 secs| Accuracy 98.389

Epoch: 4 Train Loss 0.078 Time taken: 615.95 secs

    Epoch: 4 Validation Loss 0.052 Time taken: 23.44 secs| Accuracy 98.192

Epoch: 5 Train Loss 0.059 Time taken: 603.07 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.62 secs| Accuracy 98.672
T\P      0       1
0       5254        0055        
1       0059        5249        

Accuracy 98.926
Time taken: 34.91 secs
3252.733 secs


On the best model
T\P      0       1
0       5254        0055        
1       0059        5249        

Accuracy 98.926
Time taken: 35.8 secs
Sun Apr 17 02:47:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=286
Sun Apr 17 02:47:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.051 GB
32 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 286,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 135,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.963 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.253 Time taken: 605.36 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 23.48 secs| Accuracy 97.711

Epoch: 2 Train Loss 0.093 Time taken: 605.65 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.53 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.058 Time taken: 602.6 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.61 secs| Accuracy 98.827

Epoch: 4 Train Loss 0.042 Time taken: 602.62 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.6 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.032 Time taken: 615.93 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.41 secs| Accuracy 98.898
T\P      0       1
0       5266        0043        
1       0062        5246        

Accuracy 99.011
Time taken: 37.57 secs
3248.214 secs


On the best model
T\P      0       1
0       5281        0028        
1       0069        5239        

Accuracy 99.086
Time taken: 36.18 secs
Sun Apr 17 03:42:56 2022




======================================================================
----------------------------------------------------------------------
                run_ID=287
Sun Apr 17 11:48:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.94 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 287,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 136,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 7.067 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.284 Time taken: 633.57 secs

    Epoch: 1 Validation Loss 0.091 Time taken: 24.27 secs| Accuracy 97.782

Epoch: 2 Train Loss 0.091 Time taken: 609.89 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.51 secs| Accuracy 98.036

Epoch: 3 Train Loss 0.059 Time taken: 600.09 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 23.67 secs| Accuracy 97.739

Epoch: 4 Train Loss 0.038 Time taken: 599.47 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.71 secs| Accuracy 99.039

Epoch: 5 Train Loss 0.028 Time taken: 602.18 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 23.72 secs| Accuracy 99.181
T\P      0       1
0       5244        0065        
1       0045        5263        

Accuracy 98.964
Time taken: 34.27 secs
3198.954 secs



======================================================================
----------------------------------------------------------------------
                run_ID=288
Sun Apr 17 16:07:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.322 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 288,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 137,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.228 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.282 Time taken: 613.2 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.51 secs| Accuracy 98.163

Epoch: 2 Train Loss 0.082 Time taken: 606.91 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 23.45 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.055 Time taken: 601.99 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.38 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.04 Time taken: 598.34 secs

    Epoch: 4 Validation Loss 0.054 Time taken: 23.37 secs| Accuracy 98.418

Epoch: 5 Train Loss 0.038 Time taken: 600.16 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.52 secs| Accuracy 98.898
T\P      0       1
0       5284        0025        
1       0088        5220        

Accuracy 98.936
Time taken: 34.12 secs
3172.483 secs

Sun Apr 17 17:01:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=289
Sun Apr 17 17:01:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.713 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 289,
    "message": "124 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 138,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 9.281 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.254 Time taken: 602.4 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.28 secs| Accuracy 98.192

Epoch: 2 Train Loss 0.073 Time taken: 596.89 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 23.23 secs| Accuracy 98.545

Epoch: 3 Train Loss 0.051 Time taken: 595.56 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.29 secs| Accuracy 98.955

Epoch: 4 Train Loss 0.039 Time taken: 595.84 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 23.27 secs| Accuracy 99.166

Epoch: 5 Train Loss 0.032 Time taken: 600.32 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 23.29 secs| Accuracy 99.152
T\P      0       1
0       5281        0028        
1       0064        5244        

Accuracy 99.133
Time taken: 34.3 secs
3142.148 secs

Sun Apr 17 17:54:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=290
Sun Apr 17 17:54:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.771 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 290,
    "message": "188 repeat with CLS | rseed constant",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 11.217 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 608.64 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.26 secs| Accuracy 98.361

Epoch: 2 Train Loss 0.075 Time taken: 605.6 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.32 secs| Accuracy 98.644

Epoch: 3 Train Loss 0.056 Time taken: 602.51 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.41 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.037 Time taken: 599.92 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.35 secs| Accuracy 98.926

Epoch: 5 Train Loss 0.032 Time taken: 604.3 secs

    Epoch: 5 Validation Loss 0.029 Time taken: 23.47 secs| Accuracy 99.124
T\P      0       1
0       5247        0062        
1       0046        5262        

Accuracy 98.983
Time taken: 34.52 secs
3172.899 secs

Sun Apr 17 18:48:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=291
Sun Apr 17 18:48:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.94 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 291,
    "message": "188 repeat with different learning rate | rseed constant",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 2e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.245 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 2e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.181 Time taken: 608.59 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 23.36 secs| Accuracy 98.064

Epoch: 2 Train Loss 0.073 Time taken: 609.58 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.29 secs| Accuracy 98.644

Epoch: 3 Train Loss 0.048 Time taken: 610.55 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.23 secs| Accuracy 99.025

Epoch: 4 Train Loss 0.034 Time taken: 604.31 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.31 secs| Accuracy 98.912

Epoch: 5 Train Loss 0.032 Time taken: 601.01 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.44 secs| Accuracy 98.827
T\P      0       1
0       5206        0103        
1       0034        5274        

Accuracy 98.71
Time taken: 34.19 secs
3185.157 secs

Sun Apr 17 19:42:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=292
Sun Apr 17 19:42:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.158 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 292,
    "message": "188 repeat with different learning rate | rseed constant",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 5e-06,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.265 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 5e-06
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 610.94 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.53 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.085 Time taken: 610.18 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.42 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.057 Time taken: 613.01 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.34 secs| Accuracy 98.757

Epoch: 4 Train Loss 0.042 Time taken: 606.04 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 23.45 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.032 Time taken: 605.89 secs

    Epoch: 5 Validation Loss 0.027 Time taken: 23.46 secs| Accuracy 99.209
T\P      0       1
0       5256        0053        
1       0045        5263        

Accuracy 99.077
Time taken: 34.55 secs
3198.268 secs

Sun Apr 17 20:36:41 2022




======================================================================
----------------------------------------------------------------------
                run_ID=293
Sun Apr 17 20:53:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.993 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 293,
    "message": "188 repeat with different learning rate | rseed constant",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-06,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.518 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-06
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.583 Time taken: 615.55 secs

    Epoch: 1 Validation Loss 0.344 Time taken: 23.07 secs| Accuracy 96.454

Epoch: 2 Train Loss 0.326 Time taken: 604.97 secs

    Epoch: 2 Validation Loss 0.195 Time taken: 23.27 secs| Accuracy 97.923

Epoch: 3 Train Loss 0.218 Time taken: 599.94 secs

    Epoch: 3 Validation Loss 0.137 Time taken: 23.23 secs| Accuracy 97.923

Epoch: 4 Train Loss 0.134 Time taken: 607.7 secs

    Epoch: 4 Validation Loss 0.082 Time taken: 23.06 secs| Accuracy 97.994

Epoch: 5 Train Loss 0.099 Time taken: 610.74 secs

    Epoch: 5 Validation Loss 0.068 Time taken: 23.11 secs| Accuracy 98.248
T\P      0       1
0       5241        0068        
1       0093        5215        

Accuracy 98.484
Time taken: 34.22 secs
3189.338 secs

Sun Apr 17 21:47:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=331
Sun Apr 17 21:47:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.12 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 331,
    "message": "EL0L1 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.178 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.214 Time taken: 581.69 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 23.08 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.066 Time taken: 583.61 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 22.96 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.046 Time taken: 578.07 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.41 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.036 Time taken: 573.08 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.17 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.027 Time taken: 573.16 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.27 secs| Accuracy 99.082
T\P      0       1
0       5250        0059        
1       0046        5262        

Accuracy 99.011
Time taken: 34.22 secs
3040.167 secs

Sun Apr 17 22:39:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=332
Sun Apr 17 22:39:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.923 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 332,
    "message": "331 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.441 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.207 Time taken: 580.44 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 24.03 secs| Accuracy 98.361

Epoch: 2 Train Loss 0.072 Time taken: 575.32 secs

    Epoch: 2 Validation Loss 0.047 Time taken: 23.26 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.05 Time taken: 573.21 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.46 secs| Accuracy 98.206

Epoch: 4 Train Loss 0.039 Time taken: 578.42 secs

    Epoch: 4 Validation Loss 0.049 Time taken: 23.34 secs| Accuracy 98.615

Epoch: 5 Train Loss 0.031 Time taken: 574.94 secs

    Epoch: 5 Validation Loss 0.087 Time taken: 23.38 secs| Accuracy 97.895
T\P      0       1
0       5174        0135        
1       0036        5272        

Accuracy 98.389
Time taken: 34.31 secs
3034.538 secs

Sun Apr 17 23:31:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=333
Sun Apr 17 23:31:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.881 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 333,
    "message": "331 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 7.286 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.285 Time taken: 578.87 secs

    Epoch: 1 Validation Loss 0.1 Time taken: 23.01 secs| Accuracy 96.835

Epoch: 2 Train Loss 0.118 Time taken: 575.38 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 22.93 secs| Accuracy 98.008

Epoch: 3 Train Loss 0.082 Time taken: 570.81 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.05 secs| Accuracy 98.559

Epoch: 4 Train Loss 0.059 Time taken: 569.92 secs

    Epoch: 4 Validation Loss 0.06 Time taken: 23.21 secs| Accuracy 97.994

Epoch: 5 Train Loss 0.047 Time taken: 569.22 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.18 secs| Accuracy 98.827
T\P      0       1
0       5234        0075        
1       0044        5264        

Accuracy 98.879
Time taken: 33.9 secs
3013.967 secs

Mon Apr 18 00:22:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=334
Mon Apr 18 00:22:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.688 GB
32 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 334,
    "message": "331 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 7.377 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.279 Time taken: 576.79 secs

    Epoch: 1 Validation Loss 0.108 Time taken: 23.19 secs| Accuracy 96.962

Epoch: 2 Train Loss 0.145 Time taken: 570.52 secs

    Epoch: 2 Validation Loss 0.08 Time taken: 23.23 secs| Accuracy 97.443

Epoch: 3 Train Loss 0.09 Time taken: 574.39 secs

    Epoch: 3 Validation Loss 0.067 Time taken: 23.24 secs| Accuracy 97.513

Epoch: 4 Train Loss 0.071 Time taken: 579.96 secs

    Epoch: 4 Validation Loss 0.057 Time taken: 23.23 secs| Accuracy 97.754

Epoch: 5 Train Loss 0.056 Time taken: 574.74 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 23.26 secs| Accuracy 98.234
T\P      0       1
0       5208        0101        
1       0047        5261        

Accuracy 98.606
Time taken: 33.96 secs
3026.961 secs

Mon Apr 18 01:13:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=335
Mon Apr 18 01:14:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.979 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 335,
    "message": "331 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -0.521 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.266 Time taken: 583.59 secs

    Epoch: 1 Validation Loss 0.106 Time taken: 23.44 secs| Accuracy 97.542

Epoch: 2 Train Loss 0.142 Time taken: 582.08 secs

    Epoch: 2 Validation Loss 0.075 Time taken: 23.31 secs| Accuracy 97.951

Epoch: 3 Train Loss 0.084 Time taken: 583.0 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 23.41 secs| Accuracy 98.248

Epoch: 4 Train Loss 0.062 Time taken: 575.9 secs

    Epoch: 4 Validation Loss 0.08 Time taken: 23.4 secs| Accuracy 97.853

Epoch: 5 Train Loss 0.056 Time taken: 582.62 secs

    Epoch: 5 Validation Loss 0.037 Time taken: 23.32 secs| Accuracy 98.799
T\P      0       1
0       5273        0036        
1       0073        5235        

Accuracy 98.973
Time taken: 34.78 secs
3059.26 secs

Mon Apr 18 02:05:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=336
Mon Apr 18 02:06:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.153 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 336,
    "message": "EL0L1 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -0.21 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.287 Time taken: 580.83 secs

    Epoch: 1 Validation Loss 0.094 Time taken: 23.47 secs| Accuracy 96.157

Epoch: 2 Train Loss 0.095 Time taken: 580.46 secs

    Epoch: 2 Validation Loss 0.081 Time taken: 23.64 secs| Accuracy 97.923

Epoch: 3 Train Loss 0.075 Time taken: 582.94 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.48 secs| Accuracy 98.375

Epoch: 4 Train Loss 0.055 Time taken: 575.02 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.83 secs| Accuracy 98.87

Epoch: 5 Train Loss 0.043 Time taken: 578.26 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 23.45 secs| Accuracy 98.969
T\P      0       1
0       5232        0077        
1       0048        5260        

Accuracy 98.823
Time taken: 35.03 secs
3050.901 secs

Mon Apr 18 02:57:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=337
Mon Apr 18 02:57:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.041 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 337,
    "message": "336 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 4.131 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.285 Time taken: 584.07 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.61 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.088 Time taken: 577.14 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 23.67 secs| Accuracy 98.46

Epoch: 3 Train Loss 0.059 Time taken: 583.24 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.48 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.044 Time taken: 588.4 secs

    Epoch: 4 Validation Loss 0.064 Time taken: 23.49 secs| Accuracy 97.739

Epoch: 5 Train Loss 0.034 Time taken: 578.61 secs

    Epoch: 5 Validation Loss 0.052 Time taken: 23.49 secs| Accuracy 98.389
T\P      0       1
0       5162        0147        
1       0025        5283        

Accuracy 98.38
Time taken: 34.36 secs
3064.011 secs

Mon Apr 18 03:50:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=338
Mon Apr 18 03:50:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.002 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 338,
    "message": "336 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -0.456 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.253 Time taken: 579.65 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 23.48 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.074 Time taken: 571.37 secs

    Epoch: 2 Validation Loss 0.047 Time taken: 23.52 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.047 Time taken: 570.79 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.44 secs| Accuracy 98.615

Epoch: 4 Train Loss 0.033 Time taken: 570.68 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.36 secs| Accuracy 99.068

Epoch: 5 Train Loss 0.026 Time taken: 571.68 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 23.41 secs| Accuracy 99.082
T\P      0       1
0       5278        0031        
1       0051        5257        

Accuracy 99.228
Time taken: 34.31 secs
3016.12 secs

Mon Apr 18 04:41:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=339
Mon Apr 18 04:41:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.235 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 339,
    "message": "336 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.288 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.227 Time taken: 581.52 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 23.23 secs| Accuracy 98.333

Epoch: 2 Train Loss 0.073 Time taken: 582.7 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.17 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.053 Time taken: 585.64 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.29 secs| Accuracy 98.827

Epoch: 4 Train Loss 0.04 Time taken: 577.71 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 23.38 secs| Accuracy 98.728

Epoch: 5 Train Loss 0.028 Time taken: 576.63 secs

    Epoch: 5 Validation Loss 0.062 Time taken: 23.4 secs| Accuracy 98.686
T\P      0       1
0       5209        0100        
1       0041        5267        

Accuracy 98.672
Time taken: 34.33 secs
3055.445 secs

Mon Apr 18 05:33:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=340
Mon Apr 18 05:33:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.297 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 340,
    "message": "336 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.374 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.28 Time taken: 576.64 secs

    Epoch: 1 Validation Loss 0.086 Time taken: 23.15 secs| Accuracy 97.768

Epoch: 2 Train Loss 0.091 Time taken: 573.91 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 23.15 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.056 Time taken: 571.18 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.28 secs| Accuracy 98.531

Epoch: 4 Train Loss 0.045 Time taken: 575.98 secs

    Epoch: 4 Validation Loss 0.049 Time taken: 23.19 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.034 Time taken: 573.23 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 23.34 secs| Accuracy 98.884
T\P      0       1
0       5277        0032        
1       0072        5236        

Accuracy 99.02
Time taken: 34.14 secs
3021.577 secs

Mon Apr 18 06:24:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=341
Mon Apr 18 06:24:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.98 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 341,
    "message": "EL0L1L2 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.515 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.162 Time taken: 573.96 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 23.27 secs| Accuracy 98.163

Epoch: 2 Train Loss 0.067 Time taken: 571.94 secs

    Epoch: 2 Validation Loss 0.036 Time taken: 23.3 secs| Accuracy 98.743

Epoch: 3 Train Loss 0.05 Time taken: 573.81 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 23.22 secs| Accuracy 98.856

Epoch: 4 Train Loss 0.037 Time taken: 567.8 secs

    Epoch: 4 Validation Loss 0.066 Time taken: 23.3 secs| Accuracy 98.135

Epoch: 5 Train Loss 0.03 Time taken: 565.57 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.36 secs| Accuracy 98.827
T\P      0       1
0       5294        0015        
1       0119        5189        

Accuracy 98.738
Time taken: 34.28 secs
3004.452 secs

Mon Apr 18 07:15:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=342
Mon Apr 18 07:15:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.196 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 342,
    "message": "341 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 10.74 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.221 Time taken: 573.47 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 23.45 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.081 Time taken: 569.54 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.49 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.055 Time taken: 564.1 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.64 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.043 Time taken: 562.35 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 23.72 secs| Accuracy 98.276

Epoch: 5 Train Loss 0.03 Time taken: 560.97 secs

    Epoch: 5 Validation Loss 0.049 Time taken: 23.63 secs| Accuracy 98.686
T\P      0       1
0       5231        0078        
1       0038        5270        

Accuracy 98.907
Time taken: 34.6 secs
2983.41 secs

Mon Apr 18 08:06:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=343
Mon Apr 18 08:06:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.07 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 343,
    "message": "341 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.577 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.173 Time taken: 566.82 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.28 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.069 Time taken: 561.61 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 23.37 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.054 Time taken: 566.14 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.26 secs| Accuracy 98.573

Epoch: 4 Train Loss 0.041 Time taken: 562.37 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 23.39 secs| Accuracy 98.898

Epoch: 5 Train Loss 0.03 Time taken: 561.35 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 23.41 secs| Accuracy 98.94
T\P      0       1
0       5245        0064        
1       0041        5267        

Accuracy 99.011
Time taken: 34.32 secs
2969.779 secs

Mon Apr 18 08:57:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=344
Mon Apr 18 08:57:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.891 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 344,
    "message": "341 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.373 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.2 Time taken: 573.02 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.32 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.077 Time taken: 571.18 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.25 secs| Accuracy 98.474

Epoch: 3 Train Loss 0.056 Time taken: 569.74 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.55 secs| Accuracy 98.714

Epoch: 4 Train Loss 0.043 Time taken: 566.37 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.57 secs| Accuracy 98.757

Epoch: 5 Train Loss 0.034 Time taken: 563.87 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.52 secs| Accuracy 98.672
T\P      0       1
0       5215        0094        
1       0035        5273        

Accuracy 98.785
Time taken: 34.5 secs
2996.607 secs

Mon Apr 18 09:48:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=345
Mon Apr 18 09:48:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.162 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 345,
    "message": "341 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.359 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.167 Time taken: 573.14 secs

    Epoch: 1 Validation Loss 0.082 Time taken: 23.35 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.082 Time taken: 565.0 secs

    Epoch: 2 Validation Loss 0.07 Time taken: 23.67 secs| Accuracy 98.361

Epoch: 3 Train Loss 0.063 Time taken: 571.22 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 23.36 secs| Accuracy 98.432

Epoch: 4 Train Loss 0.049 Time taken: 574.75 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 23.18 secs| Accuracy 98.559

Epoch: 5 Train Loss 0.037 Time taken: 568.16 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 23.2 secs| Accuracy 98.757
T\P      0       1
0       5251        0058        
1       0047        5261        

Accuracy 99.011
Time taken: 34.29 secs
3003.786 secs

Mon Apr 18 10:39:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=346
Mon Apr 18 10:39:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.998 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 346,
    "message": "EL0L1L2 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.353 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.208 Time taken: 574.58 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.58 secs| Accuracy 98.418

Epoch: 2 Train Loss 0.072 Time taken: 574.27 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.08 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.051 Time taken: 567.02 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.21 secs| Accuracy 98.955

Epoch: 4 Train Loss 0.037 Time taken: 564.15 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.06 secs| Accuracy 99.082

Epoch: 5 Train Loss 0.031 Time taken: 564.75 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 23.19 secs| Accuracy 98.884
T\P      0       1
0       5204        0105        
1       0032        5276        

Accuracy 98.71
Time taken: 34.2 secs
2995.342 secs

Mon Apr 18 11:30:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=347
Mon Apr 18 11:30:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.722 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 347,
    "message": "346 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.491 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.267 Time taken: 575.06 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 23.21 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.099 Time taken: 567.01 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 23.22 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.074 Time taken: 565.52 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.22 secs| Accuracy 98.404

Epoch: 4 Train Loss 0.056 Time taken: 564.15 secs

    Epoch: 4 Validation Loss 0.042 Time taken: 23.21 secs| Accuracy 98.743

Epoch: 5 Train Loss 0.049 Time taken: 573.31 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 23.26 secs| Accuracy 98.672
T\P      0       1
0       5254        0055        
1       0057        5251        

Accuracy 98.945
Time taken: 34.57 secs
2996.203 secs

Mon Apr 18 12:21:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=348
Mon Apr 18 12:21:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.119 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 348,
    "message": "346 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.367 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.234 Time taken: 569.97 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.01 secs| Accuracy 98.064

Epoch: 2 Train Loss 0.076 Time taken: 567.24 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 23.04 secs| Accuracy 97.838

Epoch: 3 Train Loss 0.05 Time taken: 568.04 secs

    Epoch: 3 Validation Loss 0.033 Time taken: 23.18 secs| Accuracy 99.011

Epoch: 4 Train Loss 0.035 Time taken: 569.27 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 23.14 secs| Accuracy 99.025

Epoch: 5 Train Loss 0.028 Time taken: 563.53 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.47 secs| Accuracy 98.7
T\P      0       1
0       5219        0090        
1       0031        5277        

Accuracy 98.86
Time taken: 34.34 secs
2988.81 secs

Mon Apr 18 13:12:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=349
Mon Apr 18 13:12:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.233 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 349,
    "message": "346 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.423 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.277 Time taken: 568.86 secs

    Epoch: 1 Validation Loss 0.169 Time taken: 23.09 secs| Accuracy 95.988

Epoch: 2 Train Loss 0.143 Time taken: 570.86 secs

    Epoch: 2 Validation Loss 0.079 Time taken: 23.02 secs| Accuracy 97.513

Epoch: 3 Train Loss 0.091 Time taken: 564.87 secs

    Epoch: 3 Validation Loss 0.067 Time taken: 23.2 secs| Accuracy 97.725

Epoch: 4 Train Loss 0.066 Time taken: 565.13 secs

    Epoch: 4 Validation Loss 0.049 Time taken: 23.12 secs| Accuracy 98.488

Epoch: 5 Train Loss 0.05 Time taken: 563.16 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 23.23 secs| Accuracy 98.319
T\P      0       1
0       5201        0108        
1       0037        5271        

Accuracy 98.634
Time taken: 34.05 secs
2982.962 secs

Mon Apr 18 14:02:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=350
Mon Apr 18 14:02:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.285 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 350,
    "message": "346 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.349 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.24 Time taken: 567.85 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.14 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.088 Time taken: 568.68 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.11 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.058 Time taken: 567.54 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 23.09 secs| Accuracy 98.404

Epoch: 4 Train Loss 0.042 Time taken: 561.69 secs

    Epoch: 4 Validation Loss 0.067 Time taken: 23.22 secs| Accuracy 97.951

Epoch: 5 Train Loss 0.034 Time taken: 559.83 secs

    Epoch: 5 Validation Loss 0.028 Time taken: 23.18 secs| Accuracy 99.152
T\P      0       1
0       5287        0022        
1       0063        5245        

Accuracy 99.199
Time taken: 34.14 secs
2976.018 secs

Mon Apr 18 14:53:32 2022



======================================================================
----------------------------------------------------------------------
                run_ID=383
Mon Apr 18 20:57:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.538 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 383,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.874 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.183 Time taken: 617.48 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 23.99 secs| Accuracy 98.374

Epoch: 2 Train Loss 0.07 Time taken: 611.57 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 23.87 secs| Accuracy 97.981

Epoch: 3 Train Loss 0.048 Time taken: 611.72 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.88 secs| Accuracy 98.74

Epoch: 4 Train Loss 0.038 Time taken: 609.24 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 24.03 secs| Accuracy 98.848

Epoch: 5 Train Loss 0.027 Time taken: 607.64 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 23.96 secs| Accuracy 98.875
T\P      0       1
0       5297        0105        
1       0130        5539        

Accuracy 97.877
Time taken: 35.44 secs
3227.831 secs

Mon Apr 18 21:51:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=384
Mon Apr 18 21:51:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.136 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 384,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.385 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 611.78 secs

    Epoch: 1 Validation Loss 0.05 Time taken: 24.04 secs| Accuracy 98.442

Epoch: 2 Train Loss 0.069 Time taken: 608.57 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 24.44 secs| Accuracy 98.523

Epoch: 3 Train Loss 0.045 Time taken: 607.33 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 24.22 secs| Accuracy 98.442

Epoch: 4 Train Loss 0.036 Time taken: 606.92 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 24.21 secs| Accuracy 98.889

Epoch: 5 Train Loss 0.025 Time taken: 611.66 secs

    Epoch: 5 Validation Loss 0.046 Time taken: 24.08 secs| Accuracy 98.889
T\P      0       1
0       5338        0064        
1       0199        5470        

Accuracy 97.624
Time taken: 35.45 secs
3217.366 secs

Mon Apr 18 22:46:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=385
Mon Apr 18 22:46:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 25.503 GB
36 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 385,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 19.513 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.198 Time taken: 614.02 secs

    Epoch: 1 Validation Loss 0.048 Time taken: 24.19 secs| Accuracy 98.577

Epoch: 2 Train Loss 0.068 Time taken: 618.07 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 24.01 secs| Accuracy 98.848

Epoch: 3 Train Loss 0.043 Time taken: 620.64 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.89 secs| Accuracy 98.577

Epoch: 4 Train Loss 0.031 Time taken: 613.43 secs

    Epoch: 4 Validation Loss 0.032 Time taken: 24.13 secs| Accuracy 99.092

Epoch: 5 Train Loss 0.021 Time taken: 611.8 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 24.07 secs| Accuracy 98.835
T\P      0       1
0       5275        0127        
1       0091        5578        

Accuracy 98.031
Time taken: 35.57 secs
3247.3 secs

Mon Apr 18 23:41:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=386
Mon Apr 18 23:41:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.903 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 386,
    "message": "188 with include asha data after inshorts",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.471 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.211 Time taken: 608.64 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.53 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.066 Time taken: 610.75 secs

    Epoch: 2 Validation Loss 0.034 Time taken: 23.9 secs| Accuracy 98.997

Epoch: 3 Train Loss 0.044 Time taken: 603.92 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 24.04 secs| Accuracy 98.87
T\P      0       1
0       5277        0032        
1       0081        5227        

Accuracy 98.936
Time taken: 34.6 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.572 Time taken: 396.3 secs

    Epoch: 4 Validation Loss 0.402 Time taken: 2.73 secs| Accuracy 79.801

Epoch: 5 Train Loss 0.355 Time taken: 389.8 secs

    Epoch: 5 Validation Loss 0.315 Time taken: 2.42 secs| Accuracy 85.43

Epoch: 6 Train Loss 0.251 Time taken: 383.98 secs

    Epoch: 6 Validation Loss 0.304 Time taken: 2.49 secs| Accuracy 87.086
T\P      0       1
0       0180        0018        
1       0057        0199        

Accuracy 83.48
Time taken: 3.23 secs
3146.065 secs

Tue Apr 19 00:35:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=387
Tue Apr 19 00:35:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.173 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 387,
    "message": "386 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.456 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.268 Time taken: 602.97 secs

    Epoch: 1 Validation Loss 0.101 Time taken: 23.41 secs| Accuracy 97.061

Epoch: 2 Train Loss 0.133 Time taken: 607.21 secs

    Epoch: 2 Validation Loss 0.093 Time taken: 23.54 secs| Accuracy 97.118

Epoch: 3 Train Loss 0.09 Time taken: 609.48 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 23.43 secs| Accuracy 98.163
T\P      0       1
0       5280        0029        
1       0153        5155        

Accuracy 98.286
Time taken: 34.65 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.546 Time taken: 393.57 secs

    Epoch: 4 Validation Loss 0.448 Time taken: 2.51 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.472 Time taken: 389.06 secs

    Epoch: 5 Validation Loss 0.387 Time taken: 2.36 secs| Accuracy 82.45

Epoch: 6 Train Loss 0.342 Time taken: 386.57 secs

    Epoch: 6 Validation Loss 0.332 Time taken: 2.5 secs| Accuracy 84.106
T\P      0       1
0       0185        0013        
1       0069        0187        

Accuracy 81.938
Time taken: 2.97 secs
3139.018 secs

Tue Apr 19 01:28:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=388
Tue Apr 19 01:28:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.229 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 388,
    "message": "386 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.367 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.301 Time taken: 604.24 secs

    Epoch: 1 Validation Loss 0.117 Time taken: 23.62 secs| Accuracy 97.429

Epoch: 2 Train Loss 0.129 Time taken: 605.92 secs

    Epoch: 2 Validation Loss 0.065 Time taken: 23.36 secs| Accuracy 98.05

Epoch: 3 Train Loss 0.075 Time taken: 609.06 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.36 secs| Accuracy 98.672
T\P      0       1
0       5276        0033        
1       0091        5217        

Accuracy 98.832
Time taken: 35.06 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.511 Time taken: 392.31 secs

    Epoch: 4 Validation Loss 0.451 Time taken: 2.37 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.375 Time taken: 386.87 secs

    Epoch: 5 Validation Loss 0.417 Time taken: 2.56 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.283 Time taken: 384.97 secs

    Epoch: 6 Validation Loss 0.329 Time taken: 2.48 secs| Accuracy 83.113
T\P      0       1
0       0164        0034        
1       0041        0215        

Accuracy 83.48
Time taken: 3.11 secs
3138.03 secs

Tue Apr 19 02:21:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=389
Tue Apr 19 02:22:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.436 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 389,
    "message": "386 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 4.913 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.334 Time taken: 605.66 secs

    Epoch: 1 Validation Loss 0.152 Time taken: 23.57 secs| Accuracy 96.708

Epoch: 2 Train Loss 0.127 Time taken: 623.88 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 24.84 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.077 Time taken: 666.75 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 25.18 secs| Accuracy 97.669
T\P      0       1
0       5167        0142        
1       0045        5263        

Accuracy 98.239
Time taken: 36.54 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.6 Time taken: 460.6 secs

    Epoch: 4 Validation Loss 0.501 Time taken: 3.28 secs| Accuracy 77.815

Epoch: 5 Train Loss 0.437 Time taken: 452.09 secs

    Epoch: 5 Validation Loss 0.467 Time taken: 3.33 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.406 Time taken: 440.97 secs

    Epoch: 6 Validation Loss 0.415 Time taken: 2.25 secs| Accuracy 82.781
T\P      0       1
0       0159        0039        
1       0035        0221        

Accuracy 83.7
Time taken: 2.82 secs
3406.611 secs

Tue Apr 19 03:19:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=390
Tue Apr 19 03:19:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.012 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 390,
    "message": "386 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.371 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.224 Time taken: 658.34 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 24.99 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.08 Time taken: 678.96 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.4 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.054 Time taken: 601.75 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.3 secs| Accuracy 98.63
T\P      0       1
0       5245        0064        
1       0056        5252        

Accuracy 98.87
Time taken: 34.25 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.613 Time taken: 403.8 secs

    Epoch: 4 Validation Loss 0.451 Time taken: 2.47 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.477 Time taken: 385.6 secs

    Epoch: 5 Validation Loss 0.439 Time taken: 2.33 secs| Accuracy 78.808

Epoch: 6 Train Loss 0.329 Time taken: 391.08 secs

    Epoch: 6 Validation Loss 0.299 Time taken: 2.79 secs| Accuracy 87.748
T\P      0       1
0       0158        0040        
1       0035        0221        

Accuracy 83.48
Time taken: 3.13 secs
3274.875 secs

Tue Apr 19 04:15:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=392
Tue Apr 19 04:15:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.068 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 392,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.375 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.268 Time taken: 622.29 secs

    Epoch: 1 Validation Loss 0.101 Time taken: 23.63 secs| Accuracy 97.061

Epoch: 2 Train Loss 0.133 Time taken: 622.79 secs

    Epoch: 2 Validation Loss 0.093 Time taken: 23.6 secs| Accuracy 97.118

Epoch: 3 Train Loss 0.09 Time taken: 624.44 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 23.66 secs| Accuracy 98.163

Epoch: 4 Train Loss 0.062 Time taken: 624.63 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 24.53 secs| Accuracy 98.63

Epoch: 5 Train Loss 0.047 Time taken: 654.48 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 24.52 secs| Accuracy 98.63
T\P      0       1
0       5235        0074        
1       0043        5265        

Accuracy 98.898
Time taken: 35.87 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.614 Time taken: 443.78 secs

    Epoch: 6 Validation Loss 0.424 Time taken: 2.98 secs| Accuracy 80.464

Epoch: 7 Train Loss 0.396 Time taken: 442.75 secs

    Epoch: 7 Validation Loss 0.347 Time taken: 2.83 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.291 Time taken: 434.07 secs

    Epoch: 8 Validation Loss 0.302 Time taken: 3.09 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.224 Time taken: 434.61 secs

    Epoch: 9 Validation Loss 0.301 Time taken: 2.92 secs| Accuracy 86.424

Epoch: 10 Train Loss 0.171 Time taken: 436.43 secs

    Epoch: 10 Validation Loss 0.335 Time taken: 3.34 secs| Accuracy 87.417
T\P      0       1
0       0172        0026        
1       0024        0232        

Accuracy 88.987
Time taken: 3.53 secs
5559.755 secs

Tue Apr 19 05:49:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=393
Tue Apr 19 05:49:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.072 GB
33 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 393,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.258 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.301 Time taken: 654.87 secs

    Epoch: 1 Validation Loss 0.117 Time taken: 24.0 secs| Accuracy 97.429

Epoch: 2 Train Loss 0.129 Time taken: 654.26 secs

    Epoch: 2 Validation Loss 0.065 Time taken: 24.18 secs| Accuracy 98.05

Epoch: 3 Train Loss 0.075 Time taken: 646.06 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 24.37 secs| Accuracy 98.672

Epoch: 4 Train Loss 0.055 Time taken: 631.06 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 24.05 secs| Accuracy 98.813

Epoch: 5 Train Loss 0.041 Time taken: 625.59 secs

    Epoch: 5 Validation Loss 0.047 Time taken: 24.02 secs| Accuracy 98.46
T\P      0       1
0       5196        0113        
1       0037        5271        

Accuracy 98.587
Time taken: 35.1 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.506 Time taken: 411.15 secs

    Epoch: 6 Validation Loss 0.411 Time taken: 2.29 secs| Accuracy 80.464

Epoch: 7 Train Loss 0.334 Time taken: 406.09 secs

    Epoch: 7 Validation Loss 0.327 Time taken: 2.44 secs| Accuracy 84.106

Epoch: 8 Train Loss 0.263 Time taken: 414.8 secs

    Epoch: 8 Validation Loss 0.307 Time taken: 2.41 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.207 Time taken: 417.4 secs

    Epoch: 9 Validation Loss 0.352 Time taken: 2.59 secs| Accuracy 86.424

Epoch: 10 Train Loss 0.173 Time taken: 418.66 secs

    Epoch: 10 Validation Loss 0.356 Time taken: 2.69 secs| Accuracy 87.086
T\P      0       1
0       0180        0018        
1       0036        0220        

Accuracy 88.106
Time taken: 2.94 secs
5500.524 secs

Tue Apr 19 07:22:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=394
Tue Apr 19 07:22:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -9.63 GB
31 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 394,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.996 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.334 Time taken: 642.43 secs

    Epoch: 1 Validation Loss 0.152 Time taken: 23.54 secs| Accuracy 96.708

Epoch: 2 Train Loss 0.127 Time taken: 643.25 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 23.81 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.077 Time taken: 642.33 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 23.81 secs| Accuracy 97.669

Epoch: 4 Train Loss 0.053 Time taken: 634.54 secs

    Epoch: 4 Validation Loss 0.05 Time taken: 23.3 secs| Accuracy 98.827

Epoch: 5 Train Loss 0.042 Time taken: 629.13 secs

    Epoch: 5 Validation Loss 0.058 Time taken: 23.63 secs| Accuracy 98.587
T\P      0       1
0       5207        0102        
1       0040        5268        

Accuracy 98.663
Time taken: 34.58 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.541 Time taken: 410.51 secs

    Epoch: 6 Validation Loss 0.433 Time taken: 2.37 secs| Accuracy 81.788

Epoch: 7 Train Loss 0.386 Time taken: 408.47 secs

    Epoch: 7 Validation Loss 0.359 Time taken: 2.26 secs| Accuracy 85.762

Epoch: 8 Train Loss 0.343 Time taken: 410.29 secs

    Epoch: 8 Validation Loss 0.324 Time taken: 2.26 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.26 Time taken: 406.09 secs

    Epoch: 9 Validation Loss 0.304 Time taken: 2.26 secs| Accuracy 87.417

Epoch: 10 Train Loss 0.229 Time taken: 406.35 secs

    Epoch: 10 Validation Loss 0.303 Time taken: 2.24 secs| Accuracy 87.086
T\P      0       1
0       0161        0037        
1       0016        0240        

Accuracy 88.326
Time taken: 2.82 secs
5445.887 secs

Tue Apr 19 08:53:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=391
Tue Apr 19 10:59:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.953 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 391,
    "message": "188 with include asha data after inshorts best model",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.204 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.211 Time taken: 627.51 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.35 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.066 Time taken: 630.65 secs

    Epoch: 2 Validation Loss 0.034 Time taken: 23.18 secs| Accuracy 98.997

Epoch: 3 Train Loss 0.044 Time taken: 626.58 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 23.36 secs| Accuracy 98.87

Epoch: 4 Train Loss 0.032 Time taken: 657.93 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.21 secs| Accuracy 98.714

Epoch: 5 Train Loss 0.025 Time taken: 596.24 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.48 secs| Accuracy 98.969
T\P      0       1
0       5242        0067        
1       0041        5267        

Accuracy 98.983
Time taken: 34.88 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.58 Time taken: 403.65 secs

    Epoch: 6 Validation Loss 0.397 Time taken: 2.09 secs| Accuracy 82.45

Epoch: 7 Train Loss 0.377 Time taken: 398.01 secs

    Epoch: 7 Validation Loss 0.303 Time taken: 1.98 secs| Accuracy 84.437

Epoch: 8 Train Loss 0.227 Time taken: 416.44 secs

    Epoch: 8 Validation Loss 0.247 Time taken: 2.09 secs| Accuracy 90.066

Epoch: 9 Train Loss 0.166 Time taken: 406.34 secs

    Epoch: 9 Validation Loss 0.297 Time taken: 2.09 secs| Accuracy 85.762

Epoch: 10 Train Loss 0.146 Time taken: 386.46 secs

    Epoch: 10 Validation Loss 0.336 Time taken: 2.13 secs| Accuracy 86.755
T\P      0       1
0       0189        0009        
1       0049        0207        

Accuracy 87.225
Time taken: 2.55 secs
5356.538 secs

Tue Apr 19 12:30:13 2022




======================================================================
----------------------------------------------------------------------
                run_ID=415
Tue Apr 19 12:59:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.682 GB
43 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 415,
    "message": "412 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 15.502 GB
33 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.198 Time taken: 694.66 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 24.47 secs| Accuracy 97.528

Epoch: 2 Train Loss 0.069 Time taken: 583.78 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 22.25 secs| Accuracy 98.87

Epoch: 3 Train Loss 0.048 Time taken: 572.38 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 22.26 secs| Accuracy 98.884
T\P      0       1
0       5265        0044        
1       0059        5249        

Accuracy 99.03
Time taken: 33.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.582 Time taken: 386.21 secs

    Epoch: 4 Validation Loss 0.387 Time taken: 1.46 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.333 Time taken: 385.79 secs

    Epoch: 5 Validation Loss 0.405 Time taken: 1.39 secs| Accuracy 85.43

Epoch: 6 Train Loss 0.26 Time taken: 379.23 secs

    Epoch: 6 Validation Loss 0.288 Time taken: 1.39 secs| Accuracy 85.762
T\P      0       1
0       0180        0018        
1       0034        0222        

Accuracy 88.546
Time taken: 2.01 secs
3135.114 secs

Tue Apr 19 13:53:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=416
Tue Apr 19 13:53:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.56 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 416,
    "message": "412 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 11.366 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.254 Time taken: 570.4 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 22.33 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.095 Time taken: 567.55 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 22.34 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.074 Time taken: 568.85 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 22.31 secs| Accuracy 98.644
T\P      0       1
0       5248        0061        
1       0066        5242        

Accuracy 98.804
Time taken: 33.54 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.568 Time taken: 386.33 secs

    Epoch: 4 Validation Loss 0.46 Time taken: 1.46 secs| Accuracy 81.457

Epoch: 5 Train Loss 0.413 Time taken: 378.96 secs

    Epoch: 5 Validation Loss 0.391 Time taken: 1.48 secs| Accuracy 83.444

Epoch: 6 Train Loss 0.33 Time taken: 459.66 secs

    Epoch: 6 Validation Loss 0.319 Time taken: 2.06 secs| Accuracy 85.43
T\P      0       1
0       0157        0041        
1       0034        0222        

Accuracy 83.48
Time taken: 2.52 secs
3063.84 secs

Tue Apr 19 14:45:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=417
Tue Apr 19 14:45:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.311 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 417,
    "message": "188 with include asha data after inshorts and EL0L1L2 frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 9.493 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.207 Time taken: 696.53 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 25.12 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.075 Time taken: 728.66 secs

    Epoch: 2 Validation Loss 0.085 Time taken: 25.31 secs| Accuracy 97.429

Epoch: 3 Train Loss 0.055 Time taken: 736.44 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 24.78 secs| Accuracy 98.644
T\P      0       1
0       5200        0109        
1       0041        5267        

Accuracy 98.587
Time taken: 38.13 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.651 Time taken: 532.78 secs

    Epoch: 4 Validation Loss 0.403 Time taken: 2.26 secs| Accuracy 82.45

Epoch: 5 Train Loss 0.4 Time taken: 547.83 secs

    Epoch: 5 Validation Loss 0.37 Time taken: 2.03 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.303 Time taken: 422.42 secs

    Epoch: 6 Validation Loss 0.268 Time taken: 1.55 secs| Accuracy 89.073
T\P      0       1
0       0162        0036        
1       0036        0220        

Accuracy 84.141
Time taken: 2.09 secs
3812.606 secs

Tue Apr 19 15:50:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=418
Tue Apr 19 15:50:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.493 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 418,
    "message": "417 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 9.769 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.25 Time taken: 561.93 secs

    Epoch: 1 Validation Loss 0.078 Time taken: 22.52 secs| Accuracy 97.81

Epoch: 2 Train Loss 0.102 Time taken: 564.74 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.56 secs| Accuracy 98.46

Epoch: 3 Train Loss 0.074 Time taken: 557.66 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.77 secs| Accuracy 98.827
T\P      0       1
0       5264        0045        
1       0062        5246        

Accuracy 98.992
Time taken: 33.48 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.698 Time taken: 385.54 secs

    Epoch: 4 Validation Loss 0.567 Time taken: 1.57 secs| Accuracy 65.563

Epoch: 5 Train Loss 0.487 Time taken: 383.91 secs

    Epoch: 5 Validation Loss 0.445 Time taken: 1.55 secs| Accuracy 79.139

Epoch: 6 Train Loss 0.412 Time taken: 378.87 secs

    Epoch: 6 Validation Loss 0.356 Time taken: 1.58 secs| Accuracy 82.781
T\P      0       1
0       0148        0050        
1       0028        0228        

Accuracy 82.819
Time taken: 1.96 secs
2965.963 secs

Tue Apr 19 16:40:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=419
Tue Apr 19 16:40:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.064 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 419,
    "message": "417 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 0.718 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 560.41 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 22.29 secs| Accuracy 97.499

Epoch: 2 Train Loss 0.104 Time taken: 561.6 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.3 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.073 Time taken: 556.23 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 22.51 secs| Accuracy 98.601
T\P      0       1
0       5256        0053        
1       0071        5237        

Accuracy 98.832
Time taken: 33.37 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.668 Time taken: 379.49 secs

    Epoch: 4 Validation Loss 0.526 Time taken: 1.37 secs| Accuracy 71.854

Epoch: 5 Train Loss 0.457 Time taken: 381.51 secs

    Epoch: 5 Validation Loss 0.513 Time taken: 1.39 secs| Accuracy 81.126

Epoch: 6 Train Loss 0.369 Time taken: 385.0 secs

    Epoch: 6 Validation Loss 0.347 Time taken: 1.38 secs| Accuracy 83.113
T\P      0       1
0       0150        0048        
1       0031        0225        

Accuracy 82.599
Time taken: 1.91 secs
2954.258 secs

Tue Apr 19 17:30:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=420
Tue Apr 19 17:30:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.067 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 420,
    "message": "417 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.35 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 562.15 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 22.54 secs| Accuracy 97.697

Epoch: 2 Train Loss 0.072 Time taken: 562.28 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.46 secs| Accuracy 98.7

Epoch: 3 Train Loss 0.05 Time taken: 558.79 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 22.53 secs| Accuracy 98.983
T\P      0       1
0       5281        0028        
1       0080        5228        

Accuracy 98.983
Time taken: 33.48 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.707 Time taken: 384.03 secs

    Epoch: 4 Validation Loss 0.574 Time taken: 1.37 secs| Accuracy 67.55

Epoch: 5 Train Loss 0.439 Time taken: 375.66 secs

    Epoch: 5 Validation Loss 0.473 Time taken: 1.37 secs| Accuracy 77.483

Epoch: 6 Train Loss 0.39 Time taken: 374.21 secs

    Epoch: 6 Validation Loss 0.365 Time taken: 1.42 secs| Accuracy 84.768
T\P      0       1
0       0172        0026        
1       0049        0207        

Accuracy 83.48
Time taken: 1.93 secs
2949.325 secs

Tue Apr 19 18:21:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=421
Tue Apr 19 18:21:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.006 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 421,
    "message": "417 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.351 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.26 Time taken: 562.04 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 22.42 secs| Accuracy 97.287

Epoch: 2 Train Loss 0.087 Time taken: 559.14 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 22.49 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.064 Time taken: 562.23 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.49 secs| Accuracy 98.785
T\P      0       1
0       5242        0067        
1       0055        5253        

Accuracy 98.851
Time taken: 33.58 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.645 Time taken: 384.73 secs

    Epoch: 4 Validation Loss 0.459 Time taken: 1.5 secs| Accuracy 82.45

Epoch: 5 Train Loss 0.422 Time taken: 382.3 secs

    Epoch: 5 Validation Loss 0.347 Time taken: 1.54 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.275 Time taken: 377.09 secs

    Epoch: 6 Validation Loss 0.263 Time taken: 1.55 secs| Accuracy 85.762
T\P      0       1
0       0169        0029        
1       0040        0216        

Accuracy 84.802
Time taken: 1.99 secs
2959.854 secs

Tue Apr 19 19:11:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=422
Tue Apr 19 19:11:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 422,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.354 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.218 Time taken: 577.61 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.34 secs| Accuracy 97.81

Epoch: 2 Train Loss 0.075 Time taken: 571.97 secs

    Epoch: 2 Validation Loss 0.099 Time taken: 22.53 secs| Accuracy 97.188

Epoch: 3 Train Loss 0.053 Time taken: 574.63 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.63 secs| Accuracy 97.909
T\P      0       1
0       5146        0163        
1       0037        5271        

Accuracy 98.116
Time taken: 33.5 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.577 Time taken: 390.75 secs

    Epoch: 4 Validation Loss 0.354 Time taken: 1.67 secs| Accuracy 85.099

Epoch: 5 Train Loss 0.315 Time taken: 381.5 secs

    Epoch: 5 Validation Loss 0.284 Time taken: 1.59 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.226 Time taken: 377.07 secs

    Epoch: 6 Validation Loss 0.255 Time taken: 1.6 secs| Accuracy 88.079
T\P      0       1
0       0159        0039        
1       0020        0236        

Accuracy 87.004
Time taken: 2.05 secs
3007.738 secs

Tue Apr 19 20:02:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=439
Tue Apr 19 21:05:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.261 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 439,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 8.578 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.264 Time taken: 572.64 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.31 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.072 Time taken: 565.5 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 22.48 secs| Accuracy 98.517

Epoch: 3 Train Loss 0.049 Time taken: 565.06 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.41 secs| Accuracy 98.432
T\P      0       1
0       5190        0119        
1       0034        5274        

Accuracy 98.559
Time taken: 33.32 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.727 Time taken: 387.22 secs

    Epoch: 4 Validation Loss 0.515 Time taken: 1.57 secs| Accuracy 75.828

Epoch: 5 Train Loss 0.45 Time taken: 380.94 secs

    Epoch: 5 Validation Loss 0.48 Time taken: 1.42 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.366 Time taken: 376.45 secs

    Epoch: 6 Validation Loss 0.415 Time taken: 1.5 secs| Accuracy 84.106
T\P      0       1
0       0151        0047        
1       0029        0227        

Accuracy 83.26
Time taken: 2.03 secs
2979.531 secs

Tue Apr 19 21:56:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=440
Tue Apr 19 21:56:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.487 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 440,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.306 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.286 Time taken: 569.98 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.41 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.086 Time taken: 563.23 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 22.4 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.057 Time taken: 563.36 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 22.46 secs| Accuracy 98.87
T\P      0       1
0       5239        0070        
1       0047        5261        

Accuracy 98.898
Time taken: 33.43 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.647 Time taken: 385.38 secs

    Epoch: 4 Validation Loss 0.542 Time taken: 1.51 secs| Accuracy 76.49

Epoch: 5 Train Loss 0.465 Time taken: 388.81 secs

    Epoch: 5 Validation Loss 0.487 Time taken: 1.56 secs| Accuracy 80.795

Epoch: 6 Train Loss 0.382 Time taken: 379.28 secs

    Epoch: 6 Validation Loss 0.47 Time taken: 1.53 secs| Accuracy 83.775
T\P      0       1
0       0174        0024        
1       0060        0196        

Accuracy 81.498
Time taken: 1.93 secs
2982.086 secs

Tue Apr 19 22:47:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=441
Tue Apr 19 22:47:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.016 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 441,
    "message": "188 with include asha data after inshorts and EL0L1L2 frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.4 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.271 Time taken: 560.9 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.37 secs| Accuracy 97.739

Epoch: 2 Train Loss 0.089 Time taken: 559.58 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 22.5 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.061 Time taken: 555.98 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 22.59 secs| Accuracy 98.375
T\P      0       1
0       5227        0082        
1       0053        5255        

Accuracy 98.728
Time taken: 33.53 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.566 Time taken: 383.69 secs

    Epoch: 4 Validation Loss 0.445 Time taken: 1.48 secs| Accuracy 81.457

Epoch: 5 Train Loss 0.385 Time taken: 389.82 secs

    Epoch: 5 Validation Loss 0.346 Time taken: 1.61 secs| Accuracy 86.093

Epoch: 6 Train Loss 0.292 Time taken: 379.3 secs

    Epoch: 6 Validation Loss 0.288 Time taken: 1.63 secs| Accuracy 86.755
T\P      0       1
0       0155        0043        
1       0021        0235        

Accuracy 85.903
Time taken: 2.11 secs
2962.82 secs

Tue Apr 19 23:37:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=442
Tue Apr 19 23:37:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.068 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 442,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.349 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.243 Time taken: 560.45 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 22.39 secs| Accuracy 98.488

Epoch: 2 Train Loss 0.072 Time taken: 564.24 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 22.33 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.053 Time taken: 559.22 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.56 secs| Accuracy 98.446
T\P      0       1
0       5227        0082        
1       0047        5261        

Accuracy 98.785
Time taken: 33.48 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.639 Time taken: 385.99 secs

    Epoch: 4 Validation Loss 0.417 Time taken: 1.53 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.389 Time taken: 379.34 secs

    Epoch: 5 Validation Loss 0.4 Time taken: 1.51 secs| Accuracy 84.768

Epoch: 6 Train Loss 0.287 Time taken: 375.77 secs

    Epoch: 6 Validation Loss 0.304 Time taken: 1.51 secs| Accuracy 86.093
T\P      0       1
0       0174        0024        
1       0040        0216        

Accuracy 85.903
Time taken: 2.0 secs
2956.395 secs

Wed Apr 20 00:27:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=443
Wed Apr 20 00:27:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 443,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.352 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.294 Time taken: 561.16 secs

    Epoch: 1 Validation Loss 0.081 Time taken: 22.49 secs| Accuracy 97.754

Epoch: 2 Train Loss 0.1 Time taken: 565.64 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.29 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.069 Time taken: 565.34 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 22.44 secs| Accuracy 98.418
T\P      0       1
0       5236        0073        
1       0059        5249        

Accuracy 98.757
Time taken: 33.58 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.631 Time taken: 382.47 secs

    Epoch: 4 Validation Loss 0.491 Time taken: 1.56 secs| Accuracy 78.477

Epoch: 5 Train Loss 0.412 Time taken: 384.5 secs

    Epoch: 5 Validation Loss 0.397 Time taken: 1.58 secs| Accuracy 83.444

Epoch: 6 Train Loss 0.319 Time taken: 376.81 secs

    Epoch: 6 Validation Loss 0.334 Time taken: 1.58 secs| Accuracy 85.762
T\P      0       1
0       0150        0048        
1       0025        0231        

Accuracy 83.921
Time taken: 2.05 secs
2967.392 secs

Wed Apr 20 01:18:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=444
Wed Apr 20 01:18:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.063 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 444,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.349 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.271 Time taken: 601.0 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 24.64 secs| Accuracy 98.177

Epoch: 2 Train Loss 0.087 Time taken: 666.65 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.63 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.058 Time taken: 668.57 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 24.38 secs| Accuracy 98.799
T\P      0       1
0       5259        0050        
1       0074        5234        

Accuracy 98.832
Time taken: 35.94 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.727 Time taken: 479.33 secs

    Epoch: 4 Validation Loss 0.601 Time taken: 1.86 secs| Accuracy 80.464

Epoch: 5 Train Loss 0.509 Time taken: 474.73 secs

    Epoch: 5 Validation Loss 0.45 Time taken: 1.98 secs| Accuracy 80.464

Epoch: 6 Train Loss 0.404 Time taken: 473.45 secs

    Epoch: 6 Validation Loss 0.407 Time taken: 1.66 secs| Accuracy 84.106
T\P      0       1
0       0163        0035        
1       0042        0214        

Accuracy 83.04
Time taken: 2.31 secs
3504.4 secs

Wed Apr 20 02:17:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=445
Wed Apr 20 02:17:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.276 GB
35 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 445,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.228 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.26 Time taken: 669.67 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 24.28 secs| Accuracy 98.389

Epoch: 2 Train Loss 0.073 Time taken: 669.85 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 24.22 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.054 Time taken: 667.92 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 24.95 secs| Accuracy 98.587
T\P      0       1
0       5221        0088        
1       0042        5266        

Accuracy 98.776
Time taken: 36.68 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.746 Time taken: 480.61 secs

    Epoch: 4 Validation Loss 0.657 Time taken: 1.6 secs| Accuracy 58.94

Epoch: 5 Train Loss 0.625 Time taken: 489.87 secs

    Epoch: 5 Validation Loss 0.602 Time taken: 1.63 secs| Accuracy 62.583

Epoch: 6 Train Loss 0.518 Time taken: 482.85 secs

    Epoch: 6 Validation Loss 0.411 Time taken: 1.96 secs| Accuracy 82.45
T\P      0       1
0       0133        0065        
1       0027        0229        

Accuracy 79.736
Time taken: 2.34 secs
3603.944 secs

Wed Apr 20 03:19:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=446
Wed Apr 20 03:19:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.977 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 446,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.344 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 678.98 secs

    Epoch: 1 Validation Loss 0.082 Time taken: 24.19 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.106 Time taken: 694.1 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.91 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.07 Time taken: 686.49 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 24.16 secs| Accuracy 98.135
T\P      0       1
0       5194        0115        
1       0048        5260        

Accuracy 98.465
Time taken: 35.54 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.56 Time taken: 485.72 secs

    Epoch: 4 Validation Loss 0.436 Time taken: 1.74 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.415 Time taken: 489.62 secs

    Epoch: 5 Validation Loss 0.358 Time taken: 1.56 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.347 Time taken: 478.03 secs

    Epoch: 6 Validation Loss 0.317 Time taken: 1.89 secs| Accuracy 86.755
T\P      0       1
0       0160        0038        
1       0047        0209        

Accuracy 81.278
Time taken: 2.35 secs
3653.073 secs

Wed Apr 20 04:21:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=458
Wed Apr 20 12:06:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1057 | Validation_set: 426 | Test_set: 652
Train_batches: 34 | Validation_batches: 14 | Test_batches: 21
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.59 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 458,
    "message": "just using asha_sentsim data with negs repeated twice",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 8.971 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.688 Time taken: 480.69 secs

    Epoch: 1 Validation Loss 0.674 Time taken: 2.13 secs| Macro F: 0.368

Epoch: 2 Train Loss 0.629 Time taken: 482.99 secs

    Epoch: 2 Validation Loss 0.545 Time taken: 2.19 secs| Macro F: 0.368

Epoch: 3 Train Loss 0.524 Time taken: 470.78 secs

    Epoch: 3 Validation Loss 0.469 Time taken: 2.01 secs| Macro F: 0.826

Epoch: 4 Train Loss 0.495 Time taken: 474.97 secs

    Epoch: 4 Validation Loss 0.491 Time taken: 2.19 secs| Macro F: 0.696

Epoch: 5 Train Loss 0.443 Time taken: 474.03 secs

    Epoch: 5 Validation Loss 0.412 Time taken: 1.88 secs| Macro F: 0.809
T\P      0       1
0       0304        0092        
1       0035        0221        

Macro F: 0.802
Time taken: 2.95 secs
2396.837 secs

Wed Apr 20 12:47:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=459
Wed Apr 20 12:47:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1057 | Validation_set: 426 | Test_set: 652
Train_batches: 34 | Validation_batches: 14 | Test_batches: 21
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.424 GB
26 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 459,
    "message": "458 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.204 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.684 Time taken: 454.83 secs

    Epoch: 1 Validation Loss 0.683 Time taken: 2.09 secs| Macro F: 0.368

Epoch: 2 Train Loss 0.67 Time taken: 450.06 secs

    Epoch: 2 Validation Loss 0.599 Time taken: 2.15 secs| Macro F: 0.414

Epoch: 3 Train Loss 0.55 Time taken: 460.77 secs

    Epoch: 3 Validation Loss 0.464 Time taken: 1.87 secs| Macro F: 0.771

Epoch: 4 Train Loss 0.445 Time taken: 457.38 secs

    Epoch: 4 Validation Loss 0.389 Time taken: 1.96 secs| Macro F: 0.833

Epoch: 5 Train Loss 0.416 Time taken: 459.33 secs

    Epoch: 5 Validation Loss 0.362 Time taken: 2.19 secs| Macro F: 0.846
T\P      0       1
0       0356        0040        
1       0069        0187        

Macro F: 0.821
Time taken: 2.61 secs
2295.266 secs

Wed Apr 20 13:27:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=460
Wed Apr 20 13:27:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1057 | Validation_set: 426 | Test_set: 652
Train_batches: 34 | Validation_batches: 14 | Test_batches: 21
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.4 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 460,
    "message": "458 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.393 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.679 Time taken: 463.7 secs

    Epoch: 1 Validation Loss 0.669 Time taken: 2.24 secs| Macro F: 0.368

Epoch: 2 Train Loss 0.616 Time taken: 456.15 secs

    Epoch: 2 Validation Loss 0.534 Time taken: 2.31 secs| Macro F: 0.747

Epoch: 3 Train Loss 0.525 Time taken: 454.85 secs

    Epoch: 3 Validation Loss 0.503 Time taken: 2.13 secs| Macro F: 0.694

Epoch: 4 Train Loss 0.426 Time taken: 457.02 secs

    Epoch: 4 Validation Loss 0.372 Time taken: 2.19 secs| Macro F: 0.816

Epoch: 5 Train Loss 0.352 Time taken: 463.24 secs

    Epoch: 5 Validation Loss 0.375 Time taken: 1.94 secs| Macro F: 0.819
T\P      0       1
0       0292        0104        
1       0022        0234        

Macro F: 0.805
Time taken: 2.78 secs
2308.571 secs

Wed Apr 20 14:06:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=481
Wed Apr 20 16:49:07 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.91 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 481,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 145,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 9.609 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 671.14 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 23.91 secs| Accuracy 98.46

Epoch: 2 Train Loss 0.069 Time taken: 677.75 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 24.89 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.047 Time taken: 667.99 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 24.16 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.033 Time taken: 661.1 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 24.09 secs| Accuracy 98.319

Epoch: 5 Train Loss 0.024 Time taken: 658.83 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 24.15 secs| Accuracy 98.7
T\P      0       1
0       5256        0053        
1       0042        5266        

Accuracy 99.105
Time taken: 35.82 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.571 Time taken: 465.42 secs

    Epoch: 6 Validation Loss 0.44 Time taken: 2.05 secs| Accuracy 79.139

Epoch: 7 Train Loss 0.349 Time taken: 463.87 secs

    Epoch: 7 Validation Loss 0.318 Time taken: 1.66 secs| Accuracy 86.424
T\P      0       1
0       0172        0026        
1       0041        0215        

Accuracy 85.242
Time taken: 2.26 secs
4471.578 secs

Wed Apr 20 18:04:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=482
Wed Apr 20 18:04:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.52 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 482,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 146,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.333 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.2 Time taken: 670.47 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 24.45 secs| Accuracy 98.262

Epoch: 2 Train Loss 0.067 Time taken: 669.58 secs

    Epoch: 2 Validation Loss 0.067 Time taken: 24.38 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.047 Time taken: 675.26 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 24.37 secs| Accuracy 98.94

Epoch: 4 Train Loss 0.036 Time taken: 664.35 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 24.47 secs| Accuracy 98.969

Epoch: 5 Train Loss 0.028 Time taken: 663.54 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 24.38 secs| Accuracy 99.082
T\P      0       1
0       5266        0043        
1       0060        5248        

Accuracy 99.03
Time taken: 36.07 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.631 Time taken: 477.7 secs

    Epoch: 6 Validation Loss 0.426 Time taken: 1.88 secs| Accuracy 83.775

Epoch: 7 Train Loss 0.407 Time taken: 485.89 secs

    Epoch: 7 Validation Loss 0.356 Time taken: 1.94 secs| Accuracy 82.781
T\P      0       1
0       0141        0057        
1       0032        0224        

Accuracy 80.396
Time taken: 2.36 secs
4520.383 secs

Wed Apr 20 19:21:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=483
Wed Apr 20 19:21:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.305 GB
34 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 483,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 147,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.337 GB
33 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.303 Time taken: 669.11 secs

    Epoch: 1 Validation Loss 0.115 Time taken: 24.08 secs| Accuracy 97.061

Epoch: 2 Train Loss 0.138 Time taken: 670.17 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 23.97 secs| Accuracy 98.064

Epoch: 3 Train Loss 0.087 Time taken: 670.76 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 24.56 secs| Accuracy 98.545

Epoch: 4 Train Loss 0.062 Time taken: 661.57 secs

    Epoch: 4 Validation Loss 0.035 Time taken: 24.21 secs| Accuracy 98.94

Epoch: 5 Train Loss 0.047 Time taken: 664.28 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.99 secs| Accuracy 98.94
T\P      0       1
0       5242        0067        
1       0039        5269        

Accuracy 99.002
Time taken: 35.77 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.598 Time taken: 464.36 secs

    Epoch: 6 Validation Loss 0.374 Time taken: 1.86 secs| Accuracy 82.781

Epoch: 7 Train Loss 0.394 Time taken: 468.12 secs

    Epoch: 7 Validation Loss 0.31 Time taken: 1.69 secs| Accuracy 85.762
T\P      0       1
0       0160        0038        
1       0050        0206        

Accuracy 80.617
Time taken: 2.09 secs
4486.616 secs

Wed Apr 20 20:37:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=484
Wed Apr 20 20:37:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.0 GB
32 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 484,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 148,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.49 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 652.09 secs

    Epoch: 1 Validation Loss 0.083 Time taken: 23.86 secs| Accuracy 97.584

Epoch: 2 Train Loss 0.085 Time taken: 665.06 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.62 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.066 Time taken: 689.03 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 23.52 secs| Accuracy 98.418

Epoch: 4 Train Loss 0.046 Time taken: 671.95 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 24.19 secs| Accuracy 98.601

Epoch: 5 Train Loss 0.039 Time taken: 669.14 secs

    Epoch: 5 Validation Loss 0.046 Time taken: 24.12 secs| Accuracy 98.841
T\P      0       1
0       5257        0052        
1       0054        5254        

Accuracy 99.002
Time taken: 35.99 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.611 Time taken: 473.73 secs

    Epoch: 6 Validation Loss 0.468 Time taken: 1.72 secs| Accuracy 83.444

Epoch: 7 Train Loss 0.443 Time taken: 450.19 secs

    Epoch: 7 Validation Loss 0.383 Time taken: 1.8 secs| Accuracy 84.106
T\P      0       1
0       0165        0033        
1       0062        0194        

Accuracy 79.075
Time taken: 2.47 secs
4484.47 secs

Wed Apr 20 21:53:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=485
Wed Apr 20 21:53:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.333 GB
38 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 485,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 149,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.618 GB
33 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 694.02 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 24.43 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.074 Time taken: 686.61 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 24.38 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.055 Time taken: 696.65 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 24.77 secs| Accuracy 98.418

Epoch: 4 Train Loss 0.044 Time taken: 689.35 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.75 secs| Accuracy 98.94

Epoch: 5 Train Loss 0.033 Time taken: 690.2 secs

    Epoch: 5 Validation Loss 0.064 Time taken: 24.26 secs| Accuracy 98.149
T\P      0       1
0       5173        0136        
1       0035        5273        

Accuracy 98.389
Time taken: 36.01 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.69 Time taken: 492.51 secs

    Epoch: 6 Validation Loss 0.579 Time taken: 2.04 secs| Accuracy 65.563

Epoch: 7 Train Loss 0.497 Time taken: 498.39 secs

    Epoch: 7 Validation Loss 0.387 Time taken: 2.14 secs| Accuracy 85.43
T\P      0       1
0       0150        0048        
1       0046        0210        

Accuracy 79.295
Time taken: 2.64 secs
4656.591 secs

Wed Apr 20 23:12:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=486
Wed Apr 20 23:12:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.361 GB
39 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 486,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 140,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.132 GB
28 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 705.03 secs

    Epoch: 1 Validation Loss 0.095 Time taken: 24.41 secs| Accuracy 97.443

Epoch: 2 Train Loss 0.087 Time taken: 702.07 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 24.47 secs| Accuracy 98.474

Epoch: 3 Train Loss 0.064 Time taken: 707.46 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 24.57 secs| Accuracy 98.587

Epoch: 4 Train Loss 0.047 Time taken: 585.47 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.26 secs| Accuracy 98.743

Epoch: 5 Train Loss 0.038 Time taken: 580.99 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 22.47 secs| Accuracy 98.799
T\P      0       1
0       5256        0053        
1       0055        5253        

Accuracy 98.983
Time taken: 33.71 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.609 Time taken: 385.9 secs

    Epoch: 6 Validation Loss 0.471 Time taken: 1.48 secs| Accuracy 79.139

Epoch: 7 Train Loss 0.428 Time taken: 378.06 secs

    Epoch: 7 Validation Loss 0.465 Time taken: 1.51 secs| Accuracy 77.152
T\P      0       1
0       0158        0040        
1       0055        0201        

Accuracy 79.075
Time taken: 1.99 secs
4258.628 secs

Thu Apr 21 00:24:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=487
Thu Apr 21 00:24:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.061 GB
35 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 487,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 141,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.379 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 581.02 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 22.61 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.076 Time taken: 580.67 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 22.41 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.058 Time taken: 574.72 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 22.5 secs| Accuracy 98.955

Epoch: 4 Train Loss 0.04 Time taken: 574.38 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 22.51 secs| Accuracy 98.743

Epoch: 5 Train Loss 0.035 Time taken: 573.08 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 22.49 secs| Accuracy 99.068
T\P      0       1
0       5240        0069        
1       0038        5270        

Accuracy 98.992
Time taken: 33.31 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.627 Time taken: 411.23 secs

    Epoch: 6 Validation Loss 0.402 Time taken: 1.83 secs| Accuracy 82.781

Epoch: 7 Train Loss 0.361 Time taken: 464.32 secs

    Epoch: 7 Validation Loss 0.356 Time taken: 1.83 secs| Accuracy 83.775
T\P      0       1
0       0158        0040        
1       0037        0219        

Accuracy 83.04
Time taken: 2.21 secs
3962.84 secs

Thu Apr 21 01:32:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=488
Thu Apr 21 01:32:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.805 GB
35 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 488,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 142,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.544 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 644.65 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 23.79 secs| Accuracy 98.361

Epoch: 2 Train Loss 0.085 Time taken: 591.29 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.51 secs| Accuracy 98.785

Epoch: 3 Train Loss 0.061 Time taken: 574.91 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 22.61 secs| Accuracy 98.898

Epoch: 4 Train Loss 0.045 Time taken: 574.42 secs

    Epoch: 4 Validation Loss 0.031 Time taken: 22.55 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.034 Time taken: 573.38 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 22.4 secs| Accuracy 98.799
T\P      0       1
0       5199        0110        
1       0030        5278        

Accuracy 98.681
Time taken: 33.45 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.672 Time taken: 383.39 secs

    Epoch: 6 Validation Loss 0.502 Time taken: 1.56 secs| Accuracy 78.477

Epoch: 7 Train Loss 0.441 Time taken: 380.82 secs

    Epoch: 7 Validation Loss 0.358 Time taken: 1.55 secs| Accuracy 82.119
T\P      0       1
0       0167        0031        
1       0052        0204        

Accuracy 81.718
Time taken: 1.97 secs
3926.527 secs

Thu Apr 21 02:38:42 2022


======================================================================
----------------------------------------------------------------------
                run_ID=489
Thu Apr 21 02:38:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.07 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 489,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 143,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.357 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.199 Time taken: 578.83 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 22.4 secs| Accuracy 98.177

Epoch: 2 Train Loss 0.073 Time taken: 584.17 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.33 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.049 Time taken: 576.81 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.64 secs| Accuracy 98.827

Epoch: 4 Train Loss 0.037 Time taken: 574.07 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 22.37 secs| Accuracy 98.884

Epoch: 5 Train Loss 0.028 Time taken: 572.59 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 22.28 secs| Accuracy 99.096
T\P      0       1
0       5270        0039        
1       0063        5245        

Accuracy 99.039
Time taken: 33.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.627 Time taken: 385.46 secs

    Epoch: 6 Validation Loss 0.371 Time taken: 1.46 secs| Accuracy 83.775

Epoch: 7 Train Loss 0.34 Time taken: 377.93 secs

    Epoch: 7 Validation Loss 0.331 Time taken: 1.51 secs| Accuracy 87.086
T\P      0       1
0       0173        0025        
1       0049        0207        

Accuracy 83.7
Time taken: 1.96 secs
3850.327 secs

Thu Apr 21 03:43:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=490
Thu Apr 21 03:44:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.088 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 490,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 144,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.334 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 579.94 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.24 secs| Accuracy 98.262

Epoch: 2 Train Loss 0.069 Time taken: 577.34 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.25 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.048 Time taken: 574.59 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 22.27 secs| Accuracy 98.262

Epoch: 4 Train Loss 0.038 Time taken: 571.25 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 22.45 secs| Accuracy 98.827

Epoch: 5 Train Loss 0.03 Time taken: 573.4 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 22.37 secs| Accuracy 98.785
T\P      0       1
0       5248        0061        
1       0038        5270        

Accuracy 99.068
Time taken: 33.26 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.636 Time taken: 385.65 secs

    Epoch: 6 Validation Loss 0.493 Time taken: 1.47 secs| Accuracy 80.795

Epoch: 7 Train Loss 0.421 Time taken: 379.02 secs

    Epoch: 7 Validation Loss 0.394 Time taken: 1.47 secs| Accuracy 83.775
T\P      0       1
0       0177        0021        
1       0068        0188        

Accuracy 80.396
Time taken: 1.95 secs
3826.88 secs

Thu Apr 21 04:48:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=491
Thu Apr 21 04:48:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.066 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 491,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 145,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.363 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.187 Time taken: 577.7 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 22.25 secs| Accuracy 98.064

Epoch: 2 Train Loss 0.064 Time taken: 572.89 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.31 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.047 Time taken: 572.3 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 22.36 secs| Accuracy 98.785

Epoch: 4 Train Loss 0.037 Time taken: 573.18 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 22.43 secs| Accuracy 98.771

Epoch: 5 Train Loss 0.027 Time taken: 572.14 secs

    Epoch: 5 Validation Loss 0.051 Time taken: 22.39 secs| Accuracy 98.672
T\P      0       1
0       5235        0074        
1       0037        5271        

Accuracy 98.955
Time taken: 33.17 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.576 Time taken: 378.58 secs

    Epoch: 6 Validation Loss 0.392 Time taken: 1.5 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.356 Time taken: 376.33 secs

    Epoch: 7 Validation Loss 0.361 Time taken: 1.45 secs| Accuracy 85.43
T\P      0       1
0       0159        0039        
1       0040        0216        

Accuracy 82.599
Time taken: 1.97 secs
3810.4 secs

Thu Apr 21 05:53:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=492
Thu Apr 21 05:53:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.063 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 492,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 146,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.308 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.211 Time taken: 577.44 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 22.31 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.077 Time taken: 572.3 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.44 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.06 Time taken: 572.11 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 22.52 secs| Accuracy 98.743

Epoch: 4 Train Loss 0.042 Time taken: 581.45 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 22.44 secs| Accuracy 98.983

Epoch: 5 Train Loss 0.033 Time taken: 574.99 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 22.44 secs| Accuracy 98.912
T\P      0       1
0       5262        0047        
1       0052        5256        

Accuracy 99.068
Time taken: 33.39 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.58 Time taken: 384.69 secs

    Epoch: 6 Validation Loss 0.381 Time taken: 1.57 secs| Accuracy 83.113

Epoch: 7 Train Loss 0.36 Time taken: 379.69 secs

    Epoch: 7 Validation Loss 0.366 Time taken: 1.58 secs| Accuracy 81.457
T\P      0       1
0       0131        0067        
1       0023        0233        

Accuracy 80.176
Time taken: 2.01 secs
3836.96 secs

Thu Apr 21 06:58:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=493
Thu Apr 21 06:58:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.066 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 493,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 147,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.388 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.3 Time taken: 578.3 secs

    Epoch: 1 Validation Loss 0.121 Time taken: 22.42 secs| Accuracy 97.626

Epoch: 2 Train Loss 0.14 Time taken: 581.26 secs

    Epoch: 2 Validation Loss 0.074 Time taken: 22.22 secs| Accuracy 97.994

Epoch: 3 Train Loss 0.091 Time taken: 576.71 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.3 secs| Accuracy 98.22

Epoch: 4 Train Loss 0.068 Time taken: 571.86 secs

    Epoch: 4 Validation Loss 0.058 Time taken: 22.48 secs| Accuracy 98.418

Epoch: 5 Train Loss 0.051 Time taken: 571.73 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 22.29 secs| Accuracy 98.728
T\P      0       1
0       5213        0096        
1       0053        5255        

Accuracy 98.597
Time taken: 33.15 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.626 Time taken: 383.82 secs

    Epoch: 6 Validation Loss 0.382 Time taken: 1.54 secs| Accuracy 83.444

Epoch: 7 Train Loss 0.394 Time taken: 383.01 secs

    Epoch: 7 Validation Loss 0.333 Time taken: 1.49 secs| Accuracy 85.099
T\P      0       1
0       0162        0036        
1       0059        0197        

Accuracy 79.075
Time taken: 2.0 secs
3849.234 secs

Thu Apr 21 08:03:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=494
Thu Apr 21 08:03:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 494,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 148,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.356 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.207 Time taken: 578.9 secs

    Epoch: 1 Validation Loss 0.09 Time taken: 22.36 secs| Accuracy 97.457

Epoch: 2 Train Loss 0.069 Time taken: 577.61 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 22.35 secs| Accuracy 98.05

Epoch: 3 Train Loss 0.051 Time taken: 578.64 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 22.37 secs| Accuracy 98.347

Epoch: 4 Train Loss 0.036 Time taken: 572.72 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 22.5 secs| Accuracy 98.841

Epoch: 5 Train Loss 0.028 Time taken: 572.72 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 22.39 secs| Accuracy 99.025
T\P      0       1
0       5273        0036        
1       0061        5247        

Accuracy 99.086
Time taken: 33.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.604 Time taken: 383.48 secs

    Epoch: 6 Validation Loss 0.416 Time taken: 1.57 secs| Accuracy 81.457

Epoch: 7 Train Loss 0.379 Time taken: 378.05 secs

    Epoch: 7 Validation Loss 0.342 Time taken: 1.46 secs| Accuracy 83.444
T\P      0       1
0       0165        0033        
1       0041        0215        

Accuracy 83.7
Time taken: 1.98 secs
3845.969 secs

Thu Apr 21 09:08:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=495
Thu Apr 21 09:08:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.058 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 495,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 149,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.361 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.211 Time taken: 580.71 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 22.46 secs| Accuracy 97.499

Epoch: 2 Train Loss 0.081 Time taken: 582.0 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 22.43 secs| Accuracy 98.276

Epoch: 3 Train Loss 0.062 Time taken: 575.97 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 22.55 secs| Accuracy 98.531

Epoch: 4 Train Loss 0.044 Time taken: 575.56 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 22.56 secs| Accuracy 98.714

Epoch: 5 Train Loss 0.036 Time taken: 575.08 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 22.44 secs| Accuracy 98.912
T\P      0       1
0       5262        0047        
1       0055        5253        

Accuracy 99.039
Time taken: 33.24 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.551 Time taken: 377.89 secs

    Epoch: 6 Validation Loss 0.394 Time taken: 1.45 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.391 Time taken: 376.04 secs

    Epoch: 7 Validation Loss 0.321 Time taken: 1.45 secs| Accuracy 88.411
T\P      0       1
0       0157        0041        
1       0034        0222        

Accuracy 83.48
Time taken: 1.96 secs
3841.153 secs

Thu Apr 21 10:13:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=496
Thu Apr 21 10:13:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.999 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 496,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 140,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.36 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 595.68 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 22.32 secs| Accuracy 98.375

Epoch: 2 Train Loss 0.066 Time taken: 597.9 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 22.25 secs| Accuracy 98.771

Epoch: 3 Train Loss 0.042 Time taken: 595.99 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 22.35 secs| Accuracy 98.841

Epoch: 4 Train Loss 0.029 Time taken: 591.8 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 22.39 secs| Accuracy 98.856

Epoch: 5 Train Loss 0.024 Time taken: 590.49 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 22.4 secs| Accuracy 98.94
T\P      0       1
0       5255        0054        
1       0038        5270        

Accuracy 99.133
Time taken: 33.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.731 Time taken: 379.78 secs

    Epoch: 6 Validation Loss 0.521 Time taken: 1.47 secs| Accuracy 80.795

Epoch: 7 Train Loss 0.416 Time taken: 382.78 secs

    Epoch: 7 Validation Loss 0.402 Time taken: 1.44 secs| Accuracy 80.795
T\P      0       1
0       0185        0013        
1       0083        0173        

Accuracy 78.855
Time taken: 1.92 secs
3943.09 secs

Thu Apr 21 11:20:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=497
Thu Apr 21 11:20:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.925 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 497,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 141,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.358 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.167 Time taken: 597.7 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 22.45 secs| Accuracy 98.389

Epoch: 2 Train Loss 0.067 Time taken: 593.57 secs

    Epoch: 2 Validation Loss 0.047 Time taken: 22.52 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.046 Time taken: 597.28 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.4 secs| Accuracy 98.46

Epoch: 4 Train Loss 0.035 Time taken: 602.12 secs

    Epoch: 4 Validation Loss 0.044 Time taken: 22.37 secs| Accuracy 98.771

Epoch: 5 Train Loss 0.027 Time taken: 593.47 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 22.71 secs| Accuracy 99.039
T\P      0       1
0       5259        0050        
1       0050        5258        

Accuracy 99.058
Time taken: 33.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.584 Time taken: 387.88 secs

    Epoch: 6 Validation Loss 0.387 Time taken: 1.51 secs| Accuracy 81.457

Epoch: 7 Train Loss 0.397 Time taken: 379.99 secs

    Epoch: 7 Validation Loss 0.357 Time taken: 1.48 secs| Accuracy 84.768
T\P      0       1
0       0187        0011        
1       0073        0183        

Accuracy 81.498
Time taken: 2.01 secs
3953.153 secs

Thu Apr 21 12:27:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=522
Thu Apr 21 15:14:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.585 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 522,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 8.606 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.182 Time taken: 577.07 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 22.28 secs| Accuracy 98.474

Epoch: 2 Train Loss 0.059 Time taken: 576.75 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 22.24 secs| Accuracy 98.856

Epoch: 3 Train Loss 0.042 Time taken: 572.97 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 22.41 secs| Accuracy 98.757
T\P      0       1
0       5262        0047        
1       0052        5256        

Accuracy 99.068
Time taken: 33.17 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.547 Time taken: 385.84 secs

    Epoch: 4 Validation Loss 0.383 Time taken: 1.48 secs| Accuracy 83.444

Epoch: 5 Train Loss 0.312 Time taken: 383.61 secs

    Epoch: 5 Validation Loss 0.347 Time taken: 1.41 secs| Accuracy 84.768
T\P      0       1
0       0152        0046        
1       0037        0219        

Accuracy 81.718
Time taken: 1.87 secs
2625.101 secs

Thu Apr 21 15:59:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=523
Thu Apr 21 15:59:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 7.498 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 523,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 8.096 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.167 Time taken: 581.64 secs

    Epoch: 1 Validation Loss 0.05 Time taken: 22.83 secs| Accuracy 98.545

Epoch: 2 Train Loss 0.062 Time taken: 771.35 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 24.36 secs| Accuracy 98.827

Epoch: 3 Train Loss 0.046 Time taken: 668.28 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 22.69 secs| Accuracy 98.644
T\P      0       1
0       5241        0068        
1       0048        5260        

Accuracy 98.907
Time taken: 33.65 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.736 Time taken: 384.33 secs

    Epoch: 4 Validation Loss 0.562 Time taken: 1.49 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.413 Time taken: 394.22 secs

    Epoch: 5 Validation Loss 0.335 Time taken: 2.11 secs| Accuracy 85.762
T\P      0       1
0       0151        0047        
1       0024        0232        

Accuracy 84.361
Time taken: 2.57 secs
2933.741 secs

Thu Apr 21 16:49:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=524
Thu Apr 21 16:49:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 10.961 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 524,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.685 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.176 Time taken: 691.68 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 23.64 secs| Accuracy 98.389

Epoch: 2 Train Loss 0.068 Time taken: 693.61 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 24.62 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.046 Time taken: 765.52 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 24.82 secs| Accuracy 97.782
T\P      0       1
0       5144        0165        
1       0033        5275        

Accuracy 98.135
Time taken: 36.96 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.58 Time taken: 525.0 secs

    Epoch: 4 Validation Loss 0.501 Time taken: 2.06 secs| Accuracy 79.139

Epoch: 5 Train Loss 0.361 Time taken: 517.07 secs

    Epoch: 5 Validation Loss 0.479 Time taken: 2.2 secs| Accuracy 78.146
T\P      0       1
0       0128        0070        
1       0021        0235        

Accuracy 79.956
Time taken: 1.99 secs
3334.568 secs

Thu Apr 21 17:45:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=525
Thu Apr 21 17:46:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.063 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 525,
    "message": "516 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.331 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.163 Time taken: 671.51 secs

    Epoch: 1 Validation Loss 0.045 Time taken: 23.21 secs| Accuracy 98.559

Epoch: 2 Train Loss 0.062 Time taken: 635.21 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.01 secs| Accuracy 98.46

Epoch: 3 Train Loss 0.043 Time taken: 582.8 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 22.57 secs| Accuracy 98.87
T\P      0       1
0       5262        0047        
1       0053        5255        

Accuracy 99.058
Time taken: 33.66 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.626 Time taken: 391.54 secs

    Epoch: 4 Validation Loss 0.525 Time taken: 1.63 secs| Accuracy 78.477

Epoch: 5 Train Loss 0.434 Time taken: 384.1 secs

    Epoch: 5 Validation Loss 0.412 Time taken: 1.54 secs| Accuracy 82.45
T\P      0       1
0       0189        0009        
1       0089        0167        

Accuracy 78.414
Time taken: 1.98 secs
2798.749 secs

Thu Apr 21 18:33:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=526
Thu Apr 21 18:33:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.045 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 526,
    "message": "rand sents off",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 121,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.367 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.593 Time taken: 636.06 secs

    Epoch: 1 Validation Loss 0.463 Time taken: 24.32 secs| Accuracy 72.718

Epoch: 2 Train Loss 0.286 Time taken: 766.64 secs

    Epoch: 2 Validation Loss 0.18 Time taken: 25.0 secs| Accuracy 92.879

Epoch: 3 Train Loss 0.155 Time taken: 829.67 secs

    Epoch: 3 Validation Loss 0.134 Time taken: 26.92 secs| Accuracy 94.942
T\P      0       1
0       5112        0197        
1       0324        4984        

Accuracy 95.093
Time taken: 39.01 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.715 Time taken: 558.64 secs

    Epoch: 4 Validation Loss 0.557 Time taken: 1.98 secs| Accuracy 68.874

Epoch: 5 Train Loss 0.487 Time taken: 551.54 secs

    Epoch: 5 Validation Loss 0.436 Time taken: 1.89 secs| Accuracy 81.457
T\P      0       1
0       0169        0029        
1       0073        0183        

Accuracy 77.533
Time taken: 2.7 secs
3492.301 secs

Thu Apr 21 19:32:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=527
Thu Apr 21 19:33:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.109 GB
35 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 527,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 122,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.389 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.444 Time taken: 715.89 secs

    Epoch: 1 Validation Loss 0.201 Time taken: 23.92 secs| Accuracy 92.102

Epoch: 2 Train Loss 0.166 Time taken: 800.39 secs

    Epoch: 2 Validation Loss 0.132 Time taken: 25.1 secs| Accuracy 94.999

Epoch: 3 Train Loss 0.107 Time taken: 588.31 secs

    Epoch: 3 Validation Loss 0.117 Time taken: 22.62 secs| Accuracy 95.535
T\P      0       1
0       5119        0190        
1       0258        5050        

Accuracy 95.78
Time taken: 33.54 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.729 Time taken: 386.0 secs

    Epoch: 4 Validation Loss 0.532 Time taken: 1.49 secs| Accuracy 70.53

Epoch: 5 Train Loss 0.447 Time taken: 380.75 secs

    Epoch: 5 Validation Loss 0.446 Time taken: 1.45 secs| Accuracy 81.457
T\P      0       1
0       0135        0063        
1       0040        0216        

Accuracy 77.313
Time taken: 1.89 secs
3006.659 secs

Thu Apr 21 20:24:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=528
Thu Apr 21 20:24:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 528,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 123,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.355 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.577 Time taken: 578.11 secs

    Epoch: 1 Validation Loss 0.343 Time taken: 22.31 secs| Accuracy 87.313

Epoch: 2 Train Loss 0.247 Time taken: 584.43 secs

    Epoch: 2 Validation Loss 0.18 Time taken: 22.68 secs| Accuracy 93.6

Epoch: 3 Train Loss 0.148 Time taken: 581.63 secs

    Epoch: 3 Validation Loss 0.129 Time taken: 23.08 secs| Accuracy 94.928
T\P      0       1
0       5110        0199        
1       0314        4994        

Accuracy 95.168
Time taken: 34.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.81 Time taken: 498.05 secs

    Epoch: 4 Validation Loss 0.643 Time taken: 2.03 secs| Accuracy 63.245

Epoch: 5 Train Loss 0.557 Time taken: 482.27 secs

    Epoch: 5 Validation Loss 0.487 Time taken: 1.76 secs| Accuracy 77.483
T\P      0       1
0       0141        0057        
1       0054        0202        

Accuracy 75.551
Time taken: 2.29 secs
2858.311 secs

Thu Apr 21 21:13:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=542
Wed Apr 27 10:41:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.045 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 542,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.381 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.283 Time taken: 588.3 secs

    Epoch: 1 Validation Loss 0.087 Time taken: 22.64 secs| Accuracy 96.736

Epoch: 2 Train Loss 0.093 Time taken: 594.25 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.73 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.079 Time taken: 589.63 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 22.68 secs| Accuracy 98.262
T\P      0       1
0       5221        0088        
1       0062        5246        

Accuracy 98.587
Time taken: 33.56 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.661 Time taken: 375.33 secs

    Epoch: 4 Validation Loss 0.579 Time taken: 1.83 secs| Accuracy 68.212

Epoch: 5 Train Loss 0.536 Time taken: 389.31 secs

    Epoch: 5 Validation Loss 0.485 Time taken: 2.12 secs| Accuracy 77.152

Epoch: 6 Train Loss 0.431 Time taken: 483.84 secs

    Epoch: 6 Validation Loss 0.369 Time taken: 2.48 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.36 Time taken: 391.24 secs

    Epoch: 7 Validation Loss 0.358 Time taken: 1.94 secs| Accuracy 85.762

Epoch: 8 Train Loss 0.26 Time taken: 416.7 secs

    Epoch: 8 Validation Loss 0.402 Time taken: 2.38 secs| Accuracy 85.762

Epoch: 9 Train Loss 0.202 Time taken: 416.86 secs

    Epoch: 9 Validation Loss 0.298 Time taken: 2.47 secs| Accuracy 86.424
T\P      0       1
0       0151        0047        
1       0014        0242        

Accuracy 86.564
Time taken: 2.91 secs
4386.942 secs



======================================================================
----------------------------------------------------------------------
                run_ID=543
Wed Apr 27 13:32:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.104 GB
38 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 543,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.389 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.236 Time taken: 694.33 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 24.56 secs| Accuracy 98.163

Epoch: 2 Train Loss 0.072 Time taken: 678.3 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.86 secs| Accuracy 98.432

Epoch: 3 Train Loss 0.05 Time taken: 678.22 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 24.37 secs| Accuracy 98.983
T\P      0       1
0       5264        0045        
1       0070        5238        

Accuracy 98.917
Time taken: 36.01 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.558 Time taken: 458.03 secs

    Epoch: 4 Validation Loss 0.503 Time taken: 2.45 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.396 Time taken: 460.03 secs

    Epoch: 5 Validation Loss 0.429 Time taken: 1.9 secs| Accuracy 82.781

Epoch: 6 Train Loss 0.238 Time taken: 456.65 secs

    Epoch: 6 Validation Loss 0.434 Time taken: 2.3 secs| Accuracy 84.106

Epoch: 7 Train Loss 0.177 Time taken: 459.75 secs

    Epoch: 7 Validation Loss 0.391 Time taken: 2.14 secs| Accuracy 83.113

Epoch: 8 Train Loss 0.156 Time taken: 459.0 secs

    Epoch: 8 Validation Loss 0.521 Time taken: 2.48 secs| Accuracy 85.43

Epoch: 9 Train Loss 0.1 Time taken: 449.8 secs

    Epoch: 9 Validation Loss 0.453 Time taken: 2.46 secs| Accuracy 83.444
T\P      0       1
0       0172        0026        
1       0037        0219        

Accuracy 86.123
Time taken: 2.63 secs
4949.761 secs

Wed Apr 27 14:56:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=544
Wed Apr 27 14:56:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.183 GB
36 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 544,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.517 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.273 Time taken: 682.36 secs

    Epoch: 1 Validation Loss 0.087 Time taken: 23.58 secs| Accuracy 97.471

Epoch: 2 Train Loss 0.087 Time taken: 680.26 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.87 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.063 Time taken: 681.37 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 24.1 secs| Accuracy 98.644
T\P      0       1
0       5240        0069        
1       0056        5252        

Accuracy 98.823
Time taken: 35.5 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.532 Time taken: 460.56 secs

    Epoch: 4 Validation Loss 0.373 Time taken: 1.94 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.349 Time taken: 462.49 secs

    Epoch: 5 Validation Loss 0.327 Time taken: 2.41 secs| Accuracy 83.444

Epoch: 6 Train Loss 0.244 Time taken: 460.29 secs

    Epoch: 6 Validation Loss 0.262 Time taken: 1.9 secs| Accuracy 88.411

Epoch: 7 Train Loss 0.187 Time taken: 457.82 secs

    Epoch: 7 Validation Loss 0.29 Time taken: 2.1 secs| Accuracy 87.417

Epoch: 8 Train Loss 0.146 Time taken: 459.73 secs

    Epoch: 8 Validation Loss 0.321 Time taken: 2.25 secs| Accuracy 89.735

Epoch: 9 Train Loss 0.128 Time taken: 459.27 secs

    Epoch: 9 Validation Loss 0.247 Time taken: 1.92 secs| Accuracy 90.066
T\P      0       1
0       0174        0024        
1       0026        0230        

Accuracy 88.987
Time taken: 2.77 secs
4959.678 secs

Wed Apr 27 16:20:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=545
Wed Apr 27 16:20:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.349 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 545,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.35 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.312 Time taken: 681.72 secs

    Epoch: 1 Validation Loss 0.113 Time taken: 24.13 secs| Accuracy 97.005

Epoch: 2 Train Loss 0.134 Time taken: 678.96 secs

    Epoch: 2 Validation Loss 0.069 Time taken: 23.91 secs| Accuracy 97.853

Epoch: 3 Train Loss 0.111 Time taken: 682.38 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 23.88 secs| Accuracy 98.389
T\P      0       1
0       5257        0052        
1       0103        5205        

Accuracy 98.54
Time taken: 35.0 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.637 Time taken: 456.41 secs

    Epoch: 4 Validation Loss 0.519 Time taken: 2.62 secs| Accuracy 77.815

Epoch: 5 Train Loss 0.479 Time taken: 464.64 secs

    Epoch: 5 Validation Loss 0.42 Time taken: 2.34 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.386 Time taken: 460.26 secs

    Epoch: 6 Validation Loss 0.327 Time taken: 2.52 secs| Accuracy 84.437

Epoch: 7 Train Loss 0.32 Time taken: 460.11 secs

    Epoch: 7 Validation Loss 0.275 Time taken: 2.42 secs| Accuracy 87.086

Epoch: 8 Train Loss 0.254 Time taken: 459.18 secs

    Epoch: 8 Validation Loss 0.327 Time taken: 1.86 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.269 Time taken: 453.86 secs

    Epoch: 9 Validation Loss 0.314 Time taken: 1.99 secs| Accuracy 87.748
T\P      0       1
0       0180        0018        
1       0048        0208        

Accuracy 85.463
Time taken: 2.75 secs
4947.656 secs

Wed Apr 27 17:44:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=546
Wed Apr 27 17:44:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.968 GB
31 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 546,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.299 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.253 Time taken: 671.36 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 24.03 secs| Accuracy 97.711

Epoch: 2 Train Loss 0.093 Time taken: 681.75 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.52 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.058 Time taken: 678.63 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.08 secs| Accuracy 98.827
T\P      0       1
0       5277        0032        
1       0084        5224        

Accuracy 98.907
Time taken: 35.52 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.645 Time taken: 461.95 secs

    Epoch: 4 Validation Loss 0.584 Time taken: 2.26 secs| Accuracy 67.881

Epoch: 5 Train Loss 0.466 Time taken: 453.87 secs

    Epoch: 5 Validation Loss 0.418 Time taken: 2.17 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.325 Time taken: 452.36 secs

    Epoch: 6 Validation Loss 0.306 Time taken: 2.23 secs| Accuracy 87.748

Epoch: 7 Train Loss 0.23 Time taken: 454.92 secs

    Epoch: 7 Validation Loss 0.29 Time taken: 2.44 secs| Accuracy 88.079

Epoch: 8 Train Loss 0.193 Time taken: 455.66 secs

    Epoch: 8 Validation Loss 0.303 Time taken: 2.82 secs| Accuracy 84.768

Epoch: 9 Train Loss 0.178 Time taken: 457.38 secs

    Epoch: 9 Validation Loss 0.329 Time taken: 2.06 secs| Accuracy 88.742
T\P      0       1
0       0180        0018        
1       0029        0227        

Accuracy 89.648
Time taken: 2.24 secs
4914.954 secs

Wed Apr 27 19:07:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=547
Wed Apr 27 19:07:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.314 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 547,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.629 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.224 Time taken: 654.99 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 23.96 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.078 Time taken: 658.56 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 23.81 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.052 Time taken: 650.6 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.83 secs| Accuracy 98.728
T\P      0       1
0       5244        0065        
1       0058        5250        

Accuracy 98.841
Time taken: 35.54 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.692 Time taken: 451.89 secs

    Epoch: 4 Validation Loss 0.475 Time taken: 2.28 secs| Accuracy 83.444
T\P      0       1
0       0158        0040        
1       0051        0205        

Accuracy 79.956
Time taken: 2.72 secs
2555.509 secs

Wed Apr 27 19:51:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=548
Wed Apr 27 19:51:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.292 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 548,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.149 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.258 Time taken: 661.44 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 24.21 secs| Accuracy 98.418

Epoch: 2 Train Loss 0.074 Time taken: 652.76 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 24.22 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.048 Time taken: 656.78 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.8 secs| Accuracy 98.432
T\P      0       1
0       5206        0103        
1       0039        5269        

Accuracy 98.663
Time taken: 35.02 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.623 Time taken: 450.75 secs

    Epoch: 4 Validation Loss 0.446 Time taken: 2.49 secs| Accuracy 80.795
T\P      0       1
0       0149        0049        
1       0050        0206        

Accuracy 78.194
Time taken: 2.63 secs
2558.899 secs

Wed Apr 27 20:35:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=549
Wed Apr 27 20:35:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.215 GB
35 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 549,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.476 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.232 Time taken: 657.85 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 24.1 secs| Accuracy 98.446

Epoch: 2 Train Loss 0.068 Time taken: 661.97 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.7 secs| Accuracy 98.7

Epoch: 3 Train Loss 0.044 Time taken: 657.54 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.14 secs| Accuracy 98.813
T\P      0       1
0       5250        0059        
1       0040        5268        

Accuracy 99.068
Time taken: 34.9 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.637 Time taken: 457.01 secs

    Epoch: 4 Validation Loss 0.442 Time taken: 2.34 secs| Accuracy 80.464
T\P      0       1
0       0145        0053        
1       0040        0216        

Accuracy 79.515
Time taken: 2.81 secs
2573.451 secs

Wed Apr 27 21:19:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=550
Wed Apr 27 21:19:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.057 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 550,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.937 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 668.22 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.85 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.057 Time taken: 674.23 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 24.63 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.034 Time taken: 677.26 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 23.92 secs| Accuracy 98.163
T\P      0       1
0       5224        0085        
1       0060        5248        

Accuracy 98.634
Time taken: 35.74 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.586 Time taken: 469.88 secs

    Epoch: 4 Validation Loss 0.35 Time taken: 2.01 secs| Accuracy 82.45
T\P      0       1
0       0145        0053        
1       0016        0240        

Accuracy 84.802
Time taken: 2.33 secs
2624.222 secs

Wed Apr 27 22:04:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=551
Wed Apr 27 22:04:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.97 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 551,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.036 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 673.51 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 24.45 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 668.58 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 23.86 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.034 Time taken: 670.5 secs

    Epoch: 3 Validation Loss 0.07 Time taken: 23.61 secs| Accuracy 98.036
T\P      0       1
0       5284        0025        
1       0154        5154        

Accuracy 98.314
Time taken: 36.43 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.496 Time taken: 469.8 secs

    Epoch: 4 Validation Loss 0.276 Time taken: 2.49 secs| Accuracy 86.755
T\P      0       1
0       0154        0044        
1       0022        0234        

Accuracy 85.463
Time taken: 2.76 secs
2617.661 secs

Wed Apr 27 22:48:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=552
Wed Apr 27 22:48:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.644 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 552,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 6.449 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 656.31 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 24.1 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.06 Time taken: 656.46 secs

    Epoch: 2 Validation Loss 0.068 Time taken: 23.65 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.032 Time taken: 655.61 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 23.6 secs| Accuracy 98.333
T\P      0       1
0       5251        0058        
1       0073        5235        

Accuracy 98.766
Time taken: 35.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.549 Time taken: 452.14 secs

    Epoch: 4 Validation Loss 0.325 Time taken: 1.94 secs| Accuracy 84.768
T\P      0       1
0       0162        0036        
1       0024        0232        

Accuracy 86.784
Time taken: 2.42 secs
2553.513 secs

Wed Apr 27 23:32:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=553
Wed Apr 27 23:32:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.944 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 553,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.562 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.302 Time taken: 658.34 secs

    Epoch: 1 Validation Loss 0.149 Time taken: 24.66 secs| Accuracy 97.146

Epoch: 2 Train Loss 0.126 Time taken: 660.26 secs

    Epoch: 2 Validation Loss 0.094 Time taken: 23.85 secs| Accuracy 97.909

Epoch: 3 Train Loss 0.089 Time taken: 656.86 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 23.69 secs| Accuracy 98.319
T\P      0       1
0       5262        0047        
1       0097        5211        

Accuracy 98.644
Time taken: 35.62 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.643 Time taken: 453.45 secs

    Epoch: 4 Validation Loss 0.523 Time taken: 2.27 secs| Accuracy 73.179
T\P      0       1
0       0183        0015        
1       0118        0138        

Accuracy 70.705
Time taken: 2.28 secs
2566.709 secs

Thu Apr 28 00:16:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=554
Thu Apr 28 00:16:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.212 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 554,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.35 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.293 Time taken: 651.67 secs

    Epoch: 1 Validation Loss 0.078 Time taken: 23.84 secs| Accuracy 97.796

Epoch: 2 Train Loss 0.091 Time taken: 655.36 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.52 secs| Accuracy 98.474

Epoch: 3 Train Loss 0.06 Time taken: 660.02 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 23.48 secs| Accuracy 98.517
T\P      0       1
0       5249        0060        
1       0056        5252        

Accuracy 98.907
Time taken: 34.96 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.528 Time taken: 450.69 secs

    Epoch: 4 Validation Loss 0.394 Time taken: 2.47 secs| Accuracy 80.132
T\P      0       1
0       0178        0020        
1       0085        0171        

Accuracy 76.872
Time taken: 2.49 secs
2552.584 secs

Thu Apr 28 01:00:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=555
Thu Apr 28 01:00:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.939 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 555,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.537 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.244 Time taken: 652.59 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.98 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.072 Time taken: 651.87 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.41 secs| Accuracy 98.672

Epoch: 3 Train Loss 0.051 Time taken: 660.99 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.39 secs| Accuracy 98.771
T\P      0       1
0       5281        0028        
1       0080        5228        

Accuracy 98.983
Time taken: 35.24 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.586 Time taken: 455.71 secs

    Epoch: 4 Validation Loss 0.408 Time taken: 2.23 secs| Accuracy 83.113
T\P      0       1
0       0157        0041        
1       0054        0202        

Accuracy 79.075
Time taken: 2.91 secs
2557.278 secs

Thu Apr 28 01:44:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=556
Thu Apr 28 01:44:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.051 GB
31 seconds taken
Example: ... tensor(-1.)tensor(-1.)tensor(1.)tensor(-1.)tensor(1.)tensor(1.)...


{
    "run_ID": 556,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): Tanh()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): Tanh()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.505 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.103 Time taken: 655.83 secs

    Epoch: 1 Validation Loss 0.046 Time taken: 24.16 secs| Accuracy 97.118

Epoch: 2 Train Loss 0.039 Time taken: 659.56 secs

    Epoch: 2 Validation Loss 0.019 Time taken: 24.0 secs| Accuracy 97.881

Epoch: 3 Train Loss 0.035 Time taken: 662.6 secs

    Epoch: 3 Validation Loss 0.023 Time taken: 23.82 secs| Accuracy 97.98
T\P      0       1
0       5286        0023        
1       0176        5132        

Accuracy 98.126
Time taken: 35.04 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.24 Time taken: 456.64 secs

    Epoch: 4 Validation Loss 0.185 Time taken: 2.17 secs| Accuracy 66.556
T\P      0       1
0       0051        0147        
1       0011        0245        

Accuracy 65.198
Time taken: 2.72 secs
2571.524 secs

Thu Apr 28 02:28:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=557
Thu Apr 28 02:28:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.137 GB
32 seconds taken
Example: ... tensor(-1.)tensor(-1.)tensor(1.)tensor(-1.)tensor(1.)tensor(-1.)...


{
    "run_ID": 557,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): Tanh()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): Tanh()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.352 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.107 Time taken: 658.25 secs

    Epoch: 1 Validation Loss 0.028 Time taken: 23.77 secs| Accuracy 97.146

Epoch: 2 Train Loss 0.038 Time taken: 653.95 secs

    Epoch: 2 Validation Loss 0.019 Time taken: 24.21 secs| Accuracy 98.008

Epoch: 3 Train Loss 0.03 Time taken: 654.76 secs

    Epoch: 3 Validation Loss 0.024 Time taken: 24.01 secs| Accuracy 97.259
T\P      0       1
0       5075        0234        
1       0045        5263        

Accuracy 97.372
Time taken: 34.87 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.241 Time taken: 451.56 secs

    Epoch: 4 Validation Loss 0.204 Time taken: 1.9 secs| Accuracy 81.788
T\P      0       1
0       0147        0051        
1       0047        0209        

Accuracy 78.414
Time taken: 2.54 secs
2555.034 secs

Thu Apr 28 03:11:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=558
Thu Apr 28 03:11:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.111 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(-1.)tensor(-1.)tensor(1.)...


{
    "run_ID": 558,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.0,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "HingeEmbeddingLoss",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): Tanh()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): Tanh()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Tanh()
  (criterion): HingeEmbeddingLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.25 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.091 Time taken: 656.96 secs

    Epoch: 1 Validation Loss 0.027 Time taken: 23.43 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.043 Time taken: 658.45 secs

    Epoch: 2 Validation Loss 0.023 Time taken: 23.85 secs| Accuracy 97.711

Epoch: 3 Train Loss 0.039 Time taken: 655.47 secs

    Epoch: 3 Validation Loss 0.022 Time taken: 24.48 secs| Accuracy 97.796
T\P      0       1
0       5190        0119        
1       0086        5222        

Accuracy 98.069
Time taken: 35.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.279 Time taken: 459.23 secs

    Epoch: 4 Validation Loss 0.308 Time taken: 2.06 secs| Accuracy 77.152
T\P      0       1
0       0108        0090        
1       0045        0211        

Accuracy 70.264
Time taken: 2.34 secs
2566.174 secs

Thu Apr 28 03:55:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=559
Thu Apr 28 11:58:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.555 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 559,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.347 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.237 Time taken: 684.72 secs

    Epoch: 1 Validation Loss 0.091 Time taken: 24.74 secs| Accuracy 97.372

Epoch: 2 Train Loss 0.069 Time taken: 690.18 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 24.53 secs| Accuracy 98.827

Epoch: 3 Train Loss 0.046 Time taken: 686.9 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 24.91 secs| Accuracy 98.531
T\P      0       1
0       5204        0105        
1       0041        5267        

Accuracy 98.625
Time taken: 36.27 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.596 Time taken: 481.86 secs

    Epoch: 4 Validation Loss 0.471 Time taken: 2.25 secs| Accuracy 80.132

Epoch: 5 Train Loss 0.369 Time taken: 481.09 secs

    Epoch: 5 Validation Loss 0.449 Time taken: 2.34 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.299 Time taken: 497.7 secs

    Epoch: 6 Validation Loss 0.363 Time taken: 2.32 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.238 Time taken: 516.64 secs

    Epoch: 7 Validation Loss 0.331 Time taken: 2.52 secs| Accuracy 84.437

Epoch: 8 Train Loss 0.19 Time taken: 525.01 secs

    Epoch: 8 Validation Loss 0.367 Time taken: 2.08 secs| Accuracy 86.093

Epoch: 9 Train Loss 0.18 Time taken: 518.65 secs

    Epoch: 9 Validation Loss 0.395 Time taken: 2.81 secs| Accuracy 85.43
T\P      0       1
0       0184        0014        
1       0043        0213        

Accuracy 87.445
Time taken: 3.03 secs
5240.552 secs

Thu Apr 28 13:26:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=560
Thu Apr 28 15:09:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.936 GB
36 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 560,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.466 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.238 Time taken: 676.77 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 24.86 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.073 Time taken: 680.5 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 24.53 secs| Accuracy 98.432

Epoch: 3 Train Loss 0.049 Time taken: 684.85 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 24.17 secs| Accuracy 98.587
T\P      0       1
0       5205        0104        
1       0040        5268        

Accuracy 98.644
Time taken: 35.85 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.666 Time taken: 470.42 secs

    Epoch: 4 Validation Loss 0.475 Time taken: 2.01 secs| Accuracy 81.457

Epoch: 5 Train Loss 0.441 Time taken: 472.91 secs

    Epoch: 5 Validation Loss 0.426 Time taken: 2.41 secs| Accuracy 82.45

Epoch: 6 Train Loss 0.359 Time taken: 474.83 secs

    Epoch: 6 Validation Loss 0.368 Time taken: 2.04 secs| Accuracy 85.099

Epoch: 7 Train Loss 0.263 Time taken: 474.14 secs

    Epoch: 7 Validation Loss 0.341 Time taken: 2.7 secs| Accuracy 86.093

Epoch: 8 Train Loss 0.192 Time taken: 469.16 secs

    Epoch: 8 Validation Loss 0.337 Time taken: 1.75 secs| Accuracy 86.755

Epoch: 9 Train Loss 0.155 Time taken: 468.61 secs

    Epoch: 9 Validation Loss 0.339 Time taken: 2.13 secs| Accuracy 87.417
T\P      0       1
0       0184        0014        
1       0035        0221        

Accuracy 89.207
Time taken: 2.3 secs
5032.355 secs

Thu Apr 28 16:34:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=561
Thu Apr 28 16:35:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.957 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 561,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.6 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.227 Time taken: 691.4 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 24.75 secs| Accuracy 98.46

Epoch: 2 Train Loss 0.065 Time taken: 691.65 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 24.08 secs| Accuracy 98.63

Epoch: 3 Train Loss 0.042 Time taken: 698.32 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 24.65 secs| Accuracy 98.799
T\P      0       1
0       5232        0077        
1       0032        5276        

Accuracy 98.973
Time taken: 36.28 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.661 Time taken: 479.07 secs

    Epoch: 4 Validation Loss 0.554 Time taken: 1.91 secs| Accuracy 79.801

Epoch: 5 Train Loss 0.48 Time taken: 475.95 secs

    Epoch: 5 Validation Loss 0.523 Time taken: 2.37 secs| Accuracy 79.801

Epoch: 6 Train Loss 0.35 Time taken: 482.93 secs

    Epoch: 6 Validation Loss 0.304 Time taken: 2.67 secs| Accuracy 85.43

Epoch: 7 Train Loss 0.237 Time taken: 478.97 secs

    Epoch: 7 Validation Loss 0.329 Time taken: 1.78 secs| Accuracy 86.755

Epoch: 8 Train Loss 0.161 Time taken: 481.41 secs

    Epoch: 8 Validation Loss 0.409 Time taken: 2.52 secs| Accuracy 85.43

Epoch: 9 Train Loss 0.145 Time taken: 481.41 secs

    Epoch: 9 Validation Loss 0.316 Time taken: 2.29 secs| Accuracy 87.086
T\P      0       1
0       0169        0029        
1       0039        0217        

Accuracy 85.022
Time taken: 3.23 secs
5113.172 secs

Thu Apr 28 18:01:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=562
Thu Apr 28 18:01:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.942 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 562,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.375 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.245 Time taken: 674.25 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 24.72 secs| Accuracy 98.305

Epoch: 2 Train Loss 0.076 Time taken: 673.47 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 25.09 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.053 Time taken: 679.55 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 25.23 secs| Accuracy 98.884
T\P      0       1
0       5269        0040        
1       0076        5232        

Accuracy 98.907
Time taken: 36.26 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.574 Time taken: 475.76 secs

    Epoch: 4 Validation Loss 0.42 Time taken: 2.86 secs| Accuracy 83.775

Epoch: 5 Train Loss 0.346 Time taken: 474.83 secs

    Epoch: 5 Validation Loss 0.387 Time taken: 2.09 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.232 Time taken: 473.21 secs

    Epoch: 6 Validation Loss 0.419 Time taken: 2.19 secs| Accuracy 83.775

Epoch: 7 Train Loss 0.166 Time taken: 468.28 secs

    Epoch: 7 Validation Loss 0.358 Time taken: 2.27 secs| Accuracy 85.762

Epoch: 8 Train Loss 0.145 Time taken: 469.05 secs

    Epoch: 8 Validation Loss 0.412 Time taken: 1.92 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.122 Time taken: 471.72 secs

    Epoch: 9 Validation Loss 0.39 Time taken: 1.68 secs| Accuracy 85.43
T\P      0       1
0       0180        0018        
1       0038        0218        

Accuracy 87.665
Time taken: 2.67 secs
5017.409 secs

Thu Apr 28 19:26:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=563
Thu Apr 28 19:26:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.06 GB
35 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 563,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.317 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.232 Time taken: 696.44 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 24.53 secs| Accuracy 98.192

Epoch: 2 Train Loss 0.076 Time taken: 691.15 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 24.28 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.052 Time taken: 696.66 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 24.66 secs| Accuracy 98.785
T\P      0       1
0       5252        0057        
1       0063        5245        

Accuracy 98.87
Time taken: 37.1 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.656 Time taken: 480.43 secs

    Epoch: 4 Validation Loss 0.471 Time taken: 2.04 secs| Accuracy 80.132

Epoch: 5 Train Loss 0.418 Time taken: 485.15 secs

    Epoch: 5 Validation Loss 0.337 Time taken: 2.87 secs| Accuracy 85.762

Epoch: 6 Train Loss 0.348 Time taken: 492.68 secs

    Epoch: 6 Validation Loss 0.298 Time taken: 2.36 secs| Accuracy 88.411

Epoch: 7 Train Loss 0.253 Time taken: 499.47 secs

    Epoch: 7 Validation Loss 0.248 Time taken: 2.29 secs| Accuracy 87.748

Epoch: 8 Train Loss 0.182 Time taken: 489.0 secs

    Epoch: 8 Validation Loss 0.295 Time taken: 2.13 secs| Accuracy 89.735

Epoch: 9 Train Loss 0.17 Time taken: 496.52 secs

    Epoch: 9 Validation Loss 0.242 Time taken: 2.76 secs| Accuracy 89.404
T\P      0       1
0       0187        0011        
1       0044        0212        

Accuracy 87.885
Time taken: 2.96 secs
5184.114 secs

Thu Apr 28 20:53:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=564
Thu Apr 28 20:54:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.984 GB
37 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 564,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.462 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.227 Time taken: 698.87 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 24.24 secs| Accuracy 97.853

Epoch: 2 Train Loss 0.071 Time taken: 698.37 secs

    Epoch: 2 Validation Loss 0.04 Time taken: 24.51 secs| Accuracy 98.743

Epoch: 3 Train Loss 0.045 Time taken: 704.11 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 25.0 secs| Accuracy 98.827
T\P      0       1
0       5244        0065        
1       0057        5251        

Accuracy 98.851
Time taken: 37.29 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.65 Time taken: 505.42 secs

    Epoch: 4 Validation Loss 0.409 Time taken: 2.23 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.407 Time taken: 497.31 secs

    Epoch: 5 Validation Loss 0.339 Time taken: 2.05 secs| Accuracy 85.099

Epoch: 6 Train Loss 0.282 Time taken: 497.68 secs

    Epoch: 6 Validation Loss 0.263 Time taken: 2.19 secs| Accuracy 86.093

Epoch: 7 Train Loss 0.21 Time taken: 493.05 secs

    Epoch: 7 Validation Loss 0.312 Time taken: 1.81 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.19 Time taken: 495.33 secs

    Epoch: 8 Validation Loss 0.265 Time taken: 1.88 secs| Accuracy 89.404

Epoch: 9 Train Loss 0.161 Time taken: 493.7 secs

    Epoch: 9 Validation Loss 0.324 Time taken: 2.33 secs| Accuracy 86.093
T\P      0       1
0       0191        0007        
1       0056        0200        

Accuracy 86.123
Time taken: 2.56 secs
5235.278 secs

Thu Apr 28 22:22:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=565
Thu Apr 28 22:22:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.941 GB
36 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 565,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.87 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.251 Time taken: 706.65 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 25.69 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.087 Time taken: 701.11 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 24.9 secs| Accuracy 98.474

Epoch: 3 Train Loss 0.063 Time taken: 714.41 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 24.8 secs| Accuracy 98.827
T\P      0       1
0       5288        0021        
1       0111        5197        

Accuracy 98.757
Time taken: 36.73 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.683 Time taken: 493.94 secs

    Epoch: 4 Validation Loss 0.548 Time taken: 2.31 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.448 Time taken: 489.03 secs

    Epoch: 5 Validation Loss 0.42 Time taken: 1.75 secs| Accuracy 81.457

Epoch: 6 Train Loss 0.328 Time taken: 492.37 secs

    Epoch: 6 Validation Loss 0.315 Time taken: 2.21 secs| Accuracy 85.762

Epoch: 7 Train Loss 0.247 Time taken: 498.34 secs

    Epoch: 7 Validation Loss 0.289 Time taken: 2.16 secs| Accuracy 87.417

Epoch: 8 Train Loss 0.203 Time taken: 496.74 secs

    Epoch: 8 Validation Loss 0.436 Time taken: 1.94 secs| Accuracy 82.119

Epoch: 9 Train Loss 0.194 Time taken: 489.15 secs

    Epoch: 9 Validation Loss 0.33 Time taken: 2.23 secs| Accuracy 89.073
T\P      0       1
0       0176        0022        
1       0027        0229        

Accuracy 89.207
Time taken: 2.68 secs
5234.384 secs

Thu Apr 28 23:51:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=576
Fri Apr 29 10:04:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.653 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 576,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 6.414 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 688.22 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 24.87 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 691.14 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 24.4 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.033 Time taken: 690.15 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 24.55 secs| Accuracy 98.29
T\P      0       1
0       5240        0069        
1       0078        5230        

Accuracy 98.615
Time taken: 36.02 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.516 Time taken: 491.92 secs

    Epoch: 4 Validation Loss 0.367 Time taken: 2.55 secs| Accuracy 82.45
T\P      0       1
0       0178        0020        
1       0050        0206        

Accuracy 84.581
Time taken: 2.91 secs
2699.696 secs

Fri Apr 29 10:50:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=577
Fri Apr 29 10:50:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.995 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 577,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 0.234 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 713.15 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 24.81 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.056 Time taken: 699.41 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 24.04 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.032 Time taken: 704.88 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 24.64 secs| Accuracy 98.474
T\P      0       1
0       5227        0082        
1       0068        5240        

Accuracy 98.587
Time taken: 35.85 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.495 Time taken: 490.17 secs

    Epoch: 4 Validation Loss 0.341 Time taken: 2.16 secs| Accuracy 86.093
T\P      0       1
0       0160        0038        
1       0024        0232        

Accuracy 86.344
Time taken: 2.55 secs
2744.862 secs

Fri Apr 29 11:37:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=594
Fri Apr 29 17:25:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.189 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 594,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.713 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 590.75 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 22.5 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.058 Time taken: 586.9 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.45 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.035 Time taken: 587.85 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 22.5 secs| Accuracy 98.418
T\P      0       1
0       5245        0064        
1       0080        5228        

Accuracy 98.644
Time taken: 33.41 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.468 Time taken: 382.85 secs

    Epoch: 4 Validation Loss 0.34 Time taken: 1.89 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.256 Time taken: 380.07 secs

    Epoch: 5 Validation Loss 0.278 Time taken: 1.65 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.194 Time taken: 377.43 secs

    Epoch: 6 Validation Loss 0.277 Time taken: 1.61 secs| Accuracy 85.762

Epoch: 7 Train Loss 0.143 Time taken: 377.91 secs

    Epoch: 7 Validation Loss 0.299 Time taken: 1.63 secs| Accuracy 87.086

Epoch: 8 Train Loss 0.107 Time taken: 377.84 secs

    Epoch: 8 Validation Loss 0.329 Time taken: 1.68 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.049 Time taken: 377.61 secs

    Epoch: 9 Validation Loss 0.432 Time taken: 1.58 secs| Accuracy 86.755
T\P      0       1
0       0173        0025        
1       0028        0228        

Accuracy 88.326
Time taken: 1.99 secs
4174.327 secs

Fri Apr 29 18:36:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=595
Fri Apr 29 18:36:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.869 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 595,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.99 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.187 Time taken: 591.14 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.71 secs| Accuracy 98.234

Epoch: 2 Train Loss 0.054 Time taken: 589.7 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.74 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.028 Time taken: 590.2 secs

    Epoch: 3 Validation Loss 0.077 Time taken: 22.96 secs| Accuracy 98.079
T\P      0       1
0       5236        0073        
1       0081        5227        

Accuracy 98.549
Time taken: 33.77 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.642 Time taken: 379.61 secs

    Epoch: 4 Validation Loss 0.322 Time taken: 1.87 secs| Accuracy 83.444

Epoch: 5 Train Loss 0.25 Time taken: 379.62 secs

    Epoch: 5 Validation Loss 0.293 Time taken: 1.9 secs| Accuracy 88.079

Epoch: 6 Train Loss 0.179 Time taken: 379.38 secs

    Epoch: 6 Validation Loss 0.299 Time taken: 1.88 secs| Accuracy 89.073

Epoch: 7 Train Loss 0.101 Time taken: 378.66 secs

    Epoch: 7 Validation Loss 0.462 Time taken: 2.08 secs| Accuracy 87.417

Epoch: 8 Train Loss 0.056 Time taken: 380.4 secs

    Epoch: 8 Validation Loss 0.355 Time taken: 2.06 secs| Accuracy 88.411

Epoch: 9 Train Loss 0.042 Time taken: 379.71 secs

    Epoch: 9 Validation Loss 0.4 Time taken: 1.95 secs| Accuracy 87.748
T\P      0       1
0       0165        0033        
1       0022        0234        

Accuracy 87.885
Time taken: 2.42 secs
4187.672 secs

Fri Apr 29 19:47:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=596
Fri Apr 29 19:47:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.78 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 596,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.904 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.206 Time taken: 588.87 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 22.64 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.061 Time taken: 587.11 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.64 secs| Accuracy 98.29

Epoch: 3 Train Loss 0.035 Time taken: 588.24 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 22.74 secs| Accuracy 98.46
T\P      0       1
0       5267        0042        
1       0100        5208        

Accuracy 98.663
Time taken: 33.63 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.475 Time taken: 379.29 secs

    Epoch: 4 Validation Loss 0.401 Time taken: 1.62 secs| Accuracy 84.768

Epoch: 5 Train Loss 0.224 Time taken: 377.1 secs

    Epoch: 5 Validation Loss 0.352 Time taken: 1.72 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.16 Time taken: 377.52 secs

    Epoch: 6 Validation Loss 0.44 Time taken: 1.67 secs| Accuracy 83.775

Epoch: 7 Train Loss 0.091 Time taken: 377.38 secs

    Epoch: 7 Validation Loss 0.598 Time taken: 1.64 secs| Accuracy 82.119

Epoch: 8 Train Loss 0.072 Time taken: 377.14 secs

    Epoch: 8 Validation Loss 0.59 Time taken: 1.7 secs| Accuracy 83.775

Epoch: 9 Train Loss 0.05 Time taken: 376.96 secs

    Epoch: 9 Validation Loss 0.527 Time taken: 1.62 secs| Accuracy 85.43
T\P      0       1
0       0179        0019        
1       0029        0227        

Accuracy 89.427
Time taken: 2.08 secs
4164.909 secs

Fri Apr 29 20:57:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=597
Fri Apr 29 20:57:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.863 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 597,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.989 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.196 Time taken: 588.66 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.73 secs| Accuracy 97.994

Epoch: 2 Train Loss 0.062 Time taken: 587.18 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 22.7 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.036 Time taken: 588.14 secs

    Epoch: 3 Validation Loss 0.072 Time taken: 22.78 secs| Accuracy 97.782
T\P      0       1
0       5154        0155        
1       0051        5257        

Accuracy 98.06
Time taken: 33.59 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.554 Time taken: 379.62 secs

    Epoch: 4 Validation Loss 0.319 Time taken: 1.68 secs| Accuracy 85.099

Epoch: 5 Train Loss 0.243 Time taken: 377.71 secs

    Epoch: 5 Validation Loss 0.292 Time taken: 1.85 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.184 Time taken: 377.97 secs

    Epoch: 6 Validation Loss 0.283 Time taken: 1.72 secs| Accuracy 86.424

Epoch: 7 Train Loss 0.13 Time taken: 378.17 secs

    Epoch: 7 Validation Loss 0.34 Time taken: 1.71 secs| Accuracy 86.755

Epoch: 8 Train Loss 0.08 Time taken: 378.28 secs

    Epoch: 8 Validation Loss 0.392 Time taken: 1.72 secs| Accuracy 88.079

Epoch: 9 Train Loss 0.035 Time taken: 377.23 secs

    Epoch: 9 Validation Loss 0.501 Time taken: 1.74 secs| Accuracy 86.424
T\P      0       1
0       0183        0015        
1       0041        0215        

Accuracy 87.665
Time taken: 2.17 secs
4170.548 secs

Fri Apr 29 22:07:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=598
Fri Apr 29 22:08:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 598,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.985 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.19 Time taken: 590.72 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.88 secs| Accuracy 97.754

Epoch: 2 Train Loss 0.054 Time taken: 589.33 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.91 secs| Accuracy 98.149

Epoch: 3 Train Loss 0.028 Time taken: 588.91 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 22.83 secs| Accuracy 98.375
T\P      0       1
0       5229        0080        
1       0062        5246        

Accuracy 98.663
Time taken: 33.59 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.629 Time taken: 379.58 secs

    Epoch: 4 Validation Loss 0.357 Time taken: 1.66 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.286 Time taken: 378.87 secs

    Epoch: 5 Validation Loss 0.312 Time taken: 1.65 secs| Accuracy 84.768

Epoch: 6 Train Loss 0.229 Time taken: 378.27 secs

    Epoch: 6 Validation Loss 0.293 Time taken: 1.68 secs| Accuracy 85.762

Epoch: 7 Train Loss 0.171 Time taken: 378.78 secs

    Epoch: 7 Validation Loss 0.34 Time taken: 1.66 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.104 Time taken: 378.11 secs

    Epoch: 8 Validation Loss 0.403 Time taken: 1.71 secs| Accuracy 86.093

Epoch: 9 Train Loss 0.064 Time taken: 378.12 secs

    Epoch: 9 Validation Loss 0.457 Time taken: 1.65 secs| Accuracy 86.424
T\P      0       1
0       0175        0023        
1       0038        0218        

Accuracy 86.564
Time taken: 2.12 secs
4176.656 secs

Fri Apr 29 23:18:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=599
Fri Apr 29 23:18:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.948 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 599,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.008 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.19 Time taken: 590.19 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.69 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.06 Time taken: 588.21 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.7 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.036 Time taken: 589.67 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 22.72 secs| Accuracy 98.545
T\P      0       1
0       5257        0052        
1       0086        5222        

Accuracy 98.7
Time taken: 33.79 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.467 Time taken: 378.73 secs

    Epoch: 4 Validation Loss 0.305 Time taken: 1.63 secs| Accuracy 84.768

Epoch: 5 Train Loss 0.203 Time taken: 378.14 secs

    Epoch: 5 Validation Loss 0.331 Time taken: 1.62 secs| Accuracy 85.43

Epoch: 6 Train Loss 0.125 Time taken: 378.02 secs

    Epoch: 6 Validation Loss 0.391 Time taken: 1.68 secs| Accuracy 88.742

Epoch: 7 Train Loss 0.072 Time taken: 378.28 secs

    Epoch: 7 Validation Loss 0.364 Time taken: 1.65 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.022 Time taken: 377.92 secs

    Epoch: 8 Validation Loss 0.521 Time taken: 1.58 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.023 Time taken: 377.49 secs

    Epoch: 9 Validation Loss 0.483 Time taken: 1.57 secs| Accuracy 87.748
T\P      0       1
0       0177        0021        
1       0027        0229        

Accuracy 89.427
Time taken: 2.04 secs
4173.8 secs

Sat Apr 30 00:29:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=600
Sat Apr 30 00:29:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.838 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 600,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.01 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 575.86 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.7 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.056 Time taken: 575.09 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 22.72 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.031 Time taken: 575.94 secs

    Epoch: 3 Validation Loss 0.054 Time taken: 22.79 secs| Accuracy 98.517
T\P      0       1
0       5247        0062        
1       0069        5239        

Accuracy 98.766
Time taken: 33.74 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.651 Time taken: 378.44 secs

    Epoch: 4 Validation Loss 0.335 Time taken: 1.62 secs| Accuracy 84.106

Epoch: 5 Train Loss 0.266 Time taken: 377.9 secs

    Epoch: 5 Validation Loss 0.281 Time taken: 1.67 secs| Accuracy 87.086

Epoch: 6 Train Loss 0.222 Time taken: 376.94 secs

    Epoch: 6 Validation Loss 0.28 Time taken: 1.65 secs| Accuracy 88.079

Epoch: 7 Train Loss 0.162 Time taken: 376.99 secs

    Epoch: 7 Validation Loss 0.267 Time taken: 1.64 secs| Accuracy 89.073

Epoch: 8 Train Loss 0.105 Time taken: 376.57 secs

    Epoch: 8 Validation Loss 0.289 Time taken: 1.6 secs| Accuracy 89.404

Epoch: 9 Train Loss 0.058 Time taken: 376.6 secs

    Epoch: 9 Validation Loss 0.402 Time taken: 1.64 secs| Accuracy 88.079
T\P      0       1
0       0182        0016        
1       0037        0219        

Accuracy 88.326
Time taken: 2.04 secs
4126.429 secs

Sat Apr 30 01:38:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=601
Sat Apr 30 01:38:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.751 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 601,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.993 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.181 Time taken: 575.53 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 22.76 secs| Accuracy 98.177

Epoch: 2 Train Loss 0.059 Time taken: 574.94 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 22.87 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.034 Time taken: 575.17 secs

    Epoch: 3 Validation Loss 0.07 Time taken: 22.86 secs| Accuracy 98.276
T\P      0       1
0       5281        0028        
1       0141        5167        

Accuracy 98.408
Time taken: 33.99 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.497 Time taken: 379.44 secs

    Epoch: 4 Validation Loss 0.278 Time taken: 1.73 secs| Accuracy 89.073

Epoch: 5 Train Loss 0.198 Time taken: 377.5 secs

    Epoch: 5 Validation Loss 0.255 Time taken: 1.68 secs| Accuracy 91.06

Epoch: 6 Train Loss 0.129 Time taken: 377.07 secs

    Epoch: 6 Validation Loss 0.313 Time taken: 1.71 secs| Accuracy 86.093

Epoch: 7 Train Loss 0.076 Time taken: 376.91 secs

    Epoch: 7 Validation Loss 0.324 Time taken: 1.69 secs| Accuracy 88.079

Epoch: 8 Train Loss 0.068 Time taken: 376.61 secs

    Epoch: 8 Validation Loss 0.324 Time taken: 1.68 secs| Accuracy 88.742

Epoch: 9 Train Loss 0.047 Time taken: 376.83 secs

    Epoch: 9 Validation Loss 0.332 Time taken: 1.73 secs| Accuracy 89.404
T\P      0       1
0       0171        0027        
1       0024        0232        

Accuracy 88.767
Time taken: 2.14 secs
4126.359 secs

Sat Apr 30 02:48:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=602
Sat Apr 30 02:48:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.951 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 602,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.987 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.179 Time taken: 575.89 secs

    Epoch: 1 Validation Loss 0.081 Time taken: 22.68 secs| Accuracy 97.626

Epoch: 2 Train Loss 0.055 Time taken: 575.27 secs

    Epoch: 2 Validation Loss 0.065 Time taken: 22.86 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.032 Time taken: 575.57 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 22.67 secs| Accuracy 98.404
T\P      0       1
0       5258        0051        
1       0075        5233        

Accuracy 98.813
Time taken: 33.69 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.622 Time taken: 378.55 secs

    Epoch: 4 Validation Loss 0.36 Time taken: 1.67 secs| Accuracy 86.755

Epoch: 5 Train Loss 0.288 Time taken: 377.35 secs

    Epoch: 5 Validation Loss 0.279 Time taken: 1.7 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.22 Time taken: 377.13 secs

    Epoch: 6 Validation Loss 0.286 Time taken: 1.66 secs| Accuracy 87.748

Epoch: 7 Train Loss 0.169 Time taken: 377.13 secs

    Epoch: 7 Validation Loss 0.268 Time taken: 1.65 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.149 Time taken: 376.92 secs

    Epoch: 8 Validation Loss 0.359 Time taken: 1.74 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.084 Time taken: 377.69 secs

    Epoch: 9 Validation Loss 0.336 Time taken: 1.65 secs| Accuracy 89.073
T\P      0       1
0       0171        0027        
1       0028        0228        

Accuracy 87.885
Time taken: 2.06 secs
4128.208 secs

Sat Apr 30 03:58:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=603
Sat Apr 30 03:58:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.863 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 603,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.99 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.176 Time taken: 574.99 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 22.72 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.057 Time taken: 573.65 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 22.58 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.035 Time taken: 574.26 secs

    Epoch: 3 Validation Loss 0.071 Time taken: 22.65 secs| Accuracy 97.895
T\P      0       1
0       5222        0087        
1       0114        5194        

Accuracy 98.107
Time taken: 33.55 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.554 Time taken: 378.08 secs

    Epoch: 4 Validation Loss 0.433 Time taken: 1.68 secs| Accuracy 85.099

Epoch: 5 Train Loss 0.28 Time taken: 377.54 secs

    Epoch: 5 Validation Loss 0.351 Time taken: 1.65 secs| Accuracy 85.762

Epoch: 6 Train Loss 0.212 Time taken: 377.15 secs

    Epoch: 6 Validation Loss 0.417 Time taken: 1.63 secs| Accuracy 86.755

Epoch: 7 Train Loss 0.15 Time taken: 376.45 secs

    Epoch: 7 Validation Loss 0.325 Time taken: 1.66 secs| Accuracy 87.086

Epoch: 8 Train Loss 0.076 Time taken: 375.7 secs

    Epoch: 8 Validation Loss 0.443 Time taken: 1.67 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.064 Time taken: 375.37 secs

    Epoch: 9 Validation Loss 0.462 Time taken: 1.55 secs| Accuracy 87.417
T\P      0       1
0       0174        0024        
1       0034        0222        

Accuracy 87.225
Time taken: 2.0 secs
4117.871 secs

Sat Apr 30 05:07:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=604
Sat Apr 30 05:07:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.873 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 604,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.983 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.181 Time taken: 571.54 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 22.66 secs| Accuracy 97.824

Epoch: 2 Train Loss 0.06 Time taken: 570.15 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 22.65 secs| Accuracy 98.46

Epoch: 3 Train Loss 0.034 Time taken: 572.91 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.8 secs| Accuracy 98.545
T\P      0       1
0       5239        0070        
1       0066        5242        

Accuracy 98.719
Time taken: 33.81 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.517 Time taken: 377.82 secs

    Epoch: 4 Validation Loss 0.377 Time taken: 1.79 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.28 Time taken: 377.0 secs

    Epoch: 5 Validation Loss 0.307 Time taken: 1.73 secs| Accuracy 86.093

Epoch: 6 Train Loss 0.191 Time taken: 376.52 secs

    Epoch: 6 Validation Loss 0.34 Time taken: 1.78 secs| Accuracy 87.086

Epoch: 7 Train Loss 0.151 Time taken: 376.64 secs

    Epoch: 7 Validation Loss 0.346 Time taken: 1.75 secs| Accuracy 84.437

Epoch: 8 Train Loss 0.119 Time taken: 376.64 secs

    Epoch: 8 Validation Loss 0.348 Time taken: 1.73 secs| Accuracy 86.755

Epoch: 9 Train Loss 0.086 Time taken: 376.94 secs

    Epoch: 9 Validation Loss 0.465 Time taken: 1.74 secs| Accuracy 86.424
T\P      0       1
0       0185        0013        
1       0058        0198        

Accuracy 84.361
Time taken: 2.27 secs
4112.333 secs

Sat Apr 30 06:17:24 2022


======================================================================
----------------------------------------------------------------------
                run_ID=605
Sat Apr 30 06:17:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.857 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 605,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.987 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.175 Time taken: 573.38 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 22.85 secs| Accuracy 98.234

Epoch: 2 Train Loss 0.052 Time taken: 572.17 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.91 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.029 Time taken: 573.97 secs

    Epoch: 3 Validation Loss 0.091 Time taken: 22.93 secs| Accuracy 97.471
T\P      0       1
0       5126        0183        
1       0037        5271        

Accuracy 97.928
Time taken: 33.93 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.581 Time taken: 377.31 secs

    Epoch: 4 Validation Loss 0.306 Time taken: 1.84 secs| Accuracy 87.086

Epoch: 5 Train Loss 0.249 Time taken: 377.01 secs

    Epoch: 5 Validation Loss 0.255 Time taken: 1.87 secs| Accuracy 87.748

Epoch: 6 Train Loss 0.177 Time taken: 377.02 secs

    Epoch: 6 Validation Loss 0.27 Time taken: 1.86 secs| Accuracy 88.742

Epoch: 7 Train Loss 0.111 Time taken: 377.0 secs

    Epoch: 7 Validation Loss 0.322 Time taken: 1.85 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.08 Time taken: 375.72 secs

    Epoch: 8 Validation Loss 0.353 Time taken: 1.82 secs| Accuracy 89.404

Epoch: 9 Train Loss 0.053 Time taken: 375.26 secs

    Epoch: 9 Validation Loss 0.443 Time taken: 1.83 secs| Accuracy 88.411
T\P      0       1
0       0184        0014        
1       0050        0206        

Accuracy 85.903
Time taken: 2.28 secs
4117.515 secs

Sat Apr 30 07:26:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=606
Sat Apr 30 07:27:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.861 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 606,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.005 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 570.85 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 22.62 secs| Accuracy 98.262

Epoch: 2 Train Loss 0.06 Time taken: 570.69 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.75 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.036 Time taken: 570.25 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 22.66 secs| Accuracy 98.559
T\P      0       1
0       5229        0080        
1       0059        5249        

Accuracy 98.691
Time taken: 33.65 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.57 Time taken: 376.75 secs

    Epoch: 4 Validation Loss 0.397 Time taken: 1.64 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.269 Time taken: 375.59 secs

    Epoch: 5 Validation Loss 0.353 Time taken: 1.77 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.179 Time taken: 376.14 secs

    Epoch: 6 Validation Loss 0.423 Time taken: 1.7 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.102 Time taken: 375.68 secs

    Epoch: 7 Validation Loss 0.462 Time taken: 1.81 secs| Accuracy 84.437

Epoch: 8 Train Loss 0.055 Time taken: 376.07 secs

    Epoch: 8 Validation Loss 0.54 Time taken: 1.72 secs| Accuracy 85.43


======================================================================
----------------------------------------------------------------------
                run_ID=626
Sat Apr 30 08:39:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.742 GB
21 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 626,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "only asha data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM -0.458 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=632
Sat Apr 30 08:43:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.053 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 632,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.005 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 560.98 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.26 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 561.99 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.33 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.033 Time taken: 562.31 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.23 secs| Accuracy 98.29

Epoch: 4 Train Loss 0.02 Time taken: 562.13 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 22.3 secs| Accuracy 98.276

Epoch: 5 Train Loss 0.014 Time taken: 563.07 secs

    Epoch: 5 Validation Loss 0.075 Time taken: 22.26 secs| Accuracy 98.276
T\P      0       1
0       5251        0058        
1       0086        5222        

Accuracy 98.644
Time taken: 33.13 secs
2955.234 secs

Sat Apr 30 09:34:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=633
Sat Apr 30 09:34:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.858 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 633,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.088 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 563.87 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.34 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.056 Time taken: 563.21 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.16 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.032 Time taken: 561.8 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.23 secs| Accuracy 98.474

Epoch: 4 Train Loss 0.02 Time taken: 562.08 secs

    Epoch: 4 Validation Loss 0.056 Time taken: 22.29 secs| Accuracy 98.46

Epoch: 5 Train Loss 0.015 Time taken: 561.95 secs

    Epoch: 5 Validation Loss 0.058 Time taken: 22.29 secs| Accuracy 98.262
T\P      0       1
0       5227        0082        
1       0064        5244        

Accuracy 98.625
Time taken: 33.16 secs
2957.766 secs

Sat Apr 30 10:24:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=634
Sat Apr 30 10:24:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.857 GB
26 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 634,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.087 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 562.11 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.42 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.052 Time taken: 562.91 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.4 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.031 Time taken: 562.78 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 22.48 secs| Accuracy 98.446

Epoch: 4 Train Loss 0.02 Time taken: 563.02 secs

    Epoch: 4 Validation Loss 0.057 Time taken: 22.44 secs| Accuracy 98.446

Epoch: 5 Train Loss 0.012 Time taken: 561.12 secs

    Epoch: 5 Validation Loss 0.06 Time taken: 22.68 secs| Accuracy 98.517
T\P      0       1
0       5222        0087        
1       0058        5250        

Accuracy 98.634
Time taken: 33.52 secs
2958.25 secs

Sat Apr 30 11:14:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=636
Sat Apr 30 11:14:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.955 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 636,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.993 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 578.2 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.39 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.057 Time taken: 577.67 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 22.42 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.031 Time taken: 578.56 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 22.47 secs| Accuracy 98.389

Epoch: 4 Train Loss 0.02 Time taken: 578.57 secs

    Epoch: 4 Validation Loss 0.075 Time taken: 22.47 secs| Accuracy 98.079

Epoch: 5 Train Loss 0.013 Time taken: 578.9 secs

    Epoch: 5 Validation Loss 0.07 Time taken: 22.45 secs| Accuracy 98.375
T\P      0       1
0       5276        0033        
1       0113        5195        

Accuracy 98.625
Time taken: 33.19 secs
3037.561 secs

Sat Apr 30 12:06:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=637
Sat Apr 30 12:06:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.872 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 637,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.977 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 578.3 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.38 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.06 Time taken: 581.16 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.47 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.034 Time taken: 581.64 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.54 secs| Accuracy 98.418

Epoch: 4 Train Loss 0.017 Time taken: 581.61 secs

    Epoch: 4 Validation Loss 0.074 Time taken: 22.59 secs| Accuracy 98.05

Epoch: 5 Train Loss 0.014 Time taken: 582.3 secs

    Epoch: 5 Validation Loss 0.072 Time taken: 22.74 secs| Accuracy 98.234
T\P      0       1
0       5231        0078        
1       0051        5257        

Accuracy 98.785
Time taken: 33.57 secs
3051.689 secs

Sat Apr 30 12:57:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=638
Sat Apr 30 12:58:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.031 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 638,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.034 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 706.99 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 23.47 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.058 Time taken: 598.5 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.42 secs| Accuracy 98.206


======================================================================
----------------------------------------------------------------------
                run_ID=606
Sat Apr 30 13:28:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.861 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 606,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.988 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 3 Train Loss 0.035 Time taken: 580.02 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 22.33 secs| Accuracy 98.418

Epoch: 1 Train Loss 0.193 Time taken: 580.11 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 23.86 secs| Accuracy 98.262

Epoch: 4 Train Loss 0.021 Time taken: 607.85 secs

    Epoch: 4 Validation Loss 0.057 Time taken: 23.95 secs| Accuracy 98.502

Epoch: 2 Train Loss 0.06 Time taken: 698.52 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.63 secs| Accuracy 98.714

Epoch: 5 Train Loss 0.013 Time taken: 657.85 secs

    Epoch: 5 Validation Loss 0.062 Time taken: 22.53 secs| Accuracy 98.404
T\P      0       1
0       5244        0065        
1       0069        5239        

Accuracy 98.738
Time taken: 33.04 secs
3299.228 secs

Sat Apr 30 13:53:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=639
Sat Apr 30 13:53:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.869 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 639,
    "message": "450",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "only inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.983 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 3 Train Loss 0.036 Time taken: 577.39 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 22.77 secs| Accuracy 98.559
T\P      0       1
0       5229        0080        
1       0059        5249        

Accuracy 98.691
Time taken: 33.6 secs

Epoch: 1 Train Loss 0.187 Time taken: 578.45 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.28 secs| Accuracy 98.234

Epoch: 2 Train Loss 0.054 Time taken: 577.24 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.29 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.028 Time taken: 577.31 secs

    Epoch: 3 Validation Loss 0.077 Time taken: 22.31 secs| Accuracy 98.079

Epoch: 4 Train Loss 0.021 Time taken: 577.6 secs

    Epoch: 4 Validation Loss 0.052 Time taken: 22.35 secs| Accuracy 98.418

Epoch: 5 Train Loss 0.012 Time taken: 577.68 secs

    Epoch: 5 Validation Loss 0.061 Time taken: 22.33 secs| Accuracy 98.517
T\P      0       1
0       5213        0096        
1       0060        5248        

Accuracy 98.531
Time taken: 33.09 secs
3033.268 secs



======================================================================
----------------------------------------------------------------------
                run_ID=606
Sat Apr 30 22:34:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.856 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 606,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.991 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 573.24 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 22.75 secs| Accuracy 98.262

Epoch: 2 Train Loss 0.06 Time taken: 614.7 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 24.15 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.036 Time taken: 633.86 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 23.55 secs| Accuracy 98.559
T\P      0       1
0       5229        0080        
1       0059        5249        

Accuracy 98.691
Time taken: 34.77 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.57 Time taken: 399.33 secs

    Epoch: 4 Validation Loss 0.397 Time taken: 1.54 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.269 Time taken: 376.37 secs

    Epoch: 5 Validation Loss 0.353 Time taken: 1.51 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.179 Time taken: 483.85 secs

    Epoch: 6 Validation Loss 0.423 Time taken: 1.89 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.102 Time taken: 476.19 secs

    Epoch: 7 Validation Loss 0.462 Time taken: 1.72 secs| Accuracy 84.437

Epoch: 8 Train Loss 0.055 Time taken: 450.37 secs

    Epoch: 8 Validation Loss 0.54 Time taken: 1.79 secs| Accuracy 85.43

Epoch: 9 Train Loss 0.066 Time taken: 431.71 secs

    Epoch: 9 Validation Loss 0.595 Time taken: 1.72 secs| Accuracy 83.113
T\P      0       1
0       0187        0011        
1       0054        0202        

Accuracy 85.683
Time taken: 2.23 secs
4580.583 secs

Sat Apr 30 23:51:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=607
Sat Apr 30 23:51:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.847 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 607,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.997 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.177 Time taken: 594.91 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 22.71 secs| Accuracy 98.206

Epoch: 2 Train Loss 0.057 Time taken: 573.92 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 22.38 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.035 Time taken: 574.17 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 22.5 secs| Accuracy 98.601
T\P      0       1
0       5265        0044        
1       0070        5238        

Accuracy 98.926
Time taken: 33.46 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.489 Time taken: 378.72 secs

    Epoch: 4 Validation Loss 0.331 Time taken: 1.48 secs| Accuracy 84.768

Epoch: 5 Train Loss 0.234 Time taken: 377.29 secs

    Epoch: 5 Validation Loss 0.31 Time taken: 1.49 secs| Accuracy 87.748

Epoch: 6 Train Loss 0.152 Time taken: 377.16 secs

    Epoch: 6 Validation Loss 0.258 Time taken: 1.48 secs| Accuracy 89.404

Epoch: 7 Train Loss 0.116 Time taken: 376.82 secs

    Epoch: 7 Validation Loss 0.274 Time taken: 1.48 secs| Accuracy 90.728

Epoch: 8 Train Loss 0.066 Time taken: 376.81 secs

    Epoch: 8 Validation Loss 0.312 Time taken: 1.46 secs| Accuracy 89.735

Epoch: 9 Train Loss 0.04 Time taken: 376.78 secs

    Epoch: 9 Validation Loss 0.436 Time taken: 1.44 secs| Accuracy 88.411
T\P      0       1
0       0192        0006        
1       0055        0201        

Accuracy 86.564
Time taken: 1.88 secs
4141.397 secs

Sun May  1 01:01:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=665
Sun May  1 09:49:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 11.921 GB
73 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 665,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-tlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-tlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 5.166 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.705 Time taken: 743.89 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 31.45 secs| Accuracy 52.458

Epoch: 2 Train Loss 0.693 Time taken: 747.83 secs

    Epoch: 2 Validation Loss 0.686 Time taken: 31.6 secs| Accuracy 53.758

Epoch: 3 Train Loss 0.686 Time taken: 749.99 secs

    Epoch: 3 Validation Loss 0.675 Time taken: 31.48 secs| Accuracy 57.997
T\P      0       1
0       2886        2423        
1       2137        3171        

Accuracy 57.05
Time taken: 47.09 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.714 Time taken: 586.69 secs

    Epoch: 4 Validation Loss 0.696 Time taken: 1.66 secs| Accuracy 50.331
T\P      0       1
0       0056        0142        
1       0075        0181        

Accuracy 52.203
Time taken: 2.35 secs
2999.612 secs

Sun May  1 10:40:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=666
Sun May  1 10:40:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.562 GB
79 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 666,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-tlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-tlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 4.966 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.705 Time taken: 745.89 secs

    Epoch: 1 Validation Loss 0.686 Time taken: 31.49 secs| Accuracy 54.267

Epoch: 2 Train Loss 0.694 Time taken: 744.88 secs

    Epoch: 2 Validation Loss 0.685 Time taken: 31.53 secs| Accuracy 54.691

Epoch: 3 Train Loss 0.689 Time taken: 779.04 secs

    Epoch: 3 Validation Loss 0.675 Time taken: 31.82 secs| Accuracy 59.084
T\P      0       1
0       2935        2374        
1       2035        3273        

Accuracy 58.472
Time taken: 47.38 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.704 Time taken: 645.58 secs

    Epoch: 4 Validation Loss 0.708 Time taken: 1.73 secs| Accuracy 50.993
T\P      0       1
0       0065        0133        
1       0087        0169        

Accuracy 51.542
Time taken: 2.37 secs
3088.647 secs

Sun May  1 11:34:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=667
Sun May  1 11:34:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.445 GB
80 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 667,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-tlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-tlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 4.926 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.705 Time taken: 809.1 secs

    Epoch: 1 Validation Loss 0.689 Time taken: 31.98 secs| Accuracy 53.391

Epoch: 2 Train Loss 0.694 Time taken: 929.49 secs

    Epoch: 2 Validation Loss 0.687 Time taken: 33.23 secs| Accuracy 54.917

Epoch: 3 Train Loss 0.687 Time taken: 923.41 secs

    Epoch: 3 Validation Loss 0.676 Time taken: 32.5 secs| Accuracy 57.982
T\P      0       1
0       3006        2303        
1       2239        3069        

Accuracy 57.22
Time taken: 48.08 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.716 Time taken: 677.86 secs

    Epoch: 4 Validation Loss 0.696 Time taken: 1.82 secs| Accuracy 58.278
T\P      0       1
0       0002        0196        
1       0005        0251        

Accuracy 55.727
Time taken: 2.54 secs
3516.28 secs

Sun May  1 12:34:42 2022


======================================================================
----------------------------------------------------------------------
                run_ID=668
Sun May  1 12:34:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.65 GB
90 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 668,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-tlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-tlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM -1.474 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.702 Time taken: 945.46 secs

    Epoch: 1 Validation Loss 0.68 Time taken: 32.86 secs| Accuracy 56.753

Epoch: 2 Train Loss 0.694 Time taken: 943.55 secs

    Epoch: 2 Validation Loss 0.689 Time taken: 32.57 secs| Accuracy 54.323

Epoch: 3 Train Loss 0.689 Time taken: 908.27 secs

    Epoch: 3 Validation Loss 0.681 Time taken: 32.66 secs| Accuracy 56.216
T\P      0       1
0       1260        4049        
1       0706        4602        

Accuracy 55.213
Time taken: 48.32 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.69 Time taken: 791.07 secs

    Epoch: 4 Validation Loss 0.686 Time taken: 1.95 secs| Accuracy 57.947
T\P      0       1
0       0009        0189        
1       0018        0238        

Accuracy 54.405
Time taken: 2.64 secs
3768.138 secs

Sun May  1 13:39:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=669
Sun May  1 13:39:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.724 GB
105 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 669,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-tlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-tlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 4.997 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.708 Time taken: 958.05 secs

    Epoch: 1 Validation Loss 0.686 Time taken: 32.92 secs| Accuracy 54.648

Epoch: 2 Train Loss 0.693 Time taken: 969.28 secs

    Epoch: 2 Validation Loss 0.681 Time taken: 32.86 secs| Accuracy 56.047

Epoch: 3 Train Loss 0.692 Time taken: 967.04 secs

    Epoch: 3 Validation Loss 0.676 Time taken: 32.75 secs| Accuracy 56.598
T\P      0       1
0       3835        1474        
1       3308        2000        

Accuracy 54.959
Time taken: 48.37 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.7 Time taken: 762.49 secs

    Epoch: 4 Validation Loss 0.667 Time taken: 1.86 secs| Accuracy 57.285
T\P      0       1
0       0007        0191        
1       0009        0247        

Accuracy 55.947
Time taken: 2.5 secs
3836.798 secs

Sun May  1 14:45:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=670
Sun May  1 14:46:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.438 GB
109 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 670,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 5.694 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.697 Time taken: 954.94 secs

    Epoch: 1 Validation Loss 0.707 Time taken: 33.17 secs| Accuracy 50.014

Epoch: 2 Train Loss 0.693 Time taken: 961.25 secs

    Epoch: 2 Validation Loss 0.691 Time taken: 33.09 secs| Accuracy 49.816

Epoch: 3 Train Loss 0.691 Time taken: 939.44 secs

    Epoch: 3 Validation Loss 0.684 Time taken: 32.95 secs| Accuracy 61.5
T\P      0       1
0       1648        3661        
1       0532        4776        

Accuracy 60.507
Time taken: 48.75 secs


======================================================================
----------------------------------------------------------------------
                run_ID=673
Wed May  4 12:00:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 9.027 GB
107 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 673,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 8.068 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.697 Time taken: 979.87 secs

    Epoch: 1 Validation Loss 0.682 Time taken: 33.5 secs| Accuracy 51.766

Epoch: 2 Train Loss 0.692 Time taken: 937.06 secs

    Epoch: 2 Validation Loss 0.671 Time taken: 33.41 secs| Accuracy 58.081

Epoch: 3 Train Loss 0.691 Time taken: 979.01 secs

    Epoch: 3 Validation Loss 0.668 Time taken: 33.46 secs| Accuracy 60.526
T\P      0       1
0       2619        2690        
1       1667        3641        

Accuracy 58.962
Time taken: 49.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.687 Time taken: 820.79 secs

    Epoch: 4 Validation Loss 0.73 Time taken: 1.97 secs| Accuracy 51.987
T\P      0       1
0       0023        0175        
1       0064        0192        

Accuracy 47.357
Time taken: 2.63 secs
3901.182 secs

Wed May  4 13:08:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=674
Wed May  4 13:08:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.399 GB
106 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 674,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-mlm-xnli15-1024",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 1024
}
myBert(
  (model): XLMModel(
 xlm-mlm-xnli15-1024 )
Memory taken on GPU 0.931 GB
Memory taken on RAM 5.615 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.695 Time taken: 979.6 secs

    Epoch: 1 Validation Loss 0.712 Time taken: 33.32 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.692 Time taken: 978.26 secs

    Epoch: 2 Validation Loss 0.68 Time taken: 33.45 secs| Accuracy 52.699

Epoch: 3 Train Loss 0.692 Time taken: 983.75 secs

    Epoch: 3 Validation Loss 0.684 Time taken: 33.45 secs| Accuracy 53.348
T\P      0       1
0       4159        1150        
1       3731        1577        

Accuracy 54.027
Time taken: 49.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.701 Time taken: 824.96 secs

    Epoch: 4 Validation Loss 0.743 Time taken: 1.99 secs| Accuracy 49.007
T\P      0       1
0       0040        0158        
1       0113        0143        

Accuracy 40.308
Time taken: 2.67 secs
3948.398 secs

Wed May  4 14:16:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=675
Wed May  4 14:16:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.069 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 675,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.41 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 657.4 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 23.8 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.06 Time taken: 656.35 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 23.73 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.035 Time taken: 655.76 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 23.71 secs| Accuracy 98.474
T\P      0       1
0       5224        0085        
1       0065        5243        

Accuracy 98.587
Time taken: 35.12 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.684 Time taken: 429.14 secs

    Epoch: 4 Validation Loss 0.313 Time taken: 1.76 secs| Accuracy 87.086
T\P      0       1
0       0176        0022        
1       0042        0214        

Accuracy 85.903
Time taken: 2.22 secs
2530.929 secs

Wed May  4 14:59:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=711
Thu May  5 17:49:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.087 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 711,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 5.197 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 610.71 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.4 secs| Accuracy 98.22

Epoch: 2 Train Loss 0.055 Time taken: 604.79 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 23.62 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.03 Time taken: 604.64 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.91 secs| Accuracy 98.7
T\P      0       1
0       5243        0066        
1       0062        5246        

Accuracy 98.794
Time taken: 33.75 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.615 Time taken: 405.13 secs

    Epoch: 4 Validation Loss 0.338 Time taken: 2.06 secs| Accuracy 86.424
T\P      0       1
0       0169        0029        
1       0032        0224        

Accuracy 86.564
Time taken: 2.53 secs
2355.828 secs

Thu May  5 18:29:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=712
Thu May  5 18:29:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.026 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 712,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 5.002 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.132 Time taken: 602.35 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 23.29 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.055 Time taken: 604.41 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.14 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.035 Time taken: 601.51 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.27 secs| Accuracy 98.672
T\P      0       1
0       5238        0071        
1       0078        5230        

Accuracy 98.597
Time taken: 34.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.548 Time taken: 403.78 secs

    Epoch: 4 Validation Loss 0.414 Time taken: 1.88 secs| Accuracy 84.106
T\P      0       1
0       0167        0031        
1       0041        0215        

Accuracy 84.141
Time taken: 2.28 secs
2343.236 secs

Thu May  5 19:10:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=713
Thu May  5 19:10:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.138 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 713,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.688 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.131 Time taken: 602.73 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 23.52 secs| Accuracy 98.107

Epoch: 2 Train Loss 0.051 Time taken: 598.95 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 23.24 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.027 Time taken: 599.8 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 23.27 secs| Accuracy 98.276
T\P      0       1
0       5208        0101        
1       0064        5244        

Accuracy 98.446
Time taken: 34.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.554 Time taken: 401.81 secs

    Epoch: 4 Validation Loss 0.326 Time taken: 1.89 secs| Accuracy 84.768
T\P      0       1
0       0180        0018        
1       0048        0208        

Accuracy 85.463
Time taken: 2.3 secs
2339.054 secs

Thu May  5 19:50:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=714
Thu May  5 19:50:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.958 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 714,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.891 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.145 Time taken: 737.12 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 24.13 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.053 Time taken: 619.43 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.41 secs| Accuracy 98.531

Epoch: 3 Train Loss 0.031 Time taken: 600.02 secs

    Epoch: 3 Validation Loss 0.079 Time taken: 23.28 secs| Accuracy 97.542
T\P      0       1
0       5120        0189        
1       0035        5273        

Accuracy 97.89
Time taken: 34.46 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.558 Time taken: 403.58 secs

    Epoch: 4 Validation Loss 0.346 Time taken: 1.84 secs| Accuracy 84.106
T\P      0       1
0       0132        0066        
1       0019        0237        

Accuracy 81.278
Time taken: 2.16 secs
2492.171 secs

Thu May  5 20:32:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=715
Thu May  5 20:32:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.844 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 715,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.903 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.126 Time taken: 602.6 secs

    Epoch: 1 Validation Loss 0.103 Time taken: 23.12 secs| Accuracy 95.903

Epoch: 2 Train Loss 0.047 Time taken: 598.53 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 23.16 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.026 Time taken: 602.38 secs

    Epoch: 3 Validation Loss 0.054 Time taken: 23.42 secs| Accuracy 98.404
T\P      0       1
0       5238        0071        
1       0071        5237        

Accuracy 98.663
Time taken: 34.88 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.5 Time taken: 401.21 secs

    Epoch: 4 Validation Loss 0.335 Time taken: 1.7 secs| Accuracy 85.099
T\P      0       1
0       0169        0029        
1       0042        0214        

Accuracy 84.361
Time taken: 2.22 secs
2336.08 secs

Thu May  5 21:12:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=716
Thu May  5 21:12:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.907 GB
42 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 716,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.119 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 517.57 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 23.9 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 514.18 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 23.44 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 512.86 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 23.68 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 34.8 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.693 Time taken: 408.55 secs

    Epoch: 4 Validation Loss 0.692 Time taken: 1.96 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.33 secs
2093.342 secs

Thu May  5 21:49:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=717
Thu May  5 21:49:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.935 GB
38 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 717,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.242 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 515.72 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 23.51 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 515.84 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 23.54 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 519.27 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 23.83 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 34.91 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.692 Time taken: 409.38 secs

    Epoch: 4 Validation Loss 0.691 Time taken: 2.04 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.38 secs
2100.125 secs

Thu May  5 22:25:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=718
Thu May  5 22:25:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.099 GB
39 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 718,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.165 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 516.49 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 23.7 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 518.62 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 23.54 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 519.18 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 23.66 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 35.0 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.693 Time taken: 407.8 secs

    Epoch: 4 Validation Loss 0.692 Time taken: 2.0 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.34 secs
2102.642 secs

Thu May  5 23:01:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=719
Thu May  5 23:01:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
39 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 719,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.113 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 518.67 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 23.85 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 518.22 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 23.85 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 515.15 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 23.51 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 34.93 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.691 Time taken: 410.84 secs

    Epoch: 4 Validation Loss 0.691 Time taken: 1.92 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.35 secs
2103.147 secs

Thu May  5 23:37:51 2022




======================================================================
----------------------------------------------------------------------
                run_ID=720
Thu May  5 23:55:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.14 GB
40 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 720,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.891 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 523.79 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 24.41 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.693 Time taken: 535.66 secs

    Epoch: 2 Validation Loss 0.693 Time taken: 24.08 secs| Accuracy 49.986

Epoch: 3 Train Loss 0.693 Time taken: 527.56 secs

    Epoch: 3 Validation Loss 0.693 Time taken: 24.33 secs| Accuracy 49.986
T\P      0       1
0       5309        0000        
1       5308        0000        

Accuracy 50.005
Time taken: 35.57 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.693 Time taken: 420.98 secs

    Epoch: 4 Validation Loss 0.691 Time taken: 2.32 secs| Accuracy 41.06
T\P      0       1
0       0198        0000        
1       0256        0000        

Accuracy 43.612
Time taken: 2.7 secs
2153.775 secs

Fri May  6 00:32:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=721
Fri May  6 00:32:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.873 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 721,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.081 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.19 Time taken: 614.99 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 23.38 secs| Accuracy 98.036

Epoch: 2 Train Loss 0.055 Time taken: 612.05 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 23.12 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.03 Time taken: 614.69 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 23.32 secs| Accuracy 98.418
T\P      0       1
0       5259        0050        
1       0086        5222        

Accuracy 98.719
Time taken: 34.55 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.607 Time taken: 403.31 secs

    Epoch: 4 Validation Loss 0.306 Time taken: 1.71 secs| Accuracy 85.099
T\P      0       1
0       0150        0048        
1       0020        0236        

Accuracy 85.022
Time taken: 2.15 secs
2375.518 secs

Fri May  6 01:13:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=722
Fri May  6 01:13:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.925 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 722,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.009 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.2 Time taken: 617.3 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 23.19 secs| Accuracy 98.107

Epoch: 2 Train Loss 0.062 Time taken: 615.32 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.41 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.034 Time taken: 614.37 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.17 secs| Accuracy 98.432
T\P      0       1
0       5242        0067        
1       0078        5230        

Accuracy 98.634
Time taken: 34.39 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.474 Time taken: 409.15 secs

    Epoch: 4 Validation Loss 0.414 Time taken: 1.88 secs| Accuracy 83.444
T\P      0       1
0       0164        0034        
1       0023        0233        

Accuracy 87.445
Time taken: 2.45 secs
2387.384 secs

Fri May  6 01:54:28 2022


======================================================================
----------------------------------------------------------------------
                run_ID=723
Fri May  6 01:54:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.845 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 723,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.978 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 613.48 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 23.62 secs| Accuracy 98.036

Epoch: 2 Train Loss 0.066 Time taken: 614.79 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.21 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.035 Time taken: 611.08 secs

    Epoch: 3 Validation Loss 0.08 Time taken: 23.24 secs| Accuracy 97.655
T\P      0       1
0       5104        0205        
1       0038        5270        

Accuracy 97.711
Time taken: 34.4 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.551 Time taken: 402.18 secs

    Epoch: 4 Validation Loss 0.289 Time taken: 1.81 secs| Accuracy 86.755
T\P      0       1
0       0189        0009        
1       0051        0205        

Accuracy 86.784
Time taken: 2.27 secs
2372.686 secs

Fri May  6 02:35:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=724
Fri May  6 02:35:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.94 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 724,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.973 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 608.09 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.71 secs| Accuracy 97.768

Epoch: 2 Train Loss 0.057 Time taken: 608.66 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 23.66 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.03 Time taken: 606.9 secs

    Epoch: 3 Validation Loss 0.074 Time taken: 23.32 secs| Accuracy 98.008
T\P      0       1
0       5193        0116        
1       0051        5257        

Accuracy 98.427
Time taken: 34.54 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.69 Time taken: 403.01 secs

    Epoch: 4 Validation Loss 0.339 Time taken: 2.06 secs| Accuracy 84.768
T\P      0       1
0       0178        0020        
1       0052        0204        

Accuracy 84.141
Time taken: 2.38 secs
2356.927 secs

Fri May  6 03:15:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=725
Fri May  6 03:15:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.893 GB
31 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 725,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.965 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 613.33 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.34 secs| Accuracy 98.22

Epoch: 2 Train Loss 0.059 Time taken: 615.74 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.28 secs| Accuracy 98.474

Epoch: 3 Train Loss 0.033 Time taken: 609.1 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.51 secs| Accuracy 98.531
T\P      0       1
0       5261        0048        
1       0099        5209        

Accuracy 98.615
Time taken: 34.61 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.451 Time taken: 403.29 secs

    Epoch: 4 Validation Loss 0.306 Time taken: 1.93 secs| Accuracy 85.762
T\P      0       1
0       0171        0027        
1       0027        0229        

Accuracy 88.106
Time taken: 2.41 secs
2371.536 secs

Fri May  6 03:56:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=746
Fri May  6 03:56:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.836 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 746,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.932 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 601.57 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.23 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.057 Time taken: 598.23 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.07 secs| Accuracy 98.234
T\P      0       1
0       5252        0057        
1       0098        5210        

Accuracy 98.54
Time taken: 34.29 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.526 Time taken: 402.82 secs

    Epoch: 3 Validation Loss 0.327 Time taken: 1.96 secs| Accuracy 85.43
T\P      0       1
0       0167        0031        
1       0033        0223        

Accuracy 85.903
Time taken: 2.68 secs
1709.456 secs

Fri May  6 04:26:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=747
Fri May  6 04:26:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.786 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 747,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.924 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 604.31 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 23.62 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 603.94 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 23.33 secs| Accuracy 98.333
T\P      0       1
0       5260        0049        
1       0100        5208        

Accuracy 98.597
Time taken: 34.15 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.491 Time taken: 403.91 secs

    Epoch: 3 Validation Loss 0.293 Time taken: 1.81 secs| Accuracy 86.424
T\P      0       1
0       0169        0029        
1       0042        0214        

Accuracy 84.361
Time taken: 2.4 secs
1719.755 secs

Fri May  6 04:56:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=748
Fri May  6 04:56:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.876 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 748,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.938 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 604.96 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 23.43 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.06 Time taken: 600.44 secs

    Epoch: 2 Validation Loss 0.068 Time taken: 23.19 secs| Accuracy 98.22
T\P      0       1
0       5252        0057        
1       0079        5229        

Accuracy 98.719
Time taken: 34.2 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.613 Time taken: 400.34 secs

    Epoch: 3 Validation Loss 0.317 Time taken: 1.86 secs| Accuracy 85.099
T\P      0       1
0       0167        0031        
1       0041        0215        

Accuracy 84.141
Time taken: 2.32 secs
1712.132 secs

Fri May  6 05:25:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=749
Fri May  6 05:25:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.809 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 749,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.158 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 602.97 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.17 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 608.31 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 23.2 secs| Accuracy 98.177
T\P      0       1
0       5272        0037        
1       0115        5193        

Accuracy 98.568
Time taken: 34.16 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.456 Time taken: 408.58 secs

    Epoch: 3 Validation Loss 0.372 Time taken: 2.06 secs| Accuracy 82.119
T\P      0       1
0       0176        0022        
1       0052        0204        

Accuracy 83.7
Time taken: 2.49 secs
1728.759 secs

Fri May  6 05:55:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=750
Fri May  6 05:55:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.929 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 750,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.088 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 609.59 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.35 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.056 Time taken: 609.04 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 23.55 secs| Accuracy 98.206
T\P      0       1
0       5223        0086        
1       0069        5239        

Accuracy 98.54
Time taken: 34.32 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.486 Time taken: 406.27 secs

    Epoch: 3 Validation Loss 0.31 Time taken: 1.96 secs| Accuracy 85.762
T\P      0       1
0       0176        0022        
1       0036        0220        

Accuracy 87.225
Time taken: 2.42 secs
1732.705 secs

Fri May  6 06:25:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=761
Sat May  7 09:36:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.509 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 761,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.421 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 574.7 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.9 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.052 Time taken: 575.57 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.84 secs| Accuracy 98.262
T\P      0       1
0       5235        0074        
1       0088        5220        

Accuracy 98.474
Time taken: 33.49 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.59 Time taken: 380.66 secs

    Epoch: 3 Validation Loss 0.325 Time taken: 1.79 secs| Accuracy 85.43
T\P      0       1
0       0170        0028        
1       0033        0223        

Accuracy 86.564
Time taken: 2.24 secs
1639.987 secs

Sat May  7 10:04:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=762
Sat May  7 10:04:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.865 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 762,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.899 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 576.93 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 22.85 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.061 Time taken: 575.94 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.92 secs| Accuracy 98.347
T\P      0       1
0       5212        0097        
1       0075        5233        

Accuracy 98.38
Time taken: 33.72 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.478 Time taken: 380.49 secs

    Epoch: 3 Validation Loss 0.397 Time taken: 1.76 secs| Accuracy 82.119
T\P      0       1
0       0176        0022        
1       0046        0210        

Accuracy 85.022
Time taken: 2.25 secs
1639.386 secs

Sat May  7 10:32:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=763
Sat May  7 10:32:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.862 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 763,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.642 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 578.94 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.99 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.063 Time taken: 576.55 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.0 secs| Accuracy 98.404
T\P      0       1
0       5245        0064        
1       0089        5219        

Accuracy 98.559
Time taken: 33.82 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.429 Time taken: 380.76 secs

    Epoch: 3 Validation Loss 0.288 Time taken: 1.73 secs| Accuracy 88.742
T\P      0       1
0       0174        0024        
1       0031        0225        

Accuracy 87.885
Time taken: 2.17 secs
1642.831 secs

Sat May  7 11:01:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=764
Sat May  7 11:01:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.852 GB
44 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 764,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.998 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.192 Time taken: 577.08 secs

    Epoch: 1 Validation Loss 0.084 Time taken: 22.72 secs| Accuracy 97.415

Epoch: 2 Train Loss 0.058 Time taken: 570.88 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 22.71 secs| Accuracy 98.502
T\P      0       1
0       5249        0060        
1       0086        5222        

Accuracy 98.625
Time taken: 33.49 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.467 Time taken: 378.38 secs

    Epoch: 3 Validation Loss 0.29 Time taken: 1.72 secs| Accuracy 86.424
T\P      0       1
0       0168        0030        
1       0040        0216        

Accuracy 84.581
Time taken: 2.14 secs
1630.52 secs

Sat May  7 11:29:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=765
Sat May  7 11:29:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.869 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 765,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 2,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.992 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 577.71 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.95 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.06 Time taken: 575.78 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.86 secs| Accuracy 98.248
T\P      0       1
0       5269        0040        
1       0114        5194        

Accuracy 98.549
Time taken: 33.71 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 3 Train Loss 0.449 Time taken: 380.44 secs

    Epoch: 3 Validation Loss 0.351 Time taken: 1.86 secs| Accuracy 84.106
T\P      0       1
0       0140        0058        
1       0011        0245        

Accuracy 84.802
Time taken: 2.24 secs
1641.883 secs

Sat May  7 11:58:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=771
Sun May  8 20:47:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.751 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 771,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.827 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.222 Time taken: 577.46 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 22.77 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.058 Time taken: 578.38 secs

    Epoch: 2 Validation Loss 0.086 Time taken: 22.96 secs| Accuracy 97.344

Epoch: 3 Train Loss 0.029 Time taken: 578.69 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 22.92 secs| Accuracy 98.333
T\P      0       1
0       5242        0067        
1       0079        5229        

Accuracy 98.625
Time taken: 33.87 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.495 Time taken: 383.94 secs

    Epoch: 4 Validation Loss 0.306 Time taken: 1.69 secs| Accuracy 84.768
T\P      0       1
0       0157        0041        
1       0024        0232        

Accuracy 85.683
Time taken: 2.12 secs
2246.579 secs

Sun May  8 21:26:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=772
Sun May  8 21:26:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.199 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 772,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 7.282 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.236 Time taken: 578.0 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.76 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.061 Time taken: 583.27 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 22.79 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.036 Time taken: 581.84 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.77 secs| Accuracy 98.517
T\P      0       1
0       5210        0099        
1       0058        5250        

Accuracy 98.521
Time taken: 33.59 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.545 Time taken: 381.6 secs

    Epoch: 4 Validation Loss 0.314 Time taken: 1.68 secs| Accuracy 85.099
T\P      0       1
0       0150        0048        
1       0020        0236        

Accuracy 85.022
Time taken: 2.16 secs
2252.268 secs

Sun May  8 22:04:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=773
Sun May  8 22:04:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.156 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 773,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.57 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.219 Time taken: 577.63 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 22.76 secs| Accuracy 97.853

Epoch: 2 Train Loss 0.057 Time taken: 575.83 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.65 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.031 Time taken: 576.0 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.71 secs| Accuracy 98.446
T\P      0       1
0       5269        0040        
1       0092        5216        

Accuracy 98.757
Time taken: 33.62 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.521 Time taken: 382.26 secs

    Epoch: 4 Validation Loss 0.338 Time taken: 2.05 secs| Accuracy 81.788
T\P      0       1
0       0143        0055        
1       0014        0242        

Accuracy 84.802
Time taken: 2.67 secs
2240.087 secs

Sun May  8 22:43:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=774
Sun May  8 22:43:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.726 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 774,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.563 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.244 Time taken: 594.75 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.51 secs| Accuracy 97.768

Epoch: 2 Train Loss 0.062 Time taken: 588.56 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.89 secs| Accuracy 98.022

Epoch: 3 Train Loss 0.032 Time taken: 594.04 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 23.33 secs| Accuracy 98.404
T\P      0       1
0       5254        0055        
1       0070        5238        

Accuracy 98.823
Time taken: 34.24 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.577 Time taken: 385.67 secs

    Epoch: 4 Validation Loss 0.383 Time taken: 1.64 secs| Accuracy 79.801
T\P      0       1
0       0114        0084        
1       0002        0254        

Accuracy 81.057
Time taken: 1.98 secs
2293.187 secs

Sun May  8 23:22:24 2022


======================================================================
----------------------------------------------------------------------
                run_ID=775
Sun May  8 23:22:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.694 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 775,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.554 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.236 Time taken: 576.61 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.85 secs| Accuracy 98.036

Epoch: 2 Train Loss 0.062 Time taken: 573.58 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.79 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.032 Time taken: 573.67 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 22.97 secs| Accuracy 98.644
T\P      0       1
0       5234        0075        
1       0078        5230        

Accuracy 98.559
Time taken: 33.79 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.479 Time taken: 379.42 secs

    Epoch: 4 Validation Loss 0.337 Time taken: 1.82 secs| Accuracy 83.444
T\P      0       1
0       0173        0025        
1       0028        0228        

Accuracy 88.326
Time taken: 2.51 secs
2231.665 secs

Mon May  9 00:00:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=801
Fri May 13 11:52:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.548 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 801,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.955 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.304 Time taken: 263.0 secs

    Epoch: 1 Validation Loss 0.216 Time taken: 35.82 secs| Accuracy 91.511

Epoch: 2 Train Loss 0.115 Time taken: 263.45 secs

    Epoch: 2 Validation Loss 0.233 Time taken: 35.77 secs| Accuracy 91.308

Epoch: 3 Train Loss 0.067 Time taken: 264.67 secs

    Epoch: 3 Validation Loss 0.2 Time taken: 35.61 secs| Accuracy 92.654

Epoch: 4 Train Loss 0.041 Time taken: 264.34 secs

    Epoch: 4 Validation Loss 0.233 Time taken: 35.64 secs| Accuracy 92.637
T\P      0       1
0       0350        0150        
1       0057        0343        

Accuracy 77.0
Time taken: 3.55 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1212.995 secs

Fri May 13 12:13:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=802
Fri May 13 12:13:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.78 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 802,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.916 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.309 Time taken: 261.33 secs

    Epoch: 1 Validation Loss 0.214 Time taken: 35.58 secs| Accuracy 91.651

Epoch: 2 Train Loss 0.118 Time taken: 262.99 secs

    Epoch: 2 Validation Loss 0.25 Time taken: 35.61 secs| Accuracy 90.613

Epoch: 3 Train Loss 0.067 Time taken: 264.28 secs

    Epoch: 3 Validation Loss 0.24 Time taken: 35.58 secs| Accuracy 92.091

Epoch: 4 Train Loss 0.042 Time taken: 261.84 secs

    Epoch: 4 Validation Loss 0.277 Time taken: 35.54 secs| Accuracy 91.959
T\P      0       1
0       0379        0121        
1       0090        0310        

Accuracy 76.556
Time taken: 3.4 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1207.262 secs

Fri May 13 12:34:28 2022


======================================================================
----------------------------------------------------------------------
                run_ID=803
Fri May 13 12:34:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.23 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 803,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.998 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.31 Time taken: 263.63 secs

    Epoch: 1 Validation Loss 0.238 Time taken: 35.74 secs| Accuracy 90.472

Epoch: 2 Train Loss 0.118 Time taken: 263.94 secs

    Epoch: 2 Validation Loss 0.201 Time taken: 35.65 secs| Accuracy 92.241

Epoch: 3 Train Loss 0.071 Time taken: 263.94 secs

    Epoch: 3 Validation Loss 0.242 Time taken: 35.78 secs| Accuracy 91.836

Epoch: 4 Train Loss 0.044 Time taken: 263.47 secs

    Epoch: 4 Validation Loss 0.275 Time taken: 35.68 secs| Accuracy 92.056
T\P      0       1
0       0370        0130        
1       0092        0308        

Accuracy 75.333
Time taken: 3.44 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1212.271 secs

Fri May 13 12:55:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=804
Fri May 13 12:55:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.918 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 804,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.462 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.301 Time taken: 263.94 secs

    Epoch: 1 Validation Loss 0.255 Time taken: 35.69 secs| Accuracy 89.989

Epoch: 2 Train Loss 0.123 Time taken: 263.8 secs

    Epoch: 2 Validation Loss 0.217 Time taken: 35.77 secs| Accuracy 91.854

Epoch: 3 Train Loss 0.072 Time taken: 264.63 secs

    Epoch: 3 Validation Loss 0.279 Time taken: 35.86 secs| Accuracy 90.481

Epoch: 4 Train Loss 0.045 Time taken: 264.6 secs

    Epoch: 4 Validation Loss 0.263 Time taken: 35.76 secs| Accuracy 91.642
T\P      0       1
0       0364        0136        
1       0077        0323        

Accuracy 76.333
Time taken: 3.52 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1215.299 secs

Fri May 13 13:16:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=805
Fri May 13 13:16:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 26523 | Validation_set: 11367 | Test_set: 900
Train_batches: 829 | Validation_batches: 356 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.265 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 805,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts+dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 14.334 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.309 Time taken: 264.8 secs

    Epoch: 1 Validation Loss 0.224 Time taken: 35.78 secs| Accuracy 90.974

Epoch: 2 Train Loss 0.115 Time taken: 265.34 secs

    Epoch: 2 Validation Loss 0.209 Time taken: 35.94 secs| Accuracy 92.311

Epoch: 3 Train Loss 0.067 Time taken: 265.14 secs

    Epoch: 3 Validation Loss 0.247 Time taken: 35.83 secs| Accuracy 92.012

Epoch: 4 Train Loss 0.038 Time taken: 264.73 secs

    Epoch: 4 Validation Loss 0.286 Time taken: 35.84 secs| Accuracy 91.836
T\P      0       1
0       0320        0180        
1       0071        0329        

Accuracy 72.111
Time taken: 3.51 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1217.908 secs

Fri May 13 13:38:05 2022


======================================================================
----------------------------------------------------------------------
                run_ID=806
Fri May 13 13:38:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.358 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 806,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.133 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.629 Time taken: 21.07 secs

    Epoch: 1 Validation Loss 0.477 Time taken: 2.92 secs| Accuracy 92.8

Epoch: 2 Train Loss 0.356 Time taken: 21.28 secs

    Epoch: 2 Validation Loss 0.321 Time taken: 2.76 secs| Accuracy 91.2

Epoch: 3 Train Loss 0.201 Time taken: 21.23 secs

    Epoch: 3 Validation Loss 0.257 Time taken: 2.87 secs| Accuracy 92.267

Epoch: 4 Train Loss 0.148 Time taken: 21.54 secs

    Epoch: 4 Validation Loss 0.247 Time taken: 2.78 secs| Accuracy 91.867
T\P      0       1
0       0451        0049        
1       0029        0371        

Accuracy 91.333
Time taken: 3.27 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
110.079 secs

Fri May 13 13:40:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=807
Fri May 13 13:40:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.2 GB
25 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 807,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.173 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.582 Time taken: 21.29 secs

    Epoch: 1 Validation Loss 0.445 Time taken: 2.9 secs| Accuracy 91.333

Epoch: 2 Train Loss 0.343 Time taken: 21.6 secs

    Epoch: 2 Validation Loss 0.308 Time taken: 2.95 secs| Accuracy 92.533

Epoch: 3 Train Loss 0.228 Time taken: 21.47 secs

    Epoch: 3 Validation Loss 0.262 Time taken: 2.91 secs| Accuracy 92.133

Epoch: 4 Train Loss 0.156 Time taken: 21.51 secs

    Epoch: 4 Validation Loss 0.26 Time taken: 2.92 secs| Accuracy 92.133
T\P      0       1
0       0430        0070        
1       0008        0392        

Accuracy 91.333
Time taken: 3.42 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
111.607 secs

Fri May 13 13:43:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=808
Fri May 13 13:43:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.205 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 808,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 7.976 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.603 Time taken: 21.43 secs

    Epoch: 1 Validation Loss 0.457 Time taken: 2.91 secs| Accuracy 92.667

Epoch: 2 Train Loss 0.355 Time taken: 21.39 secs

    Epoch: 2 Validation Loss 0.311 Time taken: 2.93 secs| Accuracy 92.0

Epoch: 3 Train Loss 0.233 Time taken: 21.47 secs

    Epoch: 3 Validation Loss 0.285 Time taken: 2.95 secs| Accuracy 90.533

Epoch: 4 Train Loss 0.183 Time taken: 21.54 secs

    Epoch: 4 Validation Loss 0.307 Time taken: 2.91 secs| Accuracy 89.867
T\P      0       1
0       0402        0098        
1       0004        0396        

Accuracy 88.667
Time taken: 3.37 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
111.581 secs

Fri May 13 13:46:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=809
Fri May 13 13:46:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.203 GB
23 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 809,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.209 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.571 Time taken: 21.91 secs

    Epoch: 1 Validation Loss 0.43 Time taken: 2.98 secs| Accuracy 91.067

Epoch: 2 Train Loss 0.33 Time taken: 21.89 secs

    Epoch: 2 Validation Loss 0.289 Time taken: 3.04 secs| Accuracy 91.867

Epoch: 3 Train Loss 0.209 Time taken: 21.9 secs

    Epoch: 3 Validation Loss 0.257 Time taken: 3.02 secs| Accuracy 91.733

Epoch: 4 Train Loss 0.146 Time taken: 21.93 secs

    Epoch: 4 Validation Loss 0.243 Time taken: 3.03 secs| Accuracy 91.6
T\P      0       1
0       0416        0084        
1       0023        0377        

Accuracy 88.111
Time taken: 3.5 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
113.85 secs

Fri May 13 13:49:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=810
Fri May 13 13:49:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.198 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 810,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 7.345 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.59 Time taken: 21.66 secs

    Epoch: 1 Validation Loss 0.452 Time taken: 2.92 secs| Accuracy 92.133

Epoch: 2 Train Loss 0.358 Time taken: 21.74 secs

    Epoch: 2 Validation Loss 0.314 Time taken: 2.93 secs| Accuracy 92.133

Epoch: 3 Train Loss 0.245 Time taken: 21.73 secs

    Epoch: 3 Validation Loss 0.248 Time taken: 2.92 secs| Accuracy 92.933

Epoch: 4 Train Loss 0.164 Time taken: 21.73 secs

    Epoch: 4 Validation Loss 0.23 Time taken: 2.99 secs| Accuracy 92.533
T\P      0       1
0       0452        0048        
1       0022        0378        

Accuracy 92.222
Time taken: 3.43 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
112.773 secs

Fri May 13 13:51:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=811
Fri May 13 13:51:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.293 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 811,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.96 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.691 Time taken: 11.52 secs

    Epoch: 1 Validation Loss 0.683 Time taken: 2.01 secs| Accuracy 50.0

Epoch: 2 Train Loss 0.65 Time taken: 11.56 secs

    Epoch: 2 Validation Loss 0.626 Time taken: 1.98 secs| Accuracy 73.0

Epoch: 3 Train Loss 0.547 Time taken: 11.5 secs

    Epoch: 3 Validation Loss 0.554 Time taken: 2.01 secs| Accuracy 79.5
T\P      0       1
0       0171        0029        
1       0061        0139        

Accuracy 77.5
Time taken: 1.99 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.405 Time taken: 22.09 secs

    Epoch: 4 Validation Loss 0.307 Time taken: 3.07 secs| Accuracy 91.733

Epoch: 5 Train Loss 0.238 Time taken: 22.15 secs

    Epoch: 5 Validation Loss 0.222 Time taken: 3.08 secs| Accuracy 93.2
T\P      0       1
0       0456        0044        
1       0021        0379        

Accuracy 92.778
Time taken: 3.57 secs
118.425 secs

Fri May 13 13:54:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=812
Fri May 13 13:54:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.165 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 812,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.333 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.689 Time taken: 11.12 secs

    Epoch: 1 Validation Loss 0.677 Time taken: 1.93 secs| Accuracy 65.75

Epoch: 2 Train Loss 0.633 Time taken: 11.03 secs

    Epoch: 2 Validation Loss 0.603 Time taken: 1.89 secs| Accuracy 78.0

Epoch: 3 Train Loss 0.505 Time taken: 11.25 secs

    Epoch: 3 Validation Loss 0.544 Time taken: 1.89 secs| Accuracy 74.75
T\P      0       1
0       0184        0016        
1       0084        0116        

Accuracy 75.0
Time taken: 1.89 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.353 Time taken: 21.53 secs

    Epoch: 4 Validation Loss 0.288 Time taken: 2.97 secs| Accuracy 92.133

Epoch: 5 Train Loss 0.224 Time taken: 21.65 secs

    Epoch: 5 Validation Loss 0.25 Time taken: 2.92 secs| Accuracy 92.0
T\P      0       1
0       0438        0062        
1       0012        0388        

Accuracy 91.778
Time taken: 3.45 secs
117.269 secs

Fri May 13 13:57:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=813
Fri May 13 13:57:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.17 GB
24 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 813,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.162 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.691 Time taken: 11.69 secs

    Epoch: 1 Validation Loss 0.683 Time taken: 1.99 secs| Accuracy 50.25

Epoch: 2 Train Loss 0.652 Time taken: 11.75 secs

    Epoch: 2 Validation Loss 0.629 Time taken: 2.07 secs| Accuracy 72.75

Epoch: 3 Train Loss 0.547 Time taken: 11.73 secs

    Epoch: 3 Validation Loss 0.553 Time taken: 2.01 secs| Accuracy 79.75
T\P      0       1
0       0157        0043        
1       0049        0151        

Accuracy 77.0
Time taken: 2.1 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.395 Time taken: 22.32 secs

    Epoch: 4 Validation Loss 0.309 Time taken: 3.08 secs| Accuracy 91.467

Epoch: 5 Train Loss 0.235 Time taken: 22.24 secs

    Epoch: 5 Validation Loss 0.242 Time taken: 3.07 secs| Accuracy 92.533
T\P      0       1
0       0452        0048        
1       0017        0383        

Accuracy 92.778
Time taken: 3.55 secs
119.803 secs

Fri May 13 14:00:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=814
Fri May 13 14:00:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.109 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 814,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 9.881 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.692 Time taken: 10.97 secs

    Epoch: 1 Validation Loss 0.688 Time taken: 1.84 secs| Accuracy 54.25

Epoch: 2 Train Loss 0.641 Time taken: 10.79 secs

    Epoch: 2 Validation Loss 0.606 Time taken: 1.79 secs| Accuracy 75.75

Epoch: 3 Train Loss 0.532 Time taken: 10.91 secs

    Epoch: 3 Validation Loss 0.564 Time taken: 1.76 secs| Accuracy 73.75
T\P      0       1
0       0116        0084        
1       0028        0172        

Accuracy 72.0
Time taken: 1.78 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.359 Time taken: 21.49 secs

    Epoch: 4 Validation Loss 0.297 Time taken: 2.79 secs| Accuracy 90.667

Epoch: 5 Train Loss 0.283 Time taken: 21.61 secs

    Epoch: 5 Validation Loss 0.253 Time taken: 2.85 secs| Accuracy 92.0
T\P      0       1
0       0429        0071        
1       0013        0387        

Accuracy 90.667
Time taken: 3.33 secs
113.683 secs

Fri May 13 14:03:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=815
Fri May 13 14:03:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 800 | Validation_set: 400 | Test_set: 400
Train_batches: 25 | Validation_batches: 13 | Test_batches: 13
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.114 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 815,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.315 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.687 Time taken: 10.84 secs

    Epoch: 1 Validation Loss 0.673 Time taken: 1.8 secs| Accuracy 67.0

Epoch: 2 Train Loss 0.628 Time taken: 10.78 secs

    Epoch: 2 Validation Loss 0.603 Time taken: 1.77 secs| Accuracy 78.75

Epoch: 3 Train Loss 0.523 Time taken: 11.1 secs

    Epoch: 3 Validation Loss 0.543 Time taken: 1.82 secs| Accuracy 79.0
T\P      0       1
0       0171        0029        
1       0059        0141        

Accuracy 78.0
Time taken: 1.8 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.371 Time taken: 21.31 secs

    Epoch: 4 Validation Loss 0.291 Time taken: 2.8 secs| Accuracy 92.133

Epoch: 5 Train Loss 0.217 Time taken: 21.2 secs

    Epoch: 5 Validation Loss 0.308 Time taken: 2.8 secs| Accuracy 88.533
T\P      0       1
0       0383        0117        
1       0003        0397        

Accuracy 86.667
Time taken: 3.28 secs
114.979 secs

Fri May 13 14:05:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=816
Fri May 13 14:05:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.145 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 816,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 6.964 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.355 Time taken: 205.02 secs

    Epoch: 1 Validation Loss 0.187 Time taken: 22.5 secs| Accuracy 92.922

Epoch: 2 Train Loss 0.137 Time taken: 205.9 secs

    Epoch: 2 Validation Loss 0.153 Time taken: 22.37 secs| Accuracy 94.236

Epoch: 3 Train Loss 0.073 Time taken: 206.4 secs

    Epoch: 3 Validation Loss 0.157 Time taken: 22.51 secs| Accuracy 94.433

Epoch: 4 Train Loss 0.049 Time taken: 205.99 secs

    Epoch: 4 Validation Loss 0.142 Time taken: 22.61 secs| Accuracy 95.239
T\P      0       1
0       5029        0280        
1       0234        5074        

Accuracy 95.159
Time taken: 33.53 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
957.574 secs

Fri May 13 14:22:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=817
Fri May 13 14:22:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.826 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 817,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.126 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.357 Time taken: 206.21 secs

    Epoch: 1 Validation Loss 0.193 Time taken: 22.69 secs| Accuracy 92.286

Epoch: 2 Train Loss 0.133 Time taken: 206.85 secs

    Epoch: 2 Validation Loss 0.187 Time taken: 22.72 secs| Accuracy 92.54

Epoch: 3 Train Loss 0.078 Time taken: 207.24 secs

    Epoch: 3 Validation Loss 0.144 Time taken: 22.73 secs| Accuracy 94.702

Epoch: 4 Train Loss 0.049 Time taken: 206.32 secs

    Epoch: 4 Validation Loss 0.173 Time taken: 22.66 secs| Accuracy 94.546
T\P      0       1
0       5080        0229        
1       0349        4959        

Accuracy 94.556
Time taken: 33.61 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
962.788 secs

Fri May 13 14:39:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=818
Fri May 13 14:39:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.946 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 818,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.129 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.37 Time taken: 204.91 secs

    Epoch: 1 Validation Loss 0.198 Time taken: 22.41 secs| Accuracy 92.71

Epoch: 2 Train Loss 0.145 Time taken: 205.8 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 22.43 secs| Accuracy 94.335

Epoch: 3 Train Loss 0.081 Time taken: 205.97 secs

    Epoch: 3 Validation Loss 0.144 Time taken: 22.48 secs| Accuracy 94.716

Epoch: 4 Train Loss 0.053 Time taken: 205.0 secs

    Epoch: 4 Validation Loss 0.194 Time taken: 22.43 secs| Accuracy 93.388
T\P      0       1
0       5169        0140        
1       0487        4821        

Accuracy 94.094
Time taken: 33.35 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
955.726 secs

Fri May 13 14:56:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=819
Fri May 13 14:56:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.395 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 819,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.411 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.35 Time taken: 204.69 secs

    Epoch: 1 Validation Loss 0.197 Time taken: 22.37 secs| Accuracy 92.074

Epoch: 2 Train Loss 0.14 Time taken: 206.17 secs

    Epoch: 2 Validation Loss 0.147 Time taken: 22.43 secs| Accuracy 94.207

Epoch: 3 Train Loss 0.079 Time taken: 205.78 secs

    Epoch: 3 Validation Loss 0.151 Time taken: 22.4 secs| Accuracy 94.193

Epoch: 4 Train Loss 0.054 Time taken: 205.48 secs

    Epoch: 4 Validation Loss 0.174 Time taken: 22.36 secs| Accuracy 94.561
T\P      0       1
0       4934        0375        
1       0189        5119        

Accuracy 94.688
Time taken: 33.32 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
955.796 secs

Fri May 13 15:13:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=820
Fri May 13 15:13:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.487 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 820,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.816 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.353 Time taken: 205.27 secs

    Epoch: 1 Validation Loss 0.195 Time taken: 22.5 secs| Accuracy 92.752

Epoch: 2 Train Loss 0.138 Time taken: 206.57 secs

    Epoch: 2 Validation Loss 0.162 Time taken: 22.35 secs| Accuracy 93.713

Epoch: 3 Train Loss 0.078 Time taken: 205.05 secs

    Epoch: 3 Validation Loss 0.222 Time taken: 22.46 secs| Accuracy 92.498

Epoch: 4 Train Loss 0.049 Time taken: 205.32 secs

    Epoch: 4 Validation Loss 0.144 Time taken: 22.42 secs| Accuracy 95.606
T\P      0       1
0       5026        0283        
1       0246        5062        

Accuracy 95.017
Time taken: 33.36 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
957.548 secs

Fri May 13 15:30:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=826
Fri May 13 15:56:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.036 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 826,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.985 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.355 Time taken: 204.29 secs

    Epoch: 1 Validation Loss 0.187 Time taken: 22.33 secs| Accuracy 92.922

Epoch: 2 Train Loss 0.137 Time taken: 205.66 secs

    Epoch: 2 Validation Loss 0.153 Time taken: 22.36 secs| Accuracy 94.236

Epoch: 3 Train Loss 0.073 Time taken: 205.94 secs

    Epoch: 3 Validation Loss 0.157 Time taken: 22.38 secs| Accuracy 94.433
T\P      0       1
0       5133        0176        
1       0388        4920        

Accuracy 94.688
Time taken: 33.36 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.261 Time taken: 21.65 secs

    Epoch: 4 Validation Loss 0.204 Time taken: 2.89 secs| Accuracy 91.867

Epoch: 5 Train Loss 0.126 Time taken: 21.78 secs

    Epoch: 5 Validation Loss 0.231 Time taken: 3.1 secs| Accuracy 91.733
T\P      0       1
0       0435        0065        
1       0015        0385        

Accuracy 91.111
Time taken: 3.63 secs
791.856 secs

Fri May 13 16:10:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=827
Fri May 13 16:10:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.885 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 827,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 1.426 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.357 Time taken: 204.93 secs

    Epoch: 1 Validation Loss 0.193 Time taken: 22.39 secs| Accuracy 92.286

Epoch: 2 Train Loss 0.133 Time taken: 205.35 secs

    Epoch: 2 Validation Loss 0.187 Time taken: 22.35 secs| Accuracy 92.54

Epoch: 3 Train Loss 0.078 Time taken: 205.91 secs

    Epoch: 3 Validation Loss 0.144 Time taken: 22.4 secs| Accuracy 94.702
T\P      0       1
0       4999        0310        
1       0291        5017        

Accuracy 94.339
Time taken: 33.34 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.263 Time taken: 21.44 secs

    Epoch: 4 Validation Loss 0.206 Time taken: 2.91 secs| Accuracy 91.867

Epoch: 5 Train Loss 0.141 Time taken: 21.61 secs

    Epoch: 5 Validation Loss 0.19 Time taken: 2.89 secs| Accuracy 91.6
T\P      0       1
0       0443        0057        
1       0022        0378        

Accuracy 91.222
Time taken: 3.39 secs
791.1 secs

Fri May 13 16:24:28 2022


======================================================================
----------------------------------------------------------------------
                run_ID=828
Fri May 13 16:24:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.161 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 828,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.026 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.37 Time taken: 206.41 secs

    Epoch: 1 Validation Loss 0.198 Time taken: 22.55 secs| Accuracy 92.71

Epoch: 2 Train Loss 0.145 Time taken: 206.2 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 22.5 secs| Accuracy 94.335

Epoch: 3 Train Loss 0.081 Time taken: 206.3 secs

    Epoch: 3 Validation Loss 0.144 Time taken: 22.61 secs| Accuracy 94.716
T\P      0       1
0       5085        0224        
1       0332        4976        

Accuracy 94.763
Time taken: 33.32 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.268 Time taken: 22.0 secs

    Epoch: 4 Validation Loss 0.199 Time taken: 3.07 secs| Accuracy 92.533

Epoch: 5 Train Loss 0.128 Time taken: 22.22 secs

    Epoch: 5 Validation Loss 0.21 Time taken: 2.99 secs| Accuracy 91.733
T\P      0       1
0       0446        0054        
1       0023        0377        

Accuracy 91.444
Time taken: 3.5 secs
796.334 secs

Fri May 13 16:38:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=829
Fri May 13 16:38:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.092 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 829,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.03 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.35 Time taken: 204.48 secs

    Epoch: 1 Validation Loss 0.197 Time taken: 22.39 secs| Accuracy 92.074

Epoch: 2 Train Loss 0.14 Time taken: 205.81 secs

    Epoch: 2 Validation Loss 0.147 Time taken: 22.42 secs| Accuracy 94.207

Epoch: 3 Train Loss 0.079 Time taken: 205.64 secs

    Epoch: 3 Validation Loss 0.151 Time taken: 22.44 secs| Accuracy 94.193
T\P      0       1
0       4953        0356        
1       0239        5069        

Accuracy 94.396
Time taken: 33.33 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.319 Time taken: 21.75 secs

    Epoch: 4 Validation Loss 0.183 Time taken: 2.92 secs| Accuracy 93.333

Epoch: 5 Train Loss 0.156 Time taken: 21.42 secs

    Epoch: 5 Validation Loss 0.209 Time taken: 2.95 secs| Accuracy 92.267
T\P      0       1
0       0465        0035        
1       0031        0369        

Accuracy 92.667
Time taken: 3.32 secs
790.39 secs

Fri May 13 16:52:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=830
Fri May 13 16:52:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 830,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.986 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.353 Time taken: 205.56 secs

    Epoch: 1 Validation Loss 0.195 Time taken: 22.36 secs| Accuracy 92.752

Epoch: 2 Train Loss 0.138 Time taken: 205.62 secs

    Epoch: 2 Validation Loss 0.162 Time taken: 22.37 secs| Accuracy 93.713

Epoch: 3 Train Loss 0.078 Time taken: 205.51 secs

    Epoch: 3 Validation Loss 0.222 Time taken: 22.41 secs| Accuracy 92.498
T\P      0       1
0       5228        0081        
1       0692        4616        

Accuracy 92.719
Time taken: 33.23 secs

Including dpil data now
Batch_size: 32
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 55 | Validation_batches: 24 | Test_batches: 29

Epoch: 4 Train Loss 0.251 Time taken: 21.49 secs

    Epoch: 4 Validation Loss 0.193 Time taken: 2.88 secs| Accuracy 92.133

Epoch: 5 Train Loss 0.124 Time taken: 21.63 secs

    Epoch: 5 Validation Loss 0.215 Time taken: 2.97 secs| Accuracy 91.333
T\P      0       1
0       0442        0058        
1       0016        0384        

Accuracy 91.778
Time taken: 3.47 secs
791.901 secs

Fri May 13 17:07:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=836
Fri May 13 17:10:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.147 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 836,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.564 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.348 Time taken: 220.03 secs

    Epoch: 1 Validation Loss 0.176 Time taken: 22.52 secs| Accuracy 93.444

Epoch: 2 Train Loss 0.132 Time taken: 220.87 secs

    Epoch: 2 Validation Loss 0.144 Time taken: 22.44 secs| Accuracy 94.674

Epoch: 3 Train Loss 0.069 Time taken: 220.63 secs

    Epoch: 3 Validation Loss 0.148 Time taken: 22.43 secs| Accuracy 94.462

Epoch: 4 Train Loss 0.044 Time taken: 221.8 secs

    Epoch: 4 Validation Loss 0.148 Time taken: 22.51 secs| Accuracy 95.225
T\P      0       1
0       5116        0193        
1       0287        5021        

Accuracy 95.479
Time taken: 33.44 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1017.826 secs

Fri May 13 17:28:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=837
Fri May 13 17:28:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.07 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 837,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 0.862 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.349 Time taken: 220.08 secs

    Epoch: 1 Validation Loss 0.196 Time taken: 22.43 secs| Accuracy 91.961

Epoch: 2 Train Loss 0.128 Time taken: 221.25 secs

    Epoch: 2 Validation Loss 0.17 Time taken: 22.39 secs| Accuracy 93.727

Epoch: 3 Train Loss 0.072 Time taken: 220.52 secs

    Epoch: 3 Validation Loss 0.149 Time taken: 22.43 secs| Accuracy 94.9

Epoch: 4 Train Loss 0.047 Time taken: 221.84 secs

    Epoch: 4 Validation Loss 0.142 Time taken: 22.48 secs| Accuracy 95.168
T\P      0       1
0       5042        0267        
1       0256        5052        

Accuracy 95.074
Time taken: 33.36 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1018.087 secs

Fri May 13 17:46:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=838
Fri May 13 17:46:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.594 GB
26 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 838,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.833 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.37 Time taken: 220.54 secs

    Epoch: 1 Validation Loss 0.188 Time taken: 22.45 secs| Accuracy 92.837

Epoch: 2 Train Loss 0.144 Time taken: 221.24 secs

    Epoch: 2 Validation Loss 0.151 Time taken: 22.44 secs| Accuracy 94.476

Epoch: 3 Train Loss 0.079 Time taken: 220.53 secs

    Epoch: 3 Validation Loss 0.151 Time taken: 22.45 secs| Accuracy 94.688

Epoch: 4 Train Loss 0.048 Time taken: 220.5 secs

    Epoch: 4 Validation Loss 0.157 Time taken: 22.47 secs| Accuracy 94.476
T\P      0       1
0       5127        0182        
1       0369        4939        

Accuracy 94.81
Time taken: 33.42 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1016.709 secs

Fri May 13 18:04:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=839
Fri May 13 18:04:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 839,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.029 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.349 Time taken: 220.79 secs

    Epoch: 1 Validation Loss 0.177 Time taken: 22.37 secs| Accuracy 93.091

Epoch: 2 Train Loss 0.128 Time taken: 220.19 secs

    Epoch: 2 Validation Loss 0.176 Time taken: 22.34 secs| Accuracy 93.487

Epoch: 3 Train Loss 0.073 Time taken: 219.83 secs

    Epoch: 3 Validation Loss 0.16 Time taken: 22.38 secs| Accuracy 93.911

Epoch: 4 Train Loss 0.049 Time taken: 220.9 secs

    Epoch: 4 Validation Loss 0.155 Time taken: 22.35 secs| Accuracy 94.871
T\P      0       1
0       5074        0235        
1       0272        5036        

Accuracy 95.225
Time taken: 33.21 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1015.215 secs

Fri May 13 18:22:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=840
Fri May 13 18:22:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.857 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 840,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.97 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.358 Time taken: 219.71 secs

    Epoch: 1 Validation Loss 0.183 Time taken: 22.36 secs| Accuracy 92.809

Epoch: 2 Train Loss 0.136 Time taken: 220.89 secs

    Epoch: 2 Validation Loss 0.163 Time taken: 22.36 secs| Accuracy 93.995

Epoch: 3 Train Loss 0.075 Time taken: 220.23 secs

    Epoch: 3 Validation Loss 0.201 Time taken: 22.35 secs| Accuracy 92.724

Epoch: 4 Train Loss 0.045 Time taken: 220.95 secs

    Epoch: 4 Validation Loss 0.159 Time taken: 22.37 secs| Accuracy 95.055
T\P      0       1
0       5022        0287        
1       0245        5063        

Accuracy 94.989
Time taken: 33.27 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1015.155 secs

Fri May 13 18:40:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=851
Mon May 16 00:38:09 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.113 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 851,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.124 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.452 Time taken: 49.75 secs

    Epoch: 1 Validation Loss 0.212 Time taken: 6.69 secs| Accuracy 98.533

Epoch: 2 Train Loss 0.14 Time taken: 56.4 secs

    Epoch: 2 Validation Loss 0.111 Time taken: 6.9 secs| Accuracy 98.267

Epoch: 3 Train Loss 0.077 Time taken: 58.05 secs

    Epoch: 3 Validation Loss 0.081 Time taken: 7.07 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.043 Time taken: 58.6 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 7.11 secs| Accuracy 98.667
T\P      0       1
0       0344        0156        
1       0003        0397        

Accuracy 82.333
Time taken: 8.55 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
275.14 secs

Mon May 16 00:43:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=852
Mon May 16 00:43:33 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.525 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 852,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.64 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.427 Time taken: 54.93 secs

    Epoch: 1 Validation Loss 0.232 Time taken: 7.05 secs| Accuracy 97.733

Epoch: 2 Train Loss 0.141 Time taken: 59.05 secs

    Epoch: 2 Validation Loss 0.118 Time taken: 7.23 secs| Accuracy 97.867

Epoch: 3 Train Loss 0.068 Time taken: 59.37 secs

    Epoch: 3 Validation Loss 0.083 Time taken: 7.26 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.038 Time taken: 59.63 secs

    Epoch: 4 Validation Loss 0.08 Time taken: 7.22 secs| Accuracy 98.267
T\P      0       1
0       0397        0103        
1       0007        0393        

Accuracy 87.778
Time taken: 8.63 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
285.695 secs

Mon May 16 00:49:05 2022


======================================================================
----------------------------------------------------------------------
                run_ID=853
Mon May 16 00:49:08 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.054 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 853,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.767 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.44 Time taken: 55.43 secs

    Epoch: 1 Validation Loss 0.218 Time taken: 7.1 secs| Accuracy 98.4

Epoch: 2 Train Loss 0.137 Time taken: 59.26 secs

    Epoch: 2 Validation Loss 0.109 Time taken: 7.28 secs| Accuracy 98.0

Epoch: 3 Train Loss 0.068 Time taken: 59.39 secs

    Epoch: 3 Validation Loss 0.079 Time taken: 7.26 secs| Accuracy 98.133

Epoch: 4 Train Loss 0.044 Time taken: 59.82 secs

    Epoch: 4 Validation Loss 0.064 Time taken: 7.25 secs| Accuracy 98.4
T\P      0       1
0       0328        0172        
1       0003        0397        

Accuracy 80.556
Time taken: 8.65 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
286.662 secs

Mon May 16 00:54:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=854
Mon May 16 00:54:44 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.104 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 854,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.815 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.41 Time taken: 56.01 secs

    Epoch: 1 Validation Loss 0.201 Time taken: 7.2 secs| Accuracy 98.667

Epoch: 2 Train Loss 0.125 Time taken: 60.13 secs

    Epoch: 2 Validation Loss 0.1 Time taken: 7.3 secs| Accuracy 98.267

Epoch: 3 Train Loss 0.063 Time taken: 60.24 secs

    Epoch: 3 Validation Loss 0.078 Time taken: 7.29 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.035 Time taken: 60.12 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 7.3 secs| Accuracy 98.667
T\P      0       1
0       0339        0161        
1       0006        0394        

Accuracy 81.444
Time taken: 8.76 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
289.543 secs

Mon May 16 01:00:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=855
Mon May 16 01:00:23 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.04 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 855,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.863 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.429 Time taken: 55.68 secs

    Epoch: 1 Validation Loss 0.218 Time taken: 7.18 secs| Accuracy 98.533

Epoch: 2 Train Loss 0.132 Time taken: 59.33 secs

    Epoch: 2 Validation Loss 0.124 Time taken: 7.24 secs| Accuracy 97.333

Epoch: 3 Train Loss 0.071 Time taken: 59.64 secs

    Epoch: 3 Validation Loss 0.109 Time taken: 7.28 secs| Accuracy 97.467

Epoch: 4 Train Loss 0.043 Time taken: 59.86 secs

    Epoch: 4 Validation Loss 0.069 Time taken: 7.25 secs| Accuracy 98.4
T\P      0       1
0       0395        0105        
1       0004        0396        

Accuracy 87.889
Time taken: 8.7 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
287.079 secs

Mon May 16 01:05:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=856
Mon May 16 01:06:01 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.381 GB
21 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 856,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.104 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.514 Time taken: 55.2 secs

    Epoch: 1 Validation Loss 0.306 Time taken: 7.17 secs| Accuracy 92.667

Epoch: 2 Train Loss 0.22 Time taken: 59.51 secs

    Epoch: 2 Validation Loss 0.259 Time taken: 7.25 secs| Accuracy 91.467

Epoch: 3 Train Loss 0.132 Time taken: 59.71 secs

    Epoch: 3 Validation Loss 0.224 Time taken: 7.26 secs| Accuracy 92.267

Epoch: 4 Train Loss 0.082 Time taken: 60.07 secs

    Epoch: 4 Validation Loss 0.261 Time taken: 7.32 secs| Accuracy 91.067
T\P      0       1
0       0434        0066        
1       0007        0393        

Accuracy 91.889
Time taken: 8.78 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
287.138 secs

Mon May 16 01:11:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=857
Mon May 16 01:11:36 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.464 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 857,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.787 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.493 Time taken: 55.46 secs

    Epoch: 1 Validation Loss 0.35 Time taken: 7.12 secs| Accuracy 89.867

Epoch: 2 Train Loss 0.235 Time taken: 59.28 secs

    Epoch: 2 Validation Loss 0.245 Time taken: 7.26 secs| Accuracy 91.733

Epoch: 3 Train Loss 0.145 Time taken: 59.77 secs

    Epoch: 3 Validation Loss 0.233 Time taken: 7.27 secs| Accuracy 92.667

Epoch: 4 Train Loss 0.12 Time taken: 60.04 secs

    Epoch: 4 Validation Loss 0.239 Time taken: 7.3 secs| Accuracy 91.467
T\P      0       1
0       0423        0077        
1       0010        0390        

Accuracy 90.333
Time taken: 8.66 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
286.973 secs

Mon May 16 01:17:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=858
Mon May 16 01:17:12 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.271 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 858,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.873 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.502 Time taken: 55.38 secs

    Epoch: 1 Validation Loss 0.314 Time taken: 7.23 secs| Accuracy 92.667

Epoch: 2 Train Loss 0.238 Time taken: 59.6 secs

    Epoch: 2 Validation Loss 0.279 Time taken: 7.3 secs| Accuracy 90.533

Epoch: 3 Train Loss 0.153 Time taken: 60.04 secs

    Epoch: 3 Validation Loss 0.218 Time taken: 7.26 secs| Accuracy 92.8

Epoch: 4 Train Loss 0.107 Time taken: 59.5 secs

    Epoch: 4 Validation Loss 0.263 Time taken: 7.16 secs| Accuracy 91.067
T\P      0       1
0       0421        0079        
1       0007        0393        

Accuracy 90.444
Time taken: 8.64 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
287.149 secs

Mon May 16 01:22:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=859
Mon May 16 01:22:47 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.176 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 859,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.857 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.475 Time taken: 54.91 secs

    Epoch: 1 Validation Loss 0.302 Time taken: 7.11 secs| Accuracy 91.467

Epoch: 2 Train Loss 0.217 Time taken: 58.68 secs

    Epoch: 2 Validation Loss 0.227 Time taken: 7.16 secs| Accuracy 92.667

Epoch: 3 Train Loss 0.125 Time taken: 57.73 secs

    Epoch: 3 Validation Loss 0.239 Time taken: 6.94 secs| Accuracy 91.467

Epoch: 4 Train Loss 0.088 Time taken: 57.2 secs

    Epoch: 4 Validation Loss 0.251 Time taken: 7.02 secs| Accuracy 91.733
T\P      0       1
0       0453        0047        
1       0021        0379        

Accuracy 92.444
Time taken: 8.46 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
280.261 secs

Mon May 16 01:28:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=860
Mon May 16 01:28:17 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.014 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 860,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.852 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.494 Time taken: 54.69 secs

    Epoch: 1 Validation Loss 0.342 Time taken: 7.1 secs| Accuracy 90.4

Epoch: 2 Train Loss 0.234 Time taken: 58.91 secs

    Epoch: 2 Validation Loss 0.232 Time taken: 7.19 secs| Accuracy 92.0

Epoch: 3 Train Loss 0.146 Time taken: 59.2 secs

    Epoch: 3 Validation Loss 0.205 Time taken: 7.2 secs| Accuracy 93.067

Epoch: 4 Train Loss 0.097 Time taken: 59.32 secs

    Epoch: 4 Validation Loss 0.248 Time taken: 7.25 secs| Accuracy 91.467
T\P      0       1
0       0447        0053        
1       0022        0378        

Accuracy 91.667
Time taken: 8.68 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
285.185 secs

Mon May 16 01:33:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=861
Mon May 16 01:33:51 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.004 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 861,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.817 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.449 Time taken: 59.87 secs

    Epoch: 1 Validation Loss 0.213 Time taken: 7.12 secs| Accuracy 98.133

Epoch: 2 Train Loss 0.136 Time taken: 64.46 secs

    Epoch: 2 Validation Loss 0.103 Time taken: 7.36 secs| Accuracy 98.267

Epoch: 3 Train Loss 0.067 Time taken: 66.13 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 7.51 secs| Accuracy 98.533

Epoch: 4 Train Loss 0.042 Time taken: 66.74 secs

    Epoch: 4 Validation Loss 0.068 Time taken: 7.53 secs| Accuracy 98.533
T\P      0       1
0       0344        0156        
1       0005        0395        

Accuracy 82.111
Time taken: 9.04 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
311.242 secs

Mon May 16 01:39:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=862
Mon May 16 01:39:51 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.417 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 862,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.902 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.426 Time taken: 61.66 secs

    Epoch: 1 Validation Loss 0.224 Time taken: 7.45 secs| Accuracy 98.4

Epoch: 2 Train Loss 0.136 Time taken: 65.78 secs

    Epoch: 2 Validation Loss 0.134 Time taken: 7.44 secs| Accuracy 97.067

Epoch: 3 Train Loss 0.069 Time taken: 65.78 secs

    Epoch: 3 Validation Loss 0.084 Time taken: 7.41 secs| Accuracy 97.867

Epoch: 4 Train Loss 0.042 Time taken: 65.43 secs

    Epoch: 4 Validation Loss 0.076 Time taken: 7.43 secs| Accuracy 98.133
T\P      0       1
0       0340        0160        
1       0003        0397        

Accuracy 81.889
Time taken: 8.88 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
312.448 secs

Mon May 16 01:45:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=863
Mon May 16 01:45:53 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.224 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 863,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.918 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.442 Time taken: 60.96 secs

    Epoch: 1 Validation Loss 0.225 Time taken: 7.36 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.135 Time taken: 65.17 secs

    Epoch: 2 Validation Loss 0.126 Time taken: 7.35 secs| Accuracy 97.467

Epoch: 3 Train Loss 0.082 Time taken: 65.26 secs

    Epoch: 3 Validation Loss 0.08 Time taken: 7.43 secs| Accuracy 98.133

Epoch: 4 Train Loss 0.044 Time taken: 65.32 secs

    Epoch: 4 Validation Loss 0.062 Time taken: 7.44 secs| Accuracy 98.533
T\P      0       1
0       0364        0136        
1       0003        0397        

Accuracy 84.556
Time taken: 8.83 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
310.117 secs

Mon May 16 01:51:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=864
Mon May 16 01:51:52 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.03 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 864,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.844 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.409 Time taken: 60.39 secs

    Epoch: 1 Validation Loss 0.2 Time taken: 7.24 secs| Accuracy 98.933

Epoch: 2 Train Loss 0.131 Time taken: 64.2 secs

    Epoch: 2 Validation Loss 0.096 Time taken: 7.3 secs| Accuracy 98.667

Epoch: 3 Train Loss 0.074 Time taken: 64.72 secs

    Epoch: 3 Validation Loss 0.081 Time taken: 7.33 secs| Accuracy 98.0

Epoch: 4 Train Loss 0.038 Time taken: 64.73 secs

    Epoch: 4 Validation Loss 0.065 Time taken: 7.32 secs| Accuracy 98.533
T\P      0       1
0       0350        0150        
1       0003        0397        

Accuracy 83.0
Time taken: 8.8 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
307.039 secs

Mon May 16 01:57:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=865
Mon May 16 01:57:49 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.272 GB
22 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 865,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.937 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.431 Time taken: 60.49 secs

    Epoch: 1 Validation Loss 0.22 Time taken: 7.28 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.144 Time taken: 64.16 secs

    Epoch: 2 Validation Loss 0.1 Time taken: 7.35 secs| Accuracy 98.4

Epoch: 3 Train Loss 0.059 Time taken: 64.41 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 7.35 secs| Accuracy 98.933

Epoch: 4 Train Loss 0.033 Time taken: 64.84 secs

    Epoch: 4 Validation Loss 0.065 Time taken: 7.34 secs| Accuracy 98.4
T\P      0       1
0       0361        0139        
1       0004        0396        

Accuracy 84.111
Time taken: 8.81 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
308.13 secs

Mon May 16 02:03:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=866
Mon May 16 02:03:46 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.203 GB
21 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 866,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.963 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.516 Time taken: 60.14 secs

    Epoch: 1 Validation Loss 0.328 Time taken: 7.14 secs| Accuracy 91.2

Epoch: 2 Train Loss 0.229 Time taken: 63.88 secs

    Epoch: 2 Validation Loss 0.223 Time taken: 7.26 secs| Accuracy 92.267

Epoch: 3 Train Loss 0.136 Time taken: 64.27 secs

    Epoch: 3 Validation Loss 0.207 Time taken: 7.21 secs| Accuracy 91.6

Epoch: 4 Train Loss 0.093 Time taken: 64.62 secs

    Epoch: 4 Validation Loss 0.203 Time taken: 7.29 secs| Accuracy 93.333
T\P      0       1
0       0453        0047        
1       0022        0378        

Accuracy 92.333
Time taken: 8.76 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
305.523 secs

Mon May 16 02:09:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=867
Mon May 16 02:09:41 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.142 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 867,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.936 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.497 Time taken: 60.49 secs

    Epoch: 1 Validation Loss 0.358 Time taken: 7.25 secs| Accuracy 89.467

Epoch: 2 Train Loss 0.253 Time taken: 64.53 secs

    Epoch: 2 Validation Loss 0.243 Time taken: 7.36 secs| Accuracy 91.733

Epoch: 3 Train Loss 0.159 Time taken: 64.75 secs

    Epoch: 3 Validation Loss 0.21 Time taken: 7.28 secs| Accuracy 92.133

Epoch: 4 Train Loss 0.101 Time taken: 65.4 secs

    Epoch: 4 Validation Loss 0.232 Time taken: 7.45 secs| Accuracy 92.267
T\P      0       1
0       0428        0072        
1       0012        0388        

Accuracy 90.667
Time taken: 8.94 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
308.649 secs

Mon May 16 02:15:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=868
Mon May 16 02:15:39 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.26 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 868,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.922 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.5 Time taken: 60.37 secs

    Epoch: 1 Validation Loss 0.324 Time taken: 7.25 secs| Accuracy 91.733

Epoch: 2 Train Loss 0.239 Time taken: 64.13 secs

    Epoch: 2 Validation Loss 0.236 Time taken: 7.3 secs| Accuracy 92.4

Epoch: 3 Train Loss 0.142 Time taken: 64.42 secs

    Epoch: 3 Validation Loss 0.226 Time taken: 7.36 secs| Accuracy 92.933

Epoch: 4 Train Loss 0.102 Time taken: 64.78 secs

    Epoch: 4 Validation Loss 0.219 Time taken: 7.37 secs| Accuracy 92.0
T\P      0       1
0       0442        0058        
1       0019        0381        

Accuracy 91.444
Time taken: 8.84 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
306.683 secs

Mon May 16 02:21:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=869
Mon May 16 02:21:35 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.258 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 869,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.844 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.475 Time taken: 60.31 secs

    Epoch: 1 Validation Loss 0.303 Time taken: 7.24 secs| Accuracy 92.133

Epoch: 2 Train Loss 0.227 Time taken: 64.07 secs

    Epoch: 2 Validation Loss 0.243 Time taken: 7.3 secs| Accuracy 91.067

Epoch: 3 Train Loss 0.154 Time taken: 64.48 secs

    Epoch: 3 Validation Loss 0.211 Time taken: 7.35 secs| Accuracy 92.533

Epoch: 4 Train Loss 0.096 Time taken: 64.45 secs

    Epoch: 4 Validation Loss 0.244 Time taken: 7.31 secs| Accuracy 91.733
T\P      0       1
0       0443        0057        
1       0011        0389        

Accuracy 92.444
Time taken: 8.7 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
306.261 secs

Mon May 16 02:27:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=870
Mon May 16 02:27:30 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.105 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 870,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.789 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.492 Time taken: 59.97 secs

    Epoch: 1 Validation Loss 0.342 Time taken: 7.22 secs| Accuracy 90.667

Epoch: 2 Train Loss 0.23 Time taken: 64.07 secs

    Epoch: 2 Validation Loss 0.245 Time taken: 7.33 secs| Accuracy 92.133

Epoch: 3 Train Loss 0.15 Time taken: 64.61 secs

    Epoch: 3 Validation Loss 0.196 Time taken: 7.33 secs| Accuracy 94.0

Epoch: 4 Train Loss 0.091 Time taken: 64.69 secs

    Epoch: 4 Validation Loss 0.202 Time taken: 7.34 secs| Accuracy 92.8
T\P      0       1
0       0444        0056        
1       0013        0387        

Accuracy 92.333
Time taken: 8.77 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
306.199 secs

Mon May 16 02:33:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=871
Mon May 16 02:33:25 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.152 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 871,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.716 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.441 Time taken: 55.89 secs

    Epoch: 1 Validation Loss 0.166 Time taken: 7.19 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.124 Time taken: 59.91 secs

    Epoch: 2 Validation Loss 0.136 Time taken: 7.27 secs| Accuracy 96.267

Epoch: 3 Train Loss 0.064 Time taken: 60.13 secs

    Epoch: 3 Validation Loss 0.095 Time taken: 7.33 secs| Accuracy 97.733

Epoch: 4 Train Loss 0.042 Time taken: 60.13 secs

    Epoch: 4 Validation Loss 0.058 Time taken: 7.28 secs| Accuracy 98.667
T\P      0       1
0       0366        0134        
1       0005        0395        

Accuracy 84.556
Time taken: 8.75 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
288.815 secs

Mon May 16 02:39:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=872
Mon May 16 02:39:03 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.041 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 872,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.932 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.412 Time taken: 55.52 secs

    Epoch: 1 Validation Loss 0.153 Time taken: 7.22 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.102 Time taken: 60.17 secs

    Epoch: 2 Validation Loss 0.096 Time taken: 7.38 secs| Accuracy 98.267

Epoch: 3 Train Loss 0.056 Time taken: 61.44 secs

    Epoch: 3 Validation Loss 0.078 Time taken: 7.55 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.035 Time taken: 62.66 secs

    Epoch: 4 Validation Loss 0.081 Time taken: 7.61 secs| Accuracy 98.267
T\P      0       1
0       0363        0137        
1       0004        0396        

Accuracy 84.333
Time taken: 9.13 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
294.693 secs

Mon May 16 02:44:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=873
Mon May 16 02:44:48 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.155 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 873,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.872 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.396 Time taken: 58.27 secs

    Epoch: 1 Validation Loss 0.141 Time taken: 7.49 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.105 Time taken: 62.36 secs

    Epoch: 2 Validation Loss 0.084 Time taken: 7.57 secs| Accuracy 98.133

Epoch: 3 Train Loss 0.057 Time taken: 62.22 secs

    Epoch: 3 Validation Loss 0.067 Time taken: 7.6 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.049 Time taken: 62.34 secs

    Epoch: 4 Validation Loss 0.054 Time taken: 7.6 secs| Accuracy 98.667
T\P      0       1
0       0356        0144        
1       0004        0396        

Accuracy 83.556
Time taken: 9.02 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
300.752 secs

Mon May 16 02:50:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=874
Mon May 16 02:50:37 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.579 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 874,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.873 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.391 Time taken: 55.74 secs

    Epoch: 1 Validation Loss 0.138 Time taken: 7.18 secs| Accuracy 98.533

Epoch: 2 Train Loss 0.092 Time taken: 60.21 secs

    Epoch: 2 Validation Loss 0.077 Time taken: 7.38 secs| Accuracy 98.533

Epoch: 3 Train Loss 0.05 Time taken: 61.45 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 7.46 secs| Accuracy 98.267

Epoch: 4 Train Loss 0.025 Time taken: 61.61 secs

    Epoch: 4 Validation Loss 0.054 Time taken: 7.48 secs| Accuracy 98.8
T\P      0       1
0       0312        0188        
1       0002        0398        

Accuracy 78.889
Time taken: 8.92 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
292.406 secs

Mon May 16 02:56:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=875
Mon May 16 02:56:18 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.088 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 875,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.865 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.404 Time taken: 56.86 secs

    Epoch: 1 Validation Loss 0.139 Time taken: 7.42 secs| Accuracy 98.8

Epoch: 2 Train Loss 0.1 Time taken: 60.74 secs

    Epoch: 2 Validation Loss 0.075 Time taken: 7.41 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.054 Time taken: 61.07 secs

    Epoch: 3 Validation Loss 0.067 Time taken: 7.45 secs| Accuracy 98.4

Epoch: 4 Train Loss 0.032 Time taken: 61.38 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 7.51 secs| Accuracy 98.667
T\P      0       1
0       0336        0164        
1       0003        0397        

Accuracy 81.444
Time taken: 8.91 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
293.808 secs

Mon May 16 03:01:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=876
Mon May 16 03:02:00 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.893 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 876,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.964 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 611.51 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 68.04 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.053 Time taken: 615.18 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 67.49 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.026 Time taken: 613.61 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 68.2 secs| Accuracy 98.347

Epoch: 4 Train Loss 0.019 Time taken: 616.27 secs

    Epoch: 4 Validation Loss 0.071 Time taken: 67.84 secs| Accuracy 98.404
T\P      0       1
0       1002        4307        
1       0074        5234        

Accuracy 58.736
Time taken: 101.54 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.057 Time taken: 58.71 secs

    Epoch: 5 Validation Loss 0.047 Time taken: 7.44 secs| Accuracy 98.267

Epoch: 6 Train Loss 0.023 Time taken: 61.09 secs

    Epoch: 6 Validation Loss 0.064 Time taken: 7.45 secs| Accuracy 98.0
T\P      0       1
0       0307        0193        
1       0003        0397        

Accuracy 78.222
Time taken: 8.93 secs
2995.944 secs

Mon May 16 03:52:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=877
Mon May 16 03:52:50 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.868 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 877,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.982 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.152 Time taken: 626.91 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 71.45 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.053 Time taken: 639.61 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 68.36 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.03 Time taken: 623.98 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 69.33 secs| Accuracy 98.404

Epoch: 4 Train Loss 0.017 Time taken: 630.45 secs

    Epoch: 4 Validation Loss 0.052 Time taken: 68.96 secs| Accuracy 98.474
T\P      0       1
0       1628        3681        
1       0083        5225        

Accuracy 64.547
Time taken: 103.22 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.053 Time taken: 59.91 secs

    Epoch: 5 Validation Loss 0.052 Time taken: 7.53 secs| Accuracy 98.533

Epoch: 6 Train Loss 0.011 Time taken: 61.99 secs

    Epoch: 6 Validation Loss 0.055 Time taken: 7.57 secs| Accuracy 98.667
T\P      0       1
0       0333        0167        
1       0004        0396        

Accuracy 81.0
Time taken: 9.0 secs
3069.663 secs

Mon May 16 04:44:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=878
Mon May 16 04:44:55 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
26 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 878,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.021 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 619.75 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 68.59 secs| Accuracy 97.796

Epoch: 2 Train Loss 0.05 Time taken: 618.61 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 67.89 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.025 Time taken: 638.94 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 69.38 secs| Accuracy 98.375

Epoch: 4 Train Loss 0.017 Time taken: 626.96 secs

    Epoch: 4 Validation Loss 0.077 Time taken: 68.88 secs| Accuracy 97.98
T\P      0       1
0       0676        4633        
1       0058        5250        

Accuracy 55.816
Time taken: 103.43 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.058 Time taken: 59.97 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 7.57 secs| Accuracy 98.8

Epoch: 6 Train Loss 0.018 Time taken: 61.48 secs

    Epoch: 6 Validation Loss 0.047 Time taken: 7.49 secs| Accuracy 98.8
T\P      0       1
0       0362        0138        
1       0005        0395        

Accuracy 84.111
Time taken: 8.95 secs
3049.192 secs

Mon May 16 05:36:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=879
Mon May 16 05:36:37 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.088 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 879,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.871 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.146 Time taken: 608.1 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 68.1 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.048 Time taken: 620.59 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 68.01 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.026 Time taken: 616.7 secs

    Epoch: 3 Validation Loss 0.117 Time taken: 67.63 secs| Accuracy 96.991

Epoch: 4 Train Loss 0.017 Time taken: 619.46 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 69.4 secs| Accuracy 98.488
T\P      0       1
0       1267        4042        
1       0072        5236        

Accuracy 61.251
Time taken: 101.28 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.057 Time taken: 57.85 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 7.23 secs| Accuracy 98.533

Epoch: 6 Train Loss 0.006 Time taken: 58.74 secs

    Epoch: 6 Validation Loss 0.057 Time taken: 7.25 secs| Accuracy 98.267
T\P      0       1
0       0306        0194        
1       0003        0397        

Accuracy 78.111
Time taken: 8.53 secs
3000.756 secs

Mon May 16 06:27:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=880
Mon May 16 06:27:32 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.937 GB
26 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 880,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts then dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.118 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.15 Time taken: 581.0 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 62.81 secs| Accuracy 98.22

Epoch: 2 Train Loss 0.051 Time taken: 561.85 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 61.1 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.026 Time taken: 549.87 secs

    Epoch: 3 Validation Loss 0.068 Time taken: 59.83 secs| Accuracy 98.079

Epoch: 4 Train Loss 0.018 Time taken: 536.51 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 58.47 secs| Accuracy 98.248
T\P      0       1
0       0672        4637        
1       0034        5274        

Accuracy 56.005
Time taken: 88.12 secs

Including dpil data now
Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57

Epoch: 5 Train Loss 0.053 Time taken: 51.64 secs

    Epoch: 5 Validation Loss 0.053 Time taken: 6.51 secs| Accuracy 98.667

Epoch: 6 Train Loss 0.011 Time taken: 52.7 secs

    Epoch: 6 Validation Loss 0.054 Time taken: 6.53 secs| Accuracy 98.667
T\P      0       1
0       0339        0161        
1       0005        0395        

Accuracy 81.556
Time taken: 7.79 secs
2706.485 secs

Mon May 16 07:13:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=898
Mon May 16 17:49:15 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 270
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 17
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.899 GB
26 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 898,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.389 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=898
Mon May 16 17:55:21 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.578 GB
25 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 898,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.011 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 386.8 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 50.58 secs| Accuracy 97.796


======================================================================
----------------------------------------------------------------------
                run_ID=898
Mon May 16 18:09:12 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.516 GB
25 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 898,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.043 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 387.77 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 50.85 secs| Accuracy 97.796

Epoch: 2 Train Loss 0.05 Time taken: 395.32 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 51.07 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.025 Time taken: 397.56 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 52.09 secs| Accuracy 98.375

Epoch: 4 Train Loss 0.017 Time taken: 400.59 secs

    Epoch: 4 Validation Loss 0.077 Time taken: 51.53 secs| Accuracy 97.98
T\P      0       1
0       0242        0258        
1       0003        0397        

Accuracy 71.0
Time taken: 6.82 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
1807.782 secs

Mon May 16 18:40:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=899
Mon May 16 18:40:13 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.801 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 899,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.149 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.146 Time taken: 395.29 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 51.54 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.048 Time taken: 398.87 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 51.45 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.026 Time taken: 396.94 secs

    Epoch: 3 Validation Loss 0.117 Time taken: 51.2 secs| Accuracy 96.991

Epoch: 4 Train Loss 0.017 Time taken: 395.98 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 50.94 secs| Accuracy 98.488
T\P      0       1
0       0296        0204        
1       0004        0396        

Accuracy 76.889
Time taken: 6.9 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
1813.484 secs

Mon May 16 19:11:16 2022



======================================================================
----------------------------------------------------------------------
                run_ID=195
Thu Apr 14 20:21:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 195,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.171 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.165 Time taken: 597.01 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 23.26 secs| Accuracy 98.135

Epoch: 2 Train Loss 0.064 Time taken: 601.57 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.19 secs| Accuracy 98.545

Epoch: 3 Train Loss 0.045 Time taken: 600.43 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 23.28 secs| Accuracy 98.955

Epoch: 4 Train Loss 0.034 Time taken: 595.1 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.48 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.022 Time taken: 601.26 secs

    Epoch: 5 Validation Loss 0.03 Time taken: 23.17 secs| Accuracy 99.166
T\P      0       1
0       5281        0028        
1       0057        5251        

Accuracy 99.1994
Time taken: 33.97 secs
3185.586 secs


On the best model
T\P      0       1
0       5281        0028        
1       0057        5251        

Accuracy 99.1994
Time taken: 33.98 secs
Thu Apr 14 21:16:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=196
Thu Apr 14 21:16:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.863 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 196,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.36 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.222 Time taken: 601.4 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 24.07 secs| Accuracy 98.474

Epoch: 2 Train Loss 0.075 Time taken: 599.87 secs

    Epoch: 2 Validation Loss 0.047 Time taken: 24.03 secs| Accuracy 98.644

Epoch: 3 Train Loss 0.051 Time taken: 599.3 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 24.26 secs| Accuracy 98.743

Epoch: 4 Train Loss 0.039 Time taken: 598.67 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 24.01 secs| Accuracy 98.474

Epoch: 5 Train Loss 0.026 Time taken: 604.92 secs

    Epoch: 5 Validation Loss 0.046 Time taken: 23.35 secs| Accuracy 98.743
T\P      0       1
0       5232        0077        
1       0033        5275        

Accuracy 98.9639
Time taken: 34.54 secs
3194.483 secs


On the best model
T\P      0       1
0       5246        0063        
1       0045        5263        

Accuracy 98.9828
Time taken: 34.32 secs
Thu Apr 14 22:11:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=197
Thu Apr 14 22:11:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 197,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.209 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.167 Time taken: 598.22 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 24.0 secs| Accuracy 98.22

Epoch: 2 Train Loss 0.065 Time taken: 601.53 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.89 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.047 Time taken: 604.81 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.77 secs| Accuracy 98.912

Epoch: 4 Train Loss 0.03 Time taken: 595.22 secs

    Epoch: 4 Validation Loss 0.042 Time taken: 23.87 secs| Accuracy 98.813

Epoch: 5 Train Loss 0.023 Time taken: 594.47 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.71 secs| Accuracy 99.053
T\P      0       1
0       5261        0048        
1       0049        5259        

Accuracy 99.0864
Time taken: 34.6 secs
3187.657 secs


On the best model
T\P      0       1
0       5261        0048        
1       0049        5259        

Accuracy 99.0864
Time taken: 34.7 secs
Thu Apr 14 23:05:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=198
Thu Apr 14 23:05:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.044 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 198,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.379 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.18 Time taken: 596.3 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 23.62 secs| Accuracy 98.333

Epoch: 2 Train Loss 0.07 Time taken: 592.93 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 23.7 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.048 Time taken: 598.73 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.63 secs| Accuracy 98.884

Epoch: 4 Train Loss 0.042 Time taken: 593.63 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 23.6 secs| Accuracy 98.841

Epoch: 5 Train Loss 0.028 Time taken: 593.96 secs

    Epoch: 5 Validation Loss 0.055 Time taken: 23.36 secs| Accuracy 98.319
T\P      0       1
0       5186        0123        
1       0033        5275        

Accuracy 98.5307
Time taken: 34.37 secs
3152.479 secs


On the best model
T\P      0       1
0       5274        0035        
1       0076        5232        

Accuracy 98.9545
Time taken: 34.55 secs
Fri Apr 15 00:00:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=199
Fri Apr 15 00:00:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.045 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 199,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.361 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.157 Time taken: 596.83 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.34 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 594.92 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.32 secs| Accuracy 98.276

Epoch: 3 Train Loss 0.041 Time taken: 603.45 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.3 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.032 Time taken: 594.79 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.33 secs| Accuracy 98.898

Epoch: 5 Train Loss 0.022 Time taken: 594.62 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.13 secs| Accuracy 98.94
T\P      0       1
0       5268        0041        
1       0064        5244        

Accuracy 99.011
Time taken: 33.99 secs
3180.038 secs


On the best model
T\P      0       1
0       5268        0041        
1       0064        5244        

Accuracy 99.011
Time taken: 34.08 secs
Fri Apr 15 00:54:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=200
Fri Apr 15 00:54:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.045 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 200,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.355 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.176 Time taken: 596.05 secs

    Epoch: 1 Validation Loss 0.051 Time taken: 23.24 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.063 Time taken: 592.5 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.45 secs| Accuracy 98.771

Epoch: 3 Train Loss 0.044 Time taken: 592.88 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.5 secs| Accuracy 98.912

Epoch: 4 Train Loss 0.034 Time taken: 592.6 secs

    Epoch: 4 Validation Loss 0.044 Time taken: 23.53 secs| Accuracy 98.488

Epoch: 5 Train Loss 0.024 Time taken: 593.69 secs

    Epoch: 5 Validation Loss 0.046 Time taken: 23.22 secs| Accuracy 98.94
T\P      0       1
0       5230        0079        
1       0039        5269        

Accuracy 98.8886
Time taken: 34.31 secs
3158.363 secs


On the best model
T\P      0       1
0       5230        0079        
1       0039        5269        

Accuracy 98.8886
Time taken: 34.21 secs
Fri Apr 15 01:49:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=201
Fri Apr 15 01:49:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.283 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 201,
    "message": "118 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 4.102 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.178 Time taken: 594.99 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 23.06 secs| Accuracy 98.502

Epoch: 2 Train Loss 0.062 Time taken: 591.83 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.15 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.046 Time taken: 591.31 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.19 secs| Accuracy 98.517

Epoch: 4 Train Loss 0.034 Time taken: 591.53 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.14 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.025 Time taken: 591.61 secs

    Epoch: 5 Validation Loss 0.047 Time taken: 23.02 secs| Accuracy 98.983
T\P      0       1
0       5276        0033        
1       0061        5247        

Accuracy 99.1146
Time taken: 34.05 secs
3149.517 secs


On the best model
T\P      0       1
0       5276        0033        
1       0061        5247        

Accuracy 99.1146
Time taken: 33.76 secs
Fri Apr 15 02:43:24 2022



======================================================================
----------------------------------------------------------------------
                run_ID=214
Fri Apr 15 17:19:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 35391 | Validation_set: 14154 | Test_set: 21235
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 7.127 GB
40 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 214,
    "message": "212 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 2,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.765 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.136 Time taken: 821.09 secs

    Epoch: 1 Validation Loss 0.039 Time taken: 45.13 secs| Macro F: 0.986

Epoch: 2 Train Loss 0.051 Time taken: 825.05 secs

    Epoch: 2 Validation Loss 0.03 Time taken: 44.95 secs| Macro F: 0.989

Epoch: 3 Train Loss 0.034 Time taken: 817.44 secs

    Epoch: 3 Validation Loss 0.031 Time taken: 44.74 secs| Macro F: 0.986

Epoch: 4 Train Loss 0.025 Time taken: 815.0 secs

    Epoch: 4 Validation Loss 0.029 Time taken: 44.95 secs| Macro F: 0.99

Epoch: 5 Train Loss 0.018 Time taken: 814.93 secs

    Epoch: 5 Validation Loss 0.023 Time taken: 44.88 secs| Macro F: 0.993
T\P      0       1
0       15854       0073        
1       0074        5234        

Macro F: 0.991
Time taken: 66.55 secs
4419.497 secs


On the best model
T\P      0       1
0       15854       0073        
1       0074        5234        

Macro F: 0.991
Time taken: 66.59 secs
Fri Apr 15 18:35:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=215
Fri Apr 15 18:36:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 44239 | Validation_set: 17692 | Test_set: 26544
Train_batches: 1383 | Validation_batches: 553 | Test_batches: 830
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.188 GB
41 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 215,
    "message": "188 with repeated negatives thrice",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 3,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.14 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.123 Time taken: 930.54 secs

    Epoch: 1 Validation Loss 0.04 Time taken: 55.81 secs| Macro F: 0.983

Epoch: 2 Train Loss 0.043 Time taken: 934.43 secs

    Epoch: 2 Validation Loss 0.024 Time taken: 55.99 secs| Macro F: 0.988

Epoch: 3 Train Loss 0.028 Time taken: 925.84 secs

    Epoch: 3 Validation Loss 0.021 Time taken: 55.91 secs| Macro F: 0.991

Epoch: 4 Train Loss 0.021 Time taken: 925.62 secs

    Epoch: 4 Validation Loss 0.024 Time taken: 55.82 secs| Macro F: 0.989

Epoch: 5 Train Loss 0.016 Time taken: 927.89 secs

    Epoch: 5 Validation Loss 0.023 Time taken: 55.77 secs| Macro F: 0.989
T\P      0       1
0       21095       0141        
1       0056        5252        

Macro F: 0.988
Time taken: 82.84 secs
5034.493 secs


On the best model
T\P      0       1
0       21185       0051        
1       0089        5219        

Macro F: 0.992
Time taken: 82.56 secs
Fri Apr 15 20:02:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=216
Fri Apr 15 20:02:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 44239 | Validation_set: 17692 | Test_set: 26544
Train_batches: 1383 | Validation_batches: 553 | Test_batches: 830
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.209 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 216,
    "message": "215 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 3,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.304 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.115 Time taken: 931.8 secs

    Epoch: 1 Validation Loss 0.036 Time taken: 56.45 secs| Macro F: 0.985

Epoch: 2 Train Loss 0.042 Time taken: 927.3 secs

    Epoch: 2 Validation Loss 0.021 Time taken: 56.17 secs| Macro F: 0.992

Epoch: 3 Train Loss 0.029 Time taken: 937.52 secs

    Epoch: 3 Validation Loss 0.028 Time taken: 55.88 secs| Macro F: 0.987

Epoch: 4 Train Loss 0.02 Time taken: 927.4 secs

    Epoch: 4 Validation Loss 0.025 Time taken: 55.86 secs| Macro F: 0.987

Epoch: 5 Train Loss 0.016 Time taken: 928.77 secs

    Epoch: 5 Validation Loss 0.026 Time taken: 56.28 secs| Macro F: 0.987
T\P      0       1
0       21021       0215        
1       0048        5260        

Macro F: 0.985
Time taken: 83.62 secs
5041.224 secs


On the best model
T\P      0       1
0       21139       0097        
1       0080        5228        

Macro F: 0.99
Time taken: 83.33 secs
Fri Apr 15 21:29:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=217
Fri Apr 15 21:29:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 44239 | Validation_set: 17692 | Test_set: 26544
Train_batches: 1383 | Validation_batches: 553 | Test_batches: 830
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.378 GB
37 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 217,
    "message": "215 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 3,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.181 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.13 Time taken: 930.1 secs

    Epoch: 1 Validation Loss 0.043 Time taken: 56.26 secs| Macro F: 0.982

Epoch: 2 Train Loss 0.052 Time taken: 931.41 secs

    Epoch: 2 Validation Loss 0.031 Time taken: 55.82 secs| Macro F: 0.985

Epoch: 3 Train Loss 0.034 Time taken: 926.61 secs

    Epoch: 3 Validation Loss 0.021 Time taken: 55.92 secs| Macro F: 0.99

Epoch: 4 Train Loss 0.026 Time taken: 924.1 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 55.79 secs| Macro F: 0.981

Epoch: 5 Train Loss 0.019 Time taken: 925.49 secs

    Epoch: 5 Validation Loss 0.025 Time taken: 55.72 secs| Macro F: 0.987
T\P      0       1
0       21038       0198        
1       0049        5259        

Macro F: 0.986
Time taken: 82.7 secs
5029.302 secs


On the best model
T\P      0       1
0       21170       0066        
1       0091        5217        

Macro F: 0.991
Time taken: 82.43 secs
Fri Apr 15 22:55:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=218
Fri Apr 15 22:55:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 35391 | Validation_set: 14154 | Test_set: 21235
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.764 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 218,
    "message": "188 with repeated negatives once and curated negatives included",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": true,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.377 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.27 Time taken: 813.87 secs

    Epoch: 1 Validation Loss 0.103 Time taken: 44.64 secs| Macro F: 0.946

Epoch: 2 Train Loss 0.101 Time taken: 811.51 secs

    Epoch: 2 Validation Loss 0.077 Time taken: 44.68 secs| Macro F: 0.966

Epoch: 3 Train Loss 0.061 Time taken: 812.09 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 44.69 secs| Macro F: 0.97

Epoch: 4 Train Loss 0.044 Time taken: 812.44 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 44.69 secs| Macro F: 0.97

Epoch: 5 Train Loss 0.031 Time taken: 814.41 secs

    Epoch: 5 Validation Loss 0.093 Time taken: 44.71 secs| Macro F: 0.965
T\P      0       1
0       15387       0540        
1       0107        5201        

Macro F: 0.96
Time taken: 66.41 secs
4388.836 secs


On the best model
T\P      0       1
0       15581       0346        
1       0150        5158        

Macro F: 0.969
Time taken: 66.3 secs
Sat Apr 16 00:11:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=219
Sat Apr 16 00:11:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 35391 | Validation_set: 14154 | Test_set: 21235
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.771 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 219,
    "message": "218 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": true,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.37 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.217 Time taken: 815.27 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 44.63 secs| Macro F: 0.961

Epoch: 2 Train Loss 0.079 Time taken: 812.69 secs

    Epoch: 2 Validation Loss 0.067 Time taken: 44.81 secs| Macro F: 0.968

Epoch: 3 Train Loss 0.05 Time taken: 811.92 secs

    Epoch: 3 Validation Loss 0.068 Time taken: 44.74 secs| Macro F: 0.97

Epoch: 4 Train Loss 0.033 Time taken: 812.33 secs

    Epoch: 4 Validation Loss 0.079 Time taken: 44.93 secs| Macro F: 0.968

Epoch: 5 Train Loss 0.022 Time taken: 814.27 secs

    Epoch: 5 Validation Loss 0.102 Time taken: 44.82 secs| Macro F: 0.962
T\P      0       1
0       15365       0562        
1       0070        5238        

Macro F: 0.961
Time taken: 66.36 secs
4388.248 secs


On the best model
T\P      0       1
0       15624       0303        
1       0192        5116        

Macro F: 0.969
Time taken: 66.17 secs
Sat Apr 16 01:26:25 2022



======================================================================
----------------------------------------------------------------------
                run_ID=220
Sat Apr 16 01:31:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 35391 | Validation_set: 14154 | Test_set: 21235
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.583 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 220,
    "message": "218 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 1,
    "include_curated_negatives": true,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.207 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.245 Time taken: 819.73 secs

    Epoch: 1 Validation Loss 0.106 Time taken: 45.35 secs| Macro F: 0.948

Epoch: 2 Train Loss 0.094 Time taken: 815.59 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 45.32 secs| Macro F: 0.964

Epoch: 3 Train Loss 0.058 Time taken: 815.02 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 45.33 secs| Macro F: 0.968

Epoch: 4 Train Loss 0.039 Time taken: 816.04 secs

    Epoch: 4 Validation Loss 0.069 Time taken: 44.97 secs| Macro F: 0.968

Epoch: 5 Train Loss 0.028 Time taken: 816.71 secs

    Epoch: 5 Validation Loss 0.087 Time taken: 45.48 secs| Macro F: 0.969
T\P      0       1
0       15777       0150        
1       0322        4986        

Macro F: 0.97
Time taken: 67.26 secs
4412.124 secs


On the best model
T\P      0       1
0       15777       0150        
1       0322        4986        

Macro F: 0.97
Time taken: 66.93 secs
Sat Apr 16 02:46:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=241
Sat Apr 16 02:46:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 241,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.556 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.517 Time taken: 537.49 secs

    Epoch: 1 Validation Loss 0.296 Time taken: 23.29 secs| Accuracy 88.217

Epoch: 2 Train Loss 0.243 Time taken: 533.87 secs

    Epoch: 2 Validation Loss 0.173 Time taken: 23.36 secs| Accuracy 92.865

Epoch: 3 Train Loss 0.164 Time taken: 533.02 secs

    Epoch: 3 Validation Loss 0.147 Time taken: 23.39 secs| Accuracy 94.546

Epoch: 4 Train Loss 0.122 Time taken: 533.91 secs

    Epoch: 4 Validation Loss 0.139 Time taken: 23.51 secs| Accuracy 94.758

Epoch: 5 Train Loss 0.092 Time taken: 533.97 secs

    Epoch: 5 Validation Loss 0.131 Time taken: 23.43 secs| Accuracy 95.422
T\P      0       1
0       5020        0289        
1       0184        5124        

Accuracy 95.545
Time taken: 34.38 secs
2866.696 secs


On the best model
T\P      0       1
0       5020        0289        
1       0184        5124        

Accuracy 95.545
Time taken: 34.03 secs
Sat Apr 16 03:36:18 2022


======================================================================
----------------------------------------------------------------------
                run_ID=242
Sat Apr 16 03:36:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 242,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.369 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.491 Time taken: 534.66 secs

    Epoch: 1 Validation Loss 0.224 Time taken: 23.2 secs| Accuracy 91.029

Epoch: 2 Train Loss 0.22 Time taken: 531.28 secs

    Epoch: 2 Validation Loss 0.178 Time taken: 23.27 secs| Accuracy 92.908

Epoch: 3 Train Loss 0.147 Time taken: 538.77 secs

    Epoch: 3 Validation Loss 0.14 Time taken: 23.46 secs| Accuracy 94.886

Epoch: 4 Train Loss 0.11 Time taken: 534.1 secs

    Epoch: 4 Validation Loss 0.126 Time taken: 23.43 secs| Accuracy 95.324

Epoch: 5 Train Loss 0.086 Time taken: 534.33 secs

    Epoch: 5 Validation Loss 0.137 Time taken: 23.52 secs| Accuracy 95.182
T\P      0       1
0       4896        0413        
1       0103        5205        

Accuracy 95.14
Time taken: 34.26 secs
2862.792 secs


On the best model
T\P      0       1
0       5102        0207        
1       0258        5050        

Accuracy 95.62
Time taken: 33.97 secs
Sat Apr 16 04:25:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=243
Sat Apr 16 04:25:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 243,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.359 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.43 Time taken: 534.58 secs

    Epoch: 1 Validation Loss 0.203 Time taken: 23.12 secs| Accuracy 92.159

Epoch: 2 Train Loss 0.198 Time taken: 531.88 secs

    Epoch: 2 Validation Loss 0.156 Time taken: 23.14 secs| Accuracy 94.207

Epoch: 3 Train Loss 0.14 Time taken: 530.83 secs

    Epoch: 3 Validation Loss 0.149 Time taken: 23.3 secs| Accuracy 94.617

Epoch: 4 Train Loss 0.104 Time taken: 537.41 secs

    Epoch: 4 Validation Loss 0.138 Time taken: 23.22 secs| Accuracy 94.9

Epoch: 5 Train Loss 0.083 Time taken: 531.63 secs

    Epoch: 5 Validation Loss 0.145 Time taken: 23.27 secs| Accuracy 95.507
T\P      0       1
0       5054        0255        
1       0202        5106        

Accuracy 95.696
Time taken: 33.95 secs
2861.117 secs


On the best model
T\P      0       1
0       5054        0255        
1       0202        5106        

Accuracy 95.696
Time taken: 33.74 secs
Sat Apr 16 05:15:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=244
Sat Apr 16 05:15:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 244,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.365 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.466 Time taken: 537.21 secs

    Epoch: 1 Validation Loss 0.231 Time taken: 23.07 secs| Accuracy 90.478

Epoch: 2 Train Loss 0.213 Time taken: 533.19 secs

    Epoch: 2 Validation Loss 0.169 Time taken: 23.05 secs| Accuracy 93.515

Epoch: 3 Train Loss 0.148 Time taken: 530.43 secs

    Epoch: 3 Validation Loss 0.15 Time taken: 23.18 secs| Accuracy 93.995

Epoch: 4 Train Loss 0.112 Time taken: 531.42 secs

    Epoch: 4 Validation Loss 0.147 Time taken: 23.16 secs| Accuracy 94.645

Epoch: 5 Train Loss 0.085 Time taken: 531.22 secs

    Epoch: 5 Validation Loss 0.135 Time taken: 23.21 secs| Accuracy 95.069
T\P      0       1
0       5094        0215        
1       0281        5027        

Accuracy 95.328
Time taken: 34.2 secs
2856.988 secs


On the best model
T\P      0       1
0       5094        0215        
1       0281        5027        

Accuracy 95.328
Time taken: 33.68 secs
Sat Apr 16 06:04:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=245
Sat Apr 16 06:04:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.042 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 245,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 131,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.357 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.478 Time taken: 536.65 secs

    Epoch: 1 Validation Loss 0.292 Time taken: 23.12 secs| Accuracy 88.245

Epoch: 2 Train Loss 0.214 Time taken: 532.6 secs

    Epoch: 2 Validation Loss 0.147 Time taken: 23.11 secs| Accuracy 94.207

Epoch: 3 Train Loss 0.141 Time taken: 531.56 secs

    Epoch: 3 Validation Loss 0.147 Time taken: 23.29 secs| Accuracy 94.773

Epoch: 4 Train Loss 0.109 Time taken: 540.06 secs

    Epoch: 4 Validation Loss 0.126 Time taken: 23.27 secs| Accuracy 95.253

Epoch: 5 Train Loss 0.083 Time taken: 539.91 secs

    Epoch: 5 Validation Loss 0.129 Time taken: 23.18 secs| Accuracy 95.055
T\P      0       1
0       5050        0259        
1       0191        5117        

Accuracy 95.762
Time taken: 34.07 secs
2870.268 secs


On the best model
T\P      0       1
0       5070        0239        
1       0210        5098        

Accuracy 95.771
Time taken: 33.94 secs
Sat Apr 16 06:53:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=246
Sat Apr 16 06:54:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 246,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 132,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.359 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.473 Time taken: 536.37 secs

    Epoch: 1 Validation Loss 0.223 Time taken: 23.18 secs| Accuracy 91.424

Epoch: 2 Train Loss 0.206 Time taken: 532.79 secs

    Epoch: 2 Validation Loss 0.159 Time taken: 23.21 secs| Accuracy 94.024

Epoch: 3 Train Loss 0.141 Time taken: 531.99 secs

    Epoch: 3 Validation Loss 0.129 Time taken: 23.23 secs| Accuracy 95.211

Epoch: 4 Train Loss 0.105 Time taken: 533.39 secs

    Epoch: 4 Validation Loss 0.136 Time taken: 23.18 secs| Accuracy 95.295

Epoch: 5 Train Loss 0.082 Time taken: 533.84 secs

    Epoch: 5 Validation Loss 0.142 Time taken: 23.34 secs| Accuracy 95.352
T\P      0       1
0       4968        0341        
1       0137        5171        

Accuracy 95.498
Time taken: 34.28 secs
2864.508 secs


On the best model
T\P      0       1
0       4968        0341        
1       0137        5171        

Accuracy 95.498
Time taken: 33.88 secs
Sat Apr 16 07:43:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=247
Sat Apr 16 07:43:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 247,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 133,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.481 Time taken: 535.78 secs

    Epoch: 1 Validation Loss 0.217 Time taken: 23.22 secs| Accuracy 91.806

Epoch: 2 Train Loss 0.213 Time taken: 532.93 secs

    Epoch: 2 Validation Loss 0.168 Time taken: 23.18 secs| Accuracy 93.656

Epoch: 3 Train Loss 0.145 Time taken: 532.7 secs

    Epoch: 3 Validation Loss 0.171 Time taken: 23.27 secs| Accuracy 93.939

Epoch: 4 Train Loss 0.109 Time taken: 532.83 secs

    Epoch: 4 Validation Loss 0.134 Time taken: 23.27 secs| Accuracy 95.041

Epoch: 5 Train Loss 0.091 Time taken: 532.72 secs

    Epoch: 5 Validation Loss 0.12 Time taken: 23.25 secs| Accuracy 95.733
T\P      0       1
0       5097        0212        
1       0256        5052        

Accuracy 95.592
Time taken: 33.92 secs
2863.218 secs


On the best model
T\P      0       1
0       5097        0212        
1       0256        5052        

Accuracy 95.592
Time taken: 33.88 secs
Sat Apr 16 08:32:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=248
Sat Apr 16 08:32:51 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.175 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 248,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 134,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.358 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.518 Time taken: 533.77 secs

    Epoch: 1 Validation Loss 0.34 Time taken: 22.8 secs| Accuracy 87.228

Epoch: 2 Train Loss 0.223 Time taken: 532.52 secs

    Epoch: 2 Validation Loss 0.156 Time taken: 22.76 secs| Accuracy 93.868

Epoch: 3 Train Loss 0.148 Time taken: 532.26 secs

    Epoch: 3 Validation Loss 0.152 Time taken: 22.98 secs| Accuracy 94.532

Epoch: 4 Train Loss 0.112 Time taken: 532.44 secs

    Epoch: 4 Validation Loss 0.137 Time taken: 22.88 secs| Accuracy 95.126

Epoch: 5 Train Loss 0.087 Time taken: 531.74 secs

    Epoch: 5 Validation Loss 0.137 Time taken: 22.92 secs| Accuracy 95.112
T\P      0       1
0       4952        0357        
1       0154        5154        

Accuracy 95.187
Time taken: 33.7 secs
2854.552 secs


On the best model
T\P      0       1
0       4966        0343        
1       0179        5129        

Accuracy 95.083
Time taken: 33.39 secs
Sat Apr 16 09:22:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=249
Sat Apr 16 09:22:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.879 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 249,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 135,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.361 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.471 Time taken: 534.12 secs

    Epoch: 1 Validation Loss 0.238 Time taken: 22.73 secs| Accuracy 91.043

Epoch: 2 Train Loss 0.219 Time taken: 531.64 secs

    Epoch: 2 Validation Loss 0.162 Time taken: 22.76 secs| Accuracy 93.868

Epoch: 3 Train Loss 0.149 Time taken: 538.37 secs

    Epoch: 3 Validation Loss 0.148 Time taken: 22.69 secs| Accuracy 94.532

Epoch: 4 Train Loss 0.113 Time taken: 529.21 secs

    Epoch: 4 Validation Loss 0.135 Time taken: 22.79 secs| Accuracy 94.914

Epoch: 5 Train Loss 0.087 Time taken: 529.36 secs

    Epoch: 5 Validation Loss 0.124 Time taken: 22.84 secs| Accuracy 95.691
T\P      0       1
0       5006        0303        
1       0145        5163        

Accuracy 95.78
Time taken: 33.44 secs
2851.054 secs


On the best model
T\P      0       1
0       5006        0303        
1       0145        5163        

Accuracy 95.78
Time taken: 33.39 secs
Sat Apr 16 10:11:09 2022




======================================================================
----------------------------------------------------------------------
                run_ID=250
Sat Apr 16 10:16:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.683 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 250,
    "message": "236 different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 136,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.387 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.54 Time taken: 531.33 secs

    Epoch: 1 Validation Loss 0.263 Time taken: 23.14 secs| Accuracy 89.672

Epoch: 2 Train Loss 0.235 Time taken: 533.88 secs

    Epoch: 2 Validation Loss 0.157 Time taken: 23.33 secs| Accuracy 93.812

Epoch: 3 Train Loss 0.152 Time taken: 533.03 secs

    Epoch: 3 Validation Loss 0.137 Time taken: 23.41 secs| Accuracy 94.886

Epoch: 4 Train Loss 0.114 Time taken: 533.69 secs

    Epoch: 4 Validation Loss 0.128 Time taken: 23.37 secs| Accuracy 95.493

Epoch: 5 Train Loss 0.085 Time taken: 539.59 secs

    Epoch: 5 Validation Loss 0.124 Time taken: 23.35 secs| Accuracy 95.804
T\P      0       1
0       4993        0316        
1       0159        5149        

Accuracy 95.526
Time taken: 33.92 secs
2865.655 secs


On the best model
T\P      0       1
0       4993        0316        
1       0159        5149        

Accuracy 95.526
Time taken: 33.96 secs
Sat Apr 16 11:05:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=257
Sat Apr 16 11:05:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 70783 | Validation_set: 28306 | Test_set: 42471
Train_batches: 2212 | Validation_batches: 885 | Test_batches: 1328
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.27 GB
46 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 257,
    "message": "188 with repeated negatives six times",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 6,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.342 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.082 Time taken: 1260.46 secs

    Epoch: 1 Validation Loss 0.034 Time taken: 88.63 secs| Macro F: 0.979

Epoch: 2 Train Loss 0.029 Time taken: 1261.87 secs

    Epoch: 2 Validation Loss 0.023 Time taken: 88.68 secs| Macro F: 0.987

Epoch: 3 Train Loss 0.018 Time taken: 1258.77 secs

    Epoch: 3 Validation Loss 0.019 Time taken: 88.41 secs| Macro F: 0.989

Epoch: 4 Train Loss 0.012 Time taken: 1258.4 secs

    Epoch: 4 Validation Loss 0.016 Time taken: 88.7 secs| Macro F: 0.988

Epoch: 5 Train Loss 0.008 Time taken: 1263.01 secs

    Epoch: 5 Validation Loss 0.015 Time taken: 88.8 secs| Macro F: 0.992
T\P      0       1
0       37117       0046        
1       0124        5184        

Macro F: 0.991
Time taken: 132.71 secs
6918.101 secs


On the best model
T\P      0       1
0       37117       0046        
1       0124        5184        

Macro F: 0.991
Time taken: 133.14 secs
Sat Apr 16 13:04:33 2022



======================================================================
----------------------------------------------------------------------
                run_ID=256
Sat Apr 16 14:37:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 61935 | Validation_set: 24768 | Test_set: 37162
Train_batches: 1936 | Validation_batches: 774 | Test_batches: 1162
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.89 GB
39 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 256,
    "message": "254 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 5,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.347 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.084 Time taken: 1156.16 secs

    Epoch: 1 Validation Loss 0.026 Time taken: 77.61 secs| Macro F: 0.986

Epoch: 2 Train Loss 0.033 Time taken: 1151.4 secs

    Epoch: 2 Validation Loss 0.021 Time taken: 77.83 secs| Macro F: 0.989

Epoch: 3 Train Loss 0.022 Time taken: 1152.19 secs

    Epoch: 3 Validation Loss 0.018 Time taken: 78.02 secs| Macro F: 0.99

Epoch: 4 Train Loss 0.017 Time taken: 1147.95 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 77.61 secs| Macro F: 0.978

Epoch: 5 Train Loss 0.012 Time taken: 1148.59 secs

    Epoch: 5 Validation Loss 0.021 Time taken: 77.55 secs| Macro F: 0.99
T\P      0       1
0       31725       0129        
1       0082        5226        

Macro F: 0.988
Time taken: 115.42 secs
6305.61 secs


On the best model
T\P      0       1
0       31749       0105        
1       0083        5225        

Macro F: 0.99
Time taken: 115.7 secs
Sat Apr 16 16:25:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=294
Sat Apr 16 16:25:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.042 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 294,
    "message": "188 repeat with different learning rate | rseed constant",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-07,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.385 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-07
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.693 Time taken: 594.74 secs

    Epoch: 1 Validation Loss 0.693 Time taken: 22.93 secs| Accuracy 49.986

Epoch: 2 Train Loss 0.692 Time taken: 599.62 secs

    Epoch: 2 Validation Loss 0.687 Time taken: 23.09 secs| Accuracy 71.729

Epoch: 3 Train Loss 0.673 Time taken: 604.59 secs

    Epoch: 3 Validation Loss 0.629 Time taken: 23.01 secs| Accuracy 89.799

Epoch: 4 Train Loss 0.644 Time taken: 599.33 secs

    Epoch: 4 Validation Loss 0.577 Time taken: 23.14 secs| Accuracy 91.212

Epoch: 5 Train Loss 0.616 Time taken: 602.32 secs

    Epoch: 5 Validation Loss 0.537 Time taken: 23.01 secs| Accuracy 93.444
T\P      0       1
0       5291        0018        
1       0699        4609        

Accuracy 93.247
Time taken: 33.84 secs
3208.1 secs


On the best model
T\P      0       1
0       5291        0018        
1       0699        4609        

Accuracy 93.247
Time taken: 34.01 secs
Sat Apr 16 17:20:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=295
Sat Apr 16 17:20:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.102 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 295,
    "message": "188 repeat with indic bert | rseed constant",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "ai4bharat/indic-bert",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): AlbertMode ai4bharat/indic-bert )
Memory taken on GPU 0.127 GB
Memory taken on RAM 4.148 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.378 Time taken: 574.77 secs

    Epoch: 1 Validation Loss 0.118 Time taken: 23.15 secs| Accuracy 96.426

Epoch: 2 Train Loss 0.107 Time taken: 580.19 secs

    Epoch: 2 Validation Loss 0.093 Time taken: 23.24 secs| Accuracy 97.061

Epoch: 3 Train Loss 0.075 Time taken: 578.06 secs

    Epoch: 3 Validation Loss 0.093 Time taken: 23.14 secs| Accuracy 96.793

Epoch: 4 Train Loss 0.051 Time taken: 578.58 secs

    Epoch: 4 Validation Loss 0.084 Time taken: 23.22 secs| Accuracy 97.485

Epoch: 5 Train Loss 0.033 Time taken: 577.7 secs

    Epoch: 5 Validation Loss 0.096 Time taken: 23.31 secs| Accuracy 97.598
T\P      0       1
0       5229        0080        
1       0158        5150        

Accuracy 97.758
Time taken: 34.34 secs
3044.17 secs


On the best model
T\P      0       1
0       5229        0080        
1       0158        5150        

Accuracy 97.758
Time taken: 34.34 secs
Sat Apr 16 18:12:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=296
Sat Apr 16 18:13:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.883 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 296,
    "message": "188 repeat with mbert | rseed constant",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.082 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.153 Time taken: 593.9 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 23.32 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.048 Time taken: 595.67 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.16 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.026 Time taken: 589.11 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.37 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.019 Time taken: 588.9 secs

    Epoch: 4 Validation Loss 0.056 Time taken: 23.29 secs| Accuracy 98.446

Epoch: 5 Train Loss 0.012 Time taken: 588.3 secs

    Epoch: 5 Validation Loss 0.052 Time taken: 23.28 secs| Accuracy 98.601
T\P      0       1
0       5221        0088        
1       0047        5261        

Accuracy 98.728
Time taken: 34.13 secs
3126.629 secs


On the best model
T\P      0       1
0       5239        0070        
1       0064        5244        

Accuracy 98.738
Time taken: 34.13 secs
Sat Apr 16 19:06:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=297
Sat Apr 16 19:06:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.883 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 297,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.607 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.433 Time taken: 598.47 secs

    Epoch: 1 Validation Loss 0.195 Time taken: 23.36 secs| Accuracy 93.077

Epoch: 2 Train Loss 0.161 Time taken: 595.46 secs

    Epoch: 2 Validation Loss 0.123 Time taken: 23.51 secs| Accuracy 95.493

Epoch: 3 Train Loss 0.098 Time taken: 596.16 secs

    Epoch: 3 Validation Loss 0.136 Time taken: 23.32 secs| Accuracy 94.984

Epoch: 4 Train Loss 0.072 Time taken: 596.02 secs

    Epoch: 4 Validation Loss 0.121 Time taken: 23.46 secs| Accuracy 96.086

Epoch: 5 Train Loss 0.046 Time taken: 595.06 secs

    Epoch: 5 Validation Loss 0.119 Time taken: 23.37 secs| Accuracy 96.327
T\P      0       1
0       5062        0247        
1       0144        5164        

Accuracy 96.317
Time taken: 34.28 secs
3176.209 secs



======================================================================
----------------------------------------------------------------------
                run_ID=298
Sat Apr 16 21:51:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.078 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 298,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.191 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.494 Time taken: 595.59 secs

    Epoch: 1 Validation Loss 0.23 Time taken: 23.47 secs| Accuracy 90.562

Epoch: 2 Train Loss 0.187 Time taken: 592.02 secs

    Epoch: 2 Validation Loss 0.152 Time taken: 23.43 secs| Accuracy 93.882

Epoch: 3 Train Loss 0.115 Time taken: 594.04 secs

    Epoch: 3 Validation Loss 0.135 Time taken: 23.41 secs| Accuracy 95.013

Epoch: 4 Train Loss 0.081 Time taken: 593.23 secs

    Epoch: 4 Validation Loss 0.122 Time taken: 23.43 secs| Accuracy 95.38

Epoch: 5 Train Loss 0.054 Time taken: 593.36 secs

    Epoch: 5 Validation Loss 0.122 Time taken: 23.32 secs| Accuracy 95.988
T\P      0       1
0       4933        0376        
1       0119        5189        

Accuracy 95.338
Time taken: 34.2 secs
3168.982 secs


On the best model
T\P      0       1
0       4933        0376        
1       0119        5189        

Accuracy 95.338
Time taken: 34.17 secs
Sat Apr 16 22:45:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=299
Sat Apr 16 22:45:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.041 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 299,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.235 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.491 Time taken: 595.59 secs

    Epoch: 1 Validation Loss 0.219 Time taken: 23.3 secs| Accuracy 91.763

Epoch: 2 Train Loss 0.183 Time taken: 592.24 secs

    Epoch: 2 Validation Loss 0.226 Time taken: 23.22 secs| Accuracy 91.735

Epoch: 3 Train Loss 0.114 Time taken: 592.09 secs

    Epoch: 3 Validation Loss 0.123 Time taken: 23.17 secs| Accuracy 95.705

Epoch: 4 Train Loss 0.075 Time taken: 591.32 secs

    Epoch: 4 Validation Loss 0.113 Time taken: 23.13 secs| Accuracy 96.002

Epoch: 5 Train Loss 0.055 Time taken: 591.2 secs

    Epoch: 5 Validation Loss 0.126 Time taken: 23.12 secs| Accuracy 96.044
T\P      0       1
0       4977        0332        
1       0109        5199        

Accuracy 95.846
Time taken: 33.88 secs
3154.835 secs


On the best model
T\P      0       1
0       4977        0332        
1       0109        5199        

Accuracy 95.846
Time taken: 34.05 secs
Sat Apr 16 23:40:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=300
Sat Apr 16 23:40:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.056 GB
34 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 300,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.354 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.433 Time taken: 595.9 secs

    Epoch: 1 Validation Loss 0.192 Time taken: 23.51 secs| Accuracy 92.696

Epoch: 2 Train Loss 0.157 Time taken: 593.74 secs

    Epoch: 2 Validation Loss 0.121 Time taken: 23.48 secs| Accuracy 95.338

Epoch: 3 Train Loss 0.101 Time taken: 594.32 secs

    Epoch: 3 Validation Loss 0.126 Time taken: 23.41 secs| Accuracy 95.394

Epoch: 4 Train Loss 0.069 Time taken: 593.82 secs

    Epoch: 4 Validation Loss 0.129 Time taken: 23.66 secs| Accuracy 96.242

Epoch: 5 Train Loss 0.047 Time taken: 595.5 secs

    Epoch: 5 Validation Loss 0.134 Time taken: 23.38 secs| Accuracy 95.691
T\P      0       1
0       5193        0116        
1       0306        5002        

Accuracy 96.025
Time taken: 34.36 secs
3169.606 secs


On the best model
T\P      0       1
0       5011        0298        
1       0123        5185        

Accuracy 96.035
Time taken: 34.11 secs
Sun Apr 17 00:34:53 2022


======================================================================
----------------------------------------------------------------------
                run_ID=301
Sun Apr 17 00:34:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.233 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 301,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 0.029 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.551 Time taken: 596.84 secs

    Epoch: 1 Validation Loss 0.379 Time taken: 23.16 secs| Accuracy 85.999

Epoch: 2 Train Loss 0.299 Time taken: 593.91 secs

    Epoch: 2 Validation Loss 0.188 Time taken: 23.14 secs| Accuracy 92.922

Epoch: 3 Train Loss 0.172 Time taken: 593.48 secs

    Epoch: 3 Validation Loss 0.15 Time taken: 23.16 secs| Accuracy 94.843

Epoch: 4 Train Loss 0.117 Time taken: 593.02 secs

    Epoch: 4 Validation Loss 0.168 Time taken: 23.25 secs| Accuracy 94.603

Epoch: 5 Train Loss 0.081 Time taken: 593.88 secs

    Epoch: 5 Validation Loss 0.124 Time taken: 23.32 secs| Accuracy 95.493
T\P      0       1
0       5001        0308        
1       0190        5118        

Accuracy 95.309
Time taken: 34.14 secs
3166.805 secs


On the best model
T\P      0       1
0       5001        0308        
1       0190        5118        

Accuracy 95.309
Time taken: 33.77 secs
Sun Apr 17 01:29:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=302
Sun Apr 17 01:29:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.227 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 302,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -0.487 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.545 Time taken: 595.39 secs

    Epoch: 1 Validation Loss 0.287 Time taken: 22.96 secs| Accuracy 88.528

Epoch: 2 Train Loss 0.208 Time taken: 594.64 secs

    Epoch: 2 Validation Loss 0.141 Time taken: 23.1 secs| Accuracy 94.914

Epoch: 3 Train Loss 0.121 Time taken: 594.22 secs

    Epoch: 3 Validation Loss 0.13 Time taken: 23.32 secs| Accuracy 95.069

Epoch: 4 Train Loss 0.078 Time taken: 595.28 secs

    Epoch: 4 Validation Loss 0.165 Time taken: 23.27 secs| Accuracy 93.953

Epoch: 5 Train Loss 0.058 Time taken: 594.24 secs

    Epoch: 5 Validation Loss 0.132 Time taken: 23.25 secs| Accuracy 95.945
T\P      0       1
0       5048        0261        
1       0161        5147        

Accuracy 96.025
Time taken: 34.0 secs
3170.118 secs


On the best model
T\P      0       1
0       5048        0261        
1       0161        5147        

Accuracy 96.025
Time taken: 33.92 secs
Sun Apr 17 02:23:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=303
Sun Apr 17 02:23:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.211 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 303,
    "message": "166 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.54 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.448 Time taken: 597.28 secs

    Epoch: 1 Validation Loss 0.209 Time taken: 23.17 secs| Accuracy 91.89

Epoch: 2 Train Loss 0.169 Time taken: 594.05 secs

    Epoch: 2 Validation Loss 0.149 Time taken: 23.34 secs| Accuracy 94.674

Epoch: 3 Train Loss 0.107 Time taken: 593.61 secs

    Epoch: 3 Validation Loss 0.138 Time taken: 23.36 secs| Accuracy 95.437

Epoch: 4 Train Loss 0.072 Time taken: 593.27 secs

    Epoch: 4 Validation Loss 0.131 Time taken: 23.5 secs| Accuracy 95.578

Epoch: 5 Train Loss 0.058 Time taken: 594.5 secs

    Epoch: 5 Validation Loss 0.13 Time taken: 23.43 secs| Accuracy 95.959
T\P      0       1
0       5137        0172        
1       0249        5059        

Accuracy 96.035
Time taken: 34.24 secs
3193.858 secs


On the best model
T\P      0       1
0       5137        0172        
1       0249        5059        

Accuracy 96.035
Time taken: 34.13 secs
Sun Apr 17 03:18:52 2022



======================================================================
----------------------------------------------------------------------
                run_ID=304
Sun Apr 17 11:49:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.466 GB
27 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 304,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.092 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.389 Time taken: 601.46 secs

    Epoch: 1 Validation Loss 0.22 Time taken: 23.73 secs| Accuracy 90.93

Epoch: 2 Train Loss 0.158 Time taken: 598.5 secs

    Epoch: 2 Validation Loss 0.14 Time taken: 23.26 secs| Accuracy 95.013

Epoch: 3 Train Loss 0.099 Time taken: 600.22 secs

    Epoch: 3 Validation Loss 0.149 Time taken: 23.19 secs| Accuracy 94.66

Epoch: 4 Train Loss 0.071 Time taken: 593.66 secs

    Epoch: 4 Validation Loss 0.132 Time taken: 23.31 secs| Accuracy 95.535

Epoch: 5 Train Loss 0.047 Time taken: 595.67 secs

    Epoch: 5 Validation Loss 0.144 Time taken: 23.31 secs| Accuracy 95.648
T\P      0       1
0       4997        0312        
1       0150        5158        

Accuracy 95.648
Time taken: 34.0 secs
3140.842 secs



======================================================================
----------------------------------------------------------------------
                run_ID=305
Sun Apr 17 16:07:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.71 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 305,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.277 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.421 Time taken: 598.52 secs

    Epoch: 1 Validation Loss 0.195 Time taken: 23.21 secs| Accuracy 91.806

Epoch: 2 Train Loss 0.163 Time taken: 602.28 secs

    Epoch: 2 Validation Loss 0.126 Time taken: 23.18 secs| Accuracy 95.041

Epoch: 3 Train Loss 0.104 Time taken: 598.0 secs

    Epoch: 3 Validation Loss 0.126 Time taken: 23.63 secs| Accuracy 95.422

Epoch: 4 Train Loss 0.07 Time taken: 598.46 secs

    Epoch: 4 Validation Loss 0.121 Time taken: 23.62 secs| Accuracy 95.606

Epoch: 5 Train Loss 0.051 Time taken: 598.82 secs

    Epoch: 5 Validation Loss 0.124 Time taken: 23.62 secs| Accuracy 96.044
T\P      0       1
0       5090        0219        
1       0192        5116        

Accuracy 96.129
Time taken: 34.46 secs
3148.187 secs

Sun Apr 17 17:01:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=306
Sun Apr 17 17:01:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.081 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 306,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 10.264 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.377 Time taken: 599.16 secs

    Epoch: 1 Validation Loss 0.21 Time taken: 23.23 secs| Accuracy 90.972

Epoch: 2 Train Loss 0.158 Time taken: 597.35 secs

    Epoch: 2 Validation Loss 0.137 Time taken: 23.23 secs| Accuracy 94.688

Epoch: 3 Train Loss 0.099 Time taken: 597.16 secs

    Epoch: 3 Validation Loss 0.125 Time taken: 23.24 secs| Accuracy 95.733

Epoch: 4 Train Loss 0.064 Time taken: 597.4 secs

    Epoch: 4 Validation Loss 0.118 Time taken: 23.21 secs| Accuracy 96.199

Epoch: 5 Train Loss 0.048 Time taken: 596.9 secs

    Epoch: 5 Validation Loss 0.114 Time taken: 23.34 secs| Accuracy 96.072
T\P      0       1
0       5120        0189        
1       0194        5114        

Accuracy 96.393
Time taken: 34.2 secs
3138.943 secs

Sun Apr 17 17:54:24 2022


======================================================================
----------------------------------------------------------------------
                run_ID=307
Sun Apr 17 17:54:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.488 GB
27 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 307,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 10.925 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.391 Time taken: 596.84 secs

    Epoch: 1 Validation Loss 0.16 Time taken: 23.2 secs| Accuracy 93.586

Epoch: 2 Train Loss 0.156 Time taken: 593.81 secs

    Epoch: 2 Validation Loss 0.142 Time taken: 23.34 secs| Accuracy 95.055

Epoch: 3 Train Loss 0.1 Time taken: 593.64 secs

    Epoch: 3 Validation Loss 0.13 Time taken: 23.31 secs| Accuracy 95.182

Epoch: 4 Train Loss 0.071 Time taken: 593.23 secs

    Epoch: 4 Validation Loss 0.159 Time taken: 23.39 secs| Accuracy 94.546

Epoch: 5 Train Loss 0.054 Time taken: 594.72 secs

    Epoch: 5 Validation Loss 0.11 Time taken: 23.44 secs| Accuracy 96.411
T\P      0       1
0       5018        0291        
1       0115        5193        

Accuracy 96.176
Time taken: 34.21 secs
3123.667 secs

Sun Apr 17 18:47:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=308
Sun Apr 17 18:47:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.92 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 308,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM -1.504 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.368 Time taken: 601.34 secs

    Epoch: 1 Validation Loss 0.171 Time taken: 23.32 secs| Accuracy 93.727

Epoch: 2 Train Loss 0.154 Time taken: 599.87 secs

    Epoch: 2 Validation Loss 0.147 Time taken: 23.55 secs| Accuracy 94.787

Epoch: 3 Train Loss 0.099 Time taken: 600.38 secs

    Epoch: 3 Validation Loss 0.126 Time taken: 23.6 secs| Accuracy 95.592

Epoch: 4 Train Loss 0.065 Time taken: 599.29 secs

    Epoch: 4 Validation Loss 0.132 Time taken: 23.56 secs| Accuracy 95.903

Epoch: 5 Train Loss 0.05 Time taken: 597.73 secs

    Epoch: 5 Validation Loss 0.147 Time taken: 23.58 secs| Accuracy 96.101
T\P      0       1
0       5126        0183        
1       0209        5099        

Accuracy 96.308
Time taken: 34.52 secs
3151.425 secs

Sun Apr 17 19:41:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=309
Sun Apr 17 19:41:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.367 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 309,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 10.181 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.413 Time taken: 594.92 secs

    Epoch: 1 Validation Loss 0.178 Time taken: 23.32 secs| Accuracy 92.738

Epoch: 2 Train Loss 0.152 Time taken: 592.47 secs

    Epoch: 2 Validation Loss 0.131 Time taken: 23.43 secs| Accuracy 95.154

Epoch: 3 Train Loss 0.096 Time taken: 594.66 secs

    Epoch: 3 Validation Loss 0.111 Time taken: 23.43 secs| Accuracy 95.691

Epoch: 4 Train Loss 0.067 Time taken: 592.81 secs

    Epoch: 4 Validation Loss 0.126 Time taken: 23.52 secs| Accuracy 95.62

Epoch: 5 Train Loss 0.047 Time taken: 594.79 secs

    Epoch: 5 Validation Loss 0.151 Time taken: 23.64 secs| Accuracy 95.931
T\P      0       1
0       4960        0349        
1       0086        5222        

Accuracy 95.903
Time taken: 34.79 secs
3122.244 secs

Sun Apr 17 20:34:07 2022



======================================================================
----------------------------------------------------------------------
                run_ID=310
Sun Apr 17 20:53:57 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 7.394 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 310,
    "message": "115 with different seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 8.626 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.441 Time taken: 604.54 secs

    Epoch: 1 Validation Loss 0.2 Time taken: 23.09 secs| Accuracy 91.325

Epoch: 2 Train Loss 0.173 Time taken: 598.0 secs

    Epoch: 2 Validation Loss 0.132 Time taken: 23.21 secs| Accuracy 94.999

Epoch: 3 Train Loss 0.11 Time taken: 597.83 secs

    Epoch: 3 Validation Loss 0.132 Time taken: 23.16 secs| Accuracy 96.058

Epoch: 4 Train Loss 0.071 Time taken: 597.48 secs

    Epoch: 4 Validation Loss 0.131 Time taken: 23.29 secs| Accuracy 95.846

Epoch: 5 Train Loss 0.055 Time taken: 598.89 secs

    Epoch: 5 Validation Loss 0.145 Time taken: 23.28 secs| Accuracy 95.437
T\P      0       1
0       4887        0422        
1       0081        5227        

Accuracy 95.262
Time taken: 34.14 secs
3147.31 secs

Sun Apr 17 21:47:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=351
Sun Apr 17 21:47:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.009 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 351,
    "message": "EL0L1L2 frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.361 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.262 Time taken: 564.04 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.34 secs| Accuracy 98.121

Epoch: 2 Train Loss 0.079 Time taken: 569.45 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.36 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.057 Time taken: 569.25 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 23.58 secs| Accuracy 98.799

Epoch: 4 Train Loss 0.044 Time taken: 566.45 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 23.37 secs| Accuracy 98.714

Epoch: 5 Train Loss 0.034 Time taken: 559.36 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 23.32 secs| Accuracy 98.997
T\P      0       1
0       5246        0063        
1       0052        5256        

Accuracy 98.917
Time taken: 34.33 secs
2980.487 secs

Sun Apr 17 22:38:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=352
Sun Apr 17 22:38:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.177 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 352,
    "message": "351 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.236 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.282 Time taken: 564.06 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.32 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.089 Time taken: 567.82 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.26 secs| Accuracy 98.517

Epoch: 3 Train Loss 0.061 Time taken: 568.73 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.25 secs| Accuracy 98.559

Epoch: 4 Train Loss 0.043 Time taken: 567.65 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 23.19 secs| Accuracy 98.785

Epoch: 5 Train Loss 0.035 Time taken: 560.23 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.48 secs| Accuracy 98.955
T\P      0       1
0       5249        0060        
1       0038        5270        

Accuracy 99.077
Time taken: 34.45 secs
2979.897 secs

Sun Apr 17 23:28:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=353
Sun Apr 17 23:28:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.227 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 353,
    "message": "351 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.19 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.294 Time taken: 562.16 secs

    Epoch: 1 Validation Loss 0.082 Time taken: 23.17 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.096 Time taken: 562.16 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.36 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.069 Time taken: 568.23 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.25 secs| Accuracy 98.7

Epoch: 4 Train Loss 0.049 Time taken: 562.25 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.19 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.039 Time taken: 562.74 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 23.34 secs| Accuracy 98.813
T\P      0       1
0       5230        0079        
1       0040        5268        

Accuracy 98.879
Time taken: 34.29 secs
2968.654 secs

Mon Apr 18 00:19:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=354
Mon Apr 18 00:19:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.164 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 354,
    "message": "351 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.366 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.224 Time taken: 564.23 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.53 secs| Accuracy 98.432

Epoch: 2 Train Loss 0.067 Time taken: 568.09 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.43 secs| Accuracy 98.517

Epoch: 3 Train Loss 0.049 Time taken: 561.82 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.89 secs| Accuracy 98.912

Epoch: 4 Train Loss 0.041 Time taken: 561.92 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 23.76 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.027 Time taken: 562.46 secs

    Epoch: 5 Validation Loss 0.056 Time taken: 23.84 secs| Accuracy 98.672
T\P      0       1
0       5213        0096        
1       0042        5266        

Accuracy 98.7
Time taken: 34.68 secs
2971.93 secs

Mon Apr 18 01:09:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=355
Mon Apr 18 01:10:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.176 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 355,
    "message": "351 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.312 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.264 Time taken: 564.63 secs

    Epoch: 1 Validation Loss 0.08 Time taken: 23.39 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.076 Time taken: 562.36 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.7 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.048 Time taken: 562.11 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 23.44 secs| Accuracy 98.545

Epoch: 4 Train Loss 0.038 Time taken: 561.69 secs

    Epoch: 4 Validation Loss 0.042 Time taken: 23.84 secs| Accuracy 98.87

Epoch: 5 Train Loss 0.031 Time taken: 562.58 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 23.47 secs| Accuracy 98.87
T\P      0       1
0       5234        0075        
1       0047        5261        

Accuracy 98.851
Time taken: 34.49 secs
2966.188 secs

Mon Apr 18 02:00:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=356
Mon Apr 18 02:00:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.042 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 356,
    "message": "E frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.329 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.172 Time taken: 584.25 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 23.24 secs| Accuracy 98.135

Epoch: 2 Train Loss 0.062 Time taken: 589.69 secs

    Epoch: 2 Validation Loss 0.035 Time taken: 23.28 secs| Accuracy 98.912

Epoch: 3 Train Loss 0.041 Time taken: 581.73 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 23.35 secs| Accuracy 98.926

Epoch: 4 Train Loss 0.034 Time taken: 582.44 secs

    Epoch: 4 Validation Loss 0.06 Time taken: 23.33 secs| Accuracy 98.079

Epoch: 5 Train Loss 0.023 Time taken: 582.43 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 23.28 secs| Accuracy 99.237
T\P      0       1
0       5272        0037        
1       0054        5254        

Accuracy 99.143
Time taken: 34.21 secs
3071.661 secs

Mon Apr 18 02:52:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=357
Mon Apr 18 02:52:47 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.038 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 357,
    "message": "356 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.364 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.216 Time taken: 589.26 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.24 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.082 Time taken: 593.01 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 23.41 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.053 Time taken: 585.31 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.53 secs| Accuracy 98.728

Epoch: 4 Train Loss 0.04 Time taken: 584.84 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 23.58 secs| Accuracy 98.079

Epoch: 5 Train Loss 0.028 Time taken: 584.57 secs

    Epoch: 5 Validation Loss 0.052 Time taken: 23.54 secs| Accuracy 98.813
T\P      0       1
0       5286        0023        
1       0091        5217        

Accuracy 98.926
Time taken: 34.45 secs
3089.404 secs

Mon Apr 18 03:45:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=358
Mon Apr 18 03:45:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.763 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 358,
    "message": "356 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.085 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.171 Time taken: 588.25 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 23.33 secs| Accuracy 98.446

Epoch: 2 Train Loss 0.066 Time taken: 593.29 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.33 secs| Accuracy 98.036

Epoch: 3 Train Loss 0.044 Time taken: 594.25 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.35 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.034 Time taken: 586.38 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 23.5 secs| Accuracy 99.082

Epoch: 5 Train Loss 0.025 Time taken: 586.09 secs

    Epoch: 5 Validation Loss 0.067 Time taken: 23.36 secs| Accuracy 98.248
T\P      0       1
0       5155        0154        
1       0027        5281        

Accuracy 98.295
Time taken: 34.36 secs
3099.911 secs

Mon Apr 18 04:38:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=359
Mon Apr 18 04:38:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.035 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 359,
    "message": "356 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.384 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.181 Time taken: 587.0 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.43 secs| Accuracy 98.121

Epoch: 2 Train Loss 0.067 Time taken: 592.05 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.34 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.044 Time taken: 596.09 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.37 secs| Accuracy 98.827

Epoch: 4 Train Loss 0.035 Time taken: 585.45 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.44 secs| Accuracy 98.728

Epoch: 5 Train Loss 0.025 Time taken: 585.48 secs

    Epoch: 5 Validation Loss 0.055 Time taken: 23.46 secs| Accuracy 98.559
T\P      0       1
0       5208        0101        
1       0037        5271        

Accuracy 98.7
Time taken: 34.5 secs
3098.289 secs

Mon Apr 18 05:30:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=360
Mon Apr 18 05:30:45 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.045 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 360,
    "message": "356 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.359 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.151 Time taken: 588.15 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 23.27 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.062 Time taken: 589.11 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.18 secs| Accuracy 98.63

Epoch: 3 Train Loss 0.039 Time taken: 584.17 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.45 secs| Accuracy 98.827

Epoch: 4 Train Loss 0.031 Time taken: 592.36 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.28 secs| Accuracy 98.714

Epoch: 5 Train Loss 0.024 Time taken: 584.35 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 23.45 secs| Accuracy 98.813
T\P      0       1
0       5255        0054        
1       0055        5253        

Accuracy 98.973
Time taken: 34.23 secs
3089.47 secs

Mon Apr 18 06:23:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=361
Mon Apr 18 06:23:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.333 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 361,
    "message": "E frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.194 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 586.99 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 23.52 secs| Accuracy 98.389

Epoch: 2 Train Loss 0.064 Time taken: 592.78 secs

    Epoch: 2 Validation Loss 0.036 Time taken: 23.38 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.043 Time taken: 586.36 secs

    Epoch: 3 Validation Loss 0.037 Time taken: 23.39 secs| Accuracy 98.955

Epoch: 4 Train Loss 0.03 Time taken: 587.07 secs

    Epoch: 4 Validation Loss 0.028 Time taken: 23.33 secs| Accuracy 99.124

Epoch: 5 Train Loss 0.022 Time taken: 583.71 secs

    Epoch: 5 Validation Loss 0.029 Time taken: 23.51 secs| Accuracy 99.166
T\P      0       1
0       5254        0055        
1       0040        5268        

Accuracy 99.105
Time taken: 34.35 secs
3088.646 secs

Mon Apr 18 07:15:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=362
Mon Apr 18 07:15:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.154 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 362,
    "message": "361 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.731 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.286 Time taken: 590.21 secs

    Epoch: 1 Validation Loss 0.112 Time taken: 23.25 secs| Accuracy 97.005

Epoch: 2 Train Loss 0.112 Time taken: 593.31 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 23.34 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.076 Time taken: 590.13 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.36 secs| Accuracy 98.531

Epoch: 4 Train Loss 0.062 Time taken: 587.42 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 23.22 secs| Accuracy 98.672

Epoch: 5 Train Loss 0.048 Time taken: 586.67 secs

    Epoch: 5 Validation Loss 0.05 Time taken: 23.3 secs| Accuracy 98.615
T\P      0       1
0       5247        0062        
1       0053        5255        

Accuracy 98.917
Time taken: 34.18 secs
3098.832 secs

Mon Apr 18 08:08:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=363
Mon Apr 18 08:08:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.103 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 363,
    "message": "361 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.292 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 585.53 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 23.2 secs| Accuracy 97.598

Epoch: 2 Train Loss 0.111 Time taken: 589.27 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 23.24 secs| Accuracy 97.966

Epoch: 3 Train Loss 0.082 Time taken: 589.9 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 23.2 secs| Accuracy 98.446

Epoch: 4 Train Loss 0.06 Time taken: 593.48 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.32 secs| Accuracy 98.545

Epoch: 5 Train Loss 0.046 Time taken: 583.4 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 23.15 secs| Accuracy 98.545
T\P      0       1
0       5222        0087        
1       0046        5262        

Accuracy 98.747
Time taken: 33.99 secs
3092.122 secs

Mon Apr 18 09:01:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=364
Mon Apr 18 09:01:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.033 GB
32 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 364,
    "message": "361 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.413 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.287 Time taken: 587.68 secs

    Epoch: 1 Validation Loss 0.12 Time taken: 23.09 secs| Accuracy 93.868

Epoch: 2 Train Loss 0.109 Time taken: 588.9 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 23.38 secs| Accuracy 98.192

Epoch: 3 Train Loss 0.077 Time taken: 590.73 secs

    Epoch: 3 Validation Loss 0.072 Time taken: 23.27 secs| Accuracy 97.344

Epoch: 4 Train Loss 0.057 Time taken: 583.15 secs

    Epoch: 4 Validation Loss 0.037 Time taken: 23.42 secs| Accuracy 98.955

Epoch: 5 Train Loss 0.04 Time taken: 582.77 secs

    Epoch: 5 Validation Loss 0.062 Time taken: 23.45 secs| Accuracy 98.163
T\P      0       1
0       5169        0140        
1       0028        5280        

Accuracy 98.418
Time taken: 34.5 secs
3084.774 secs

Mon Apr 18 09:53:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=365
Mon Apr 18 09:53:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.02 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 365,
    "message": "361 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.353 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.221 Time taken: 584.18 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.09 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.078 Time taken: 582.24 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.23 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.053 Time taken: 587.48 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.17 secs| Accuracy 98.714

Epoch: 4 Train Loss 0.039 Time taken: 582.49 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 23.35 secs| Accuracy 98.093

Epoch: 5 Train Loss 0.027 Time taken: 583.2 secs

    Epoch: 5 Validation Loss 0.041 Time taken: 23.23 secs| Accuracy 98.714
T\P      0       1
0       5230        0079        
1       0039        5269        

Accuracy 98.889
Time taken: 34.07 secs
3070.284 secs

Mon Apr 18 10:45:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=366
Mon Apr 18 10:45:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.046 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 366,
    "message": "E frozen with seed of 188",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.368 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.269 Time taken: 591.24 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 23.51 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.078 Time taken: 589.52 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.31 secs| Accuracy 98.601

Epoch: 3 Train Loss 0.053 Time taken: 587.74 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 23.19 secs| Accuracy 98.87

Epoch: 4 Train Loss 0.037 Time taken: 587.88 secs

    Epoch: 4 Validation Loss 0.048 Time taken: 23.36 secs| Accuracy 98.686

Epoch: 5 Train Loss 0.026 Time taken: 586.09 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 23.39 secs| Accuracy 99.011
T\P      0       1
0       5256        0053        
1       0050        5258        

Accuracy 99.03
Time taken: 34.26 secs
3093.843 secs

Mon Apr 18 11:38:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=367
Mon Apr 18 11:38:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.137 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 367,
    "message": "366 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.098 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.288 Time taken: 587.92 secs

    Epoch: 1 Validation Loss 0.091 Time taken: 23.15 secs| Accuracy 97.16

Epoch: 2 Train Loss 0.116 Time taken: 589.97 secs

    Epoch: 2 Validation Loss 0.069 Time taken: 23.18 secs| Accuracy 98.135

Epoch: 3 Train Loss 0.089 Time taken: 590.15 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.08 secs| Accuracy 98.404

Epoch: 4 Train Loss 0.067 Time taken: 592.14 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 23.15 secs| Accuracy 98.644

Epoch: 5 Train Loss 0.052 Time taken: 584.11 secs

    Epoch: 5 Validation Loss 0.04 Time taken: 23.36 secs| Accuracy 98.841
T\P      0       1
0       5255        0054        
1       0062        5246        

Accuracy 98.907
Time taken: 34.36 secs
3094.984 secs

Mon Apr 18 12:31:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=368
Mon Apr 18 12:31:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.374 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 368,
    "message": "366 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 4.887 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.281 Time taken: 589.96 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.1 secs| Accuracy 97.754

Epoch: 2 Train Loss 0.085 Time taken: 595.62 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 23.09 secs| Accuracy 98.644

Epoch: 3 Train Loss 0.057 Time taken: 586.99 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.19 secs| Accuracy 98.573

Epoch: 4 Train Loss 0.041 Time taken: 586.94 secs

    Epoch: 4 Validation Loss 0.036 Time taken: 23.28 secs| Accuracy 98.969

Epoch: 5 Train Loss 0.032 Time taken: 586.55 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 23.16 secs| Accuracy 98.997
T\P      0       1
0       5283        0026        
1       0065        5243        

Accuracy 99.143
Time taken: 33.94 secs
3096.416 secs

Mon Apr 18 13:23:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=369
Mon Apr 18 13:23:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.004 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 369,
    "message": "366 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.383 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.229 Time taken: 590.81 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 23.12 secs| Accuracy 98.177

Epoch: 2 Train Loss 0.07 Time taken: 588.75 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 23.17 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.045 Time taken: 588.86 secs

    Epoch: 3 Validation Loss 0.034 Time taken: 23.39 secs| Accuracy 99.011

Epoch: 4 Train Loss 0.033 Time taken: 588.7 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 23.27 secs| Accuracy 98.898

Epoch: 5 Train Loss 0.025 Time taken: 588.12 secs

    Epoch: 5 Validation Loss 0.048 Time taken: 23.35 secs| Accuracy 98.658
T\P      0       1
0       5196        0113        
1       0027        5281        

Accuracy 98.681
Time taken: 34.29 secs
3096.386 secs

Mon Apr 18 14:16:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=370
Mon Apr 18 14:16:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.013 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 370,
    "message": "366 with different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.341 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.288 Time taken: 588.11 secs

    Epoch: 1 Validation Loss 0.096 Time taken: 23.18 secs| Accuracy 97.457

Epoch: 2 Train Loss 0.126 Time taken: 589.46 secs

    Epoch: 2 Validation Loss 0.07 Time taken: 23.19 secs| Accuracy 97.739

Epoch: 3 Train Loss 0.081 Time taken: 590.15 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 23.18 secs| Accuracy 97.994

Epoch: 4 Train Loss 0.062 Time taken: 585.95 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 23.08 secs| Accuracy 98.502

Epoch: 5 Train Loss 0.048 Time taken: 583.5 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 22.99 secs| Accuracy 98.714
T\P      0       1
0       5280        0029        
1       0082        5226        

Accuracy 98.955
Time taken: 33.89 secs
3087.125 secs

Mon Apr 18 15:08:54 2022



======================================================================
----------------------------------------------------------------------
                run_ID=395
Mon Apr 18 20:57:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.988 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 395,
    "message": "391 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 9.515 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.224 Time taken: 599.29 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.14 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.08 Time taken: 600.21 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.06 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.054 Time taken: 595.22 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.18 secs| Accuracy 98.63

Epoch: 4 Train Loss 0.039 Time taken: 595.2 secs

    Epoch: 4 Validation Loss 0.046 Time taken: 23.31 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.031 Time taken: 599.6 secs

    Epoch: 5 Validation Loss 0.032 Time taken: 23.33 secs| Accuracy 98.969
T\P      0       1
0       5278        0031        
1       0074        5234        

Accuracy 99.011
Time taken: 34.03 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.547 Time taken: 386.52 secs

    Epoch: 6 Validation Loss 0.421 Time taken: 2.27 secs| Accuracy 80.464

Epoch: 7 Train Loss 0.38 Time taken: 380.59 secs

    Epoch: 7 Validation Loss 0.436 Time taken: 2.24 secs| Accuracy 83.113

Epoch: 8 Train Loss 0.306 Time taken: 380.84 secs

    Epoch: 8 Validation Loss 0.282 Time taken: 2.26 secs| Accuracy 89.404

Epoch: 9 Train Loss 0.24 Time taken: 380.23 secs

    Epoch: 9 Validation Loss 0.247 Time taken: 2.18 secs| Accuracy 89.073

Epoch: 10 Train Loss 0.172 Time taken: 381.48 secs

    Epoch: 10 Validation Loss 0.27 Time taken: 2.16 secs| Accuracy 87.086
T\P      0       1
0       0154        0044        
1       0014        0242        

Accuracy 87.225
Time taken: 2.69 secs
5113.503 secs

Mon Apr 18 22:23:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=396
Mon Apr 18 22:23:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.062 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 396,
    "message": "188 with include asha data after inshorts and some layers frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 122,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.438 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.194 Time taken: 578.3 secs

    Epoch: 1 Validation Loss 0.049 Time taken: 23.14 secs| Accuracy 98.531

Epoch: 2 Train Loss 0.063 Time taken: 581.3 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 23.11 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.046 Time taken: 581.16 secs

    Epoch: 3 Validation Loss 0.035 Time taken: 23.13 secs| Accuracy 98.94
T\P      0       1
0       5279        0030        
1       0086        5222        

Accuracy 98.907
Time taken: 34.37 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.552 Time taken: 380.38 secs

    Epoch: 4 Validation Loss 0.386 Time taken: 2.2 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.361 Time taken: 379.77 secs

    Epoch: 5 Validation Loss 0.295 Time taken: 2.19 secs| Accuracy 85.762

Epoch: 6 Train Loss 0.256 Time taken: 381.07 secs

    Epoch: 6 Validation Loss 0.275 Time taken: 2.18 secs| Accuracy 88.079
T\P      0       1
0       0175        0023        
1       0045        0211        

Accuracy 85.022
Time taken: 2.77 secs
3030.641 secs

Mon Apr 18 23:14:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=397
Mon Apr 18 23:14:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.382 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 397,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 123,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 4.866 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.218 Time taken: 581.63 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.33 secs| Accuracy 97.881

Epoch: 2 Train Loss 0.086 Time taken: 587.31 secs

    Epoch: 2 Validation Loss 0.078 Time taken: 23.57 secs| Accuracy 97.443

Epoch: 3 Train Loss 0.056 Time taken: 581.72 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 23.76 secs| Accuracy 98.573
T\P      0       1
0       5245        0064        
1       0055        5253        

Accuracy 98.879
Time taken: 34.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.583 Time taken: 390.34 secs

    Epoch: 4 Validation Loss 0.447 Time taken: 2.48 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.395 Time taken: 384.3 secs

    Epoch: 5 Validation Loss 0.331 Time taken: 3.18 secs| Accuracy 84.768

Epoch: 6 Train Loss 0.265 Time taken: 385.53 secs

    Epoch: 6 Validation Loss 0.331 Time taken: 2.66 secs| Accuracy 84.106
T\P      0       1
0       0188        0010        
1       0059        0197        

Accuracy 84.802
Time taken: 2.82 secs
3062.453 secs

Tue Apr 19 00:07:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=398
Tue Apr 19 00:07:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.859 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 398,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 124,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.365 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.287 Time taken: 584.93 secs

    Epoch: 1 Validation Loss 0.175 Time taken: 23.37 secs| Accuracy 94.193

Epoch: 2 Train Loss 0.133 Time taken: 589.88 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 23.37 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.086 Time taken: 577.73 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.53 secs| Accuracy 98.361
T\P      0       1
0       5232        0077        
1       0077        5231        

Accuracy 98.549
Time taken: 34.17 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.577 Time taken: 385.09 secs

    Epoch: 4 Validation Loss 0.533 Time taken: 2.38 secs| Accuracy 75.166

Epoch: 5 Train Loss 0.419 Time taken: 384.58 secs

    Epoch: 5 Validation Loss 0.492 Time taken: 2.56 secs| Accuracy 78.477

Epoch: 6 Train Loss 0.344 Time taken: 379.59 secs

    Epoch: 6 Validation Loss 0.4 Time taken: 2.51 secs| Accuracy 80.464
T\P      0       1
0       0173        0025        
1       0068        0188        

Accuracy 79.515
Time taken: 2.79 secs
3053.807 secs

Tue Apr 19 00:59:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=399
Tue Apr 19 00:59:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.94 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 399,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 125,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.441 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.265 Time taken: 581.03 secs

    Epoch: 1 Validation Loss 0.116 Time taken: 23.61 secs| Accuracy 97.146

Epoch: 2 Train Loss 0.116 Time taken: 589.3 secs

    Epoch: 2 Validation Loss 0.07 Time taken: 23.52 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.082 Time taken: 583.65 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 23.55 secs| Accuracy 97.909
T\P      0       1
0       5182        0127        
1       0051        5257        

Accuracy 98.323
Time taken: 34.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.558 Time taken: 390.58 secs

    Epoch: 4 Validation Loss 0.481 Time taken: 2.61 secs| Accuracy 76.159

Epoch: 5 Train Loss 0.442 Time taken: 380.22 secs

    Epoch: 5 Validation Loss 0.444 Time taken: 2.9 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.377 Time taken: 385.66 secs

    Epoch: 6 Validation Loss 0.354 Time taken: 2.37 secs| Accuracy 83.444
T\P      0       1
0       0160        0038        
1       0031        0225        

Accuracy 84.802
Time taken: 2.93 secs
3061.015 secs

Tue Apr 19 01:51:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=400
Tue Apr 19 01:51:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.836 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 400,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.421 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.245 Time taken: 579.8 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 23.47 secs| Accuracy 97.796

Epoch: 2 Train Loss 0.092 Time taken: 578.65 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.64 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.065 Time taken: 578.85 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.33 secs| Accuracy 98.686
T\P      0       1
0       5281        0028        
1       0083        5225        

Accuracy 98.955
Time taken: 34.27 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.58 Time taken: 387.33 secs

    Epoch: 4 Validation Loss 0.455 Time taken: 2.49 secs| Accuracy 79.47

Epoch: 5 Train Loss 0.4 Time taken: 380.99 secs

    Epoch: 5 Validation Loss 0.341 Time taken: 2.54 secs| Accuracy 85.099

Epoch: 6 Train Loss 0.286 Time taken: 392.97 secs

    Epoch: 6 Validation Loss 0.293 Time taken: 3.35 secs| Accuracy 87.086
T\P      0       1
0       0162        0036        
1       0039        0217        

Accuracy 83.48
Time taken: 3.9 secs
3057.599 secs

Tue Apr 19 02:43:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=401
Tue Apr 19 02:43:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.159 GB
38 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 401,
    "message": "188 with include asha data after inshorts and some layers frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.235 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 643.2 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 23.97 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.095 Time taken: 647.0 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 23.81 secs| Accuracy 98.192

Epoch: 3 Train Loss 0.065 Time taken: 670.44 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 22.81 secs| Accuracy 98.658
T\P      0       1
0       5256        0053        
1       0082        5226        

Accuracy 98.728
Time taken: 33.81 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.7 Time taken: 390.54 secs

    Epoch: 4 Validation Loss 0.421 Time taken: 2.09 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.413 Time taken: 461.7 secs

    Epoch: 5 Validation Loss 0.326 Time taken: 2.98 secs| Accuracy 87.086

Epoch: 6 Train Loss 0.314 Time taken: 476.94 secs

    Epoch: 6 Validation Loss 0.267 Time taken: 2.2 secs| Accuracy 88.742
T\P      0       1
0       0172        0026        
1       0040        0216        

Accuracy 85.463
Time taken: 2.67 secs
3453.86 secs

Tue Apr 19 03:42:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=402
Tue Apr 19 03:42:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.09 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 402,
    "message": "401 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.135 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 578.89 secs

    Epoch: 1 Validation Loss 0.082 Time taken: 23.05 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.106 Time taken: 590.15 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 23.05 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.07 Time taken: 581.25 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.0 secs| Accuracy 98.135
T\P      0       1
0       5194        0115        
1       0048        5260        

Accuracy 98.465
Time taken: 34.02 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.56 Time taken: 399.64 secs

    Epoch: 4 Validation Loss 0.436 Time taken: 2.04 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.415 Time taken: 396.65 secs

    Epoch: 5 Validation Loss 0.358 Time taken: 2.21 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.347 Time taken: 394.26 secs

    Epoch: 6 Validation Loss 0.317 Time taken: 1.99 secs| Accuracy 86.755
T\P      0       1
0       0160        0038        
1       0047        0209        

Accuracy 81.278
Time taken: 2.54 secs
3085.827 secs

Tue Apr 19 04:34:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=403
Tue Apr 19 04:34:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.052 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 403,
    "message": "401 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.348 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.247 Time taken: 596.05 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 23.04 secs| Accuracy 97.584

Epoch: 2 Train Loss 0.078 Time taken: 595.11 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.91 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.05 Time taken: 627.59 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.88 secs| Accuracy 98.601
T\P      0       1
0       5230        0079        
1       0036        5272        

Accuracy 98.917
Time taken: 34.94 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.706 Time taken: 434.7 secs

    Epoch: 4 Validation Loss 0.581 Time taken: 2.56 secs| Accuracy 67.55

Epoch: 5 Train Loss 0.469 Time taken: 428.94 secs

    Epoch: 5 Validation Loss 0.38 Time taken: 2.52 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.323 Time taken: 426.74 secs

    Epoch: 6 Validation Loss 0.328 Time taken: 2.42 secs| Accuracy 88.742
T\P      0       1
0       0175        0023        
1       0038        0218        

Accuracy 86.564
Time taken: 2.92 secs
3264.061 secs

Tue Apr 19 05:30:01 2022


======================================================================
----------------------------------------------------------------------
                run_ID=404
Tue Apr 19 05:30:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.167 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 404,
    "message": "188 with include asha data after inshorts",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.195 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.312 Time taken: 649.08 secs

    Epoch: 1 Validation Loss 0.104 Time taken: 23.64 secs| Accuracy 97.457

Epoch: 2 Train Loss 0.118 Time taken: 650.26 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 23.71 secs| Accuracy 97.739

Epoch: 3 Train Loss 0.082 Time taken: 646.66 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 23.41 secs| Accuracy 98.248
T\P      0       1
0       5205        0104        
1       0067        5241        

Accuracy 98.389
Time taken: 34.61 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.633 Time taken: 423.99 secs

    Epoch: 4 Validation Loss 0.448 Time taken: 2.28 secs| Accuracy 79.139

Epoch: 5 Train Loss 0.442 Time taken: 419.24 secs

    Epoch: 5 Validation Loss 0.365 Time taken: 2.52 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.387 Time taken: 420.43 secs

    Epoch: 6 Validation Loss 0.342 Time taken: 2.2 secs| Accuracy 85.762
T\P      0       1
0       0167        0031        
1       0057        0199        

Accuracy 80.617
Time taken: 2.61 secs
3367.037 secs

Tue Apr 19 06:27:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=405
Tue Apr 19 06:27:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.071 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 405,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.21 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.322 Time taken: 625.99 secs

    Epoch: 1 Validation Loss 0.122 Time taken: 23.13 secs| Accuracy 97.344

Epoch: 2 Train Loss 0.141 Time taken: 623.9 secs

    Epoch: 2 Validation Loss 0.08 Time taken: 23.11 secs| Accuracy 97.669

Epoch: 3 Train Loss 0.113 Time taken: 616.15 secs

    Epoch: 3 Validation Loss 0.143 Time taken: 23.51 secs| Accuracy 95.521
T\P      0       1
0       5296        0013        
1       0434        4874        

Accuracy 95.79
Time taken: 34.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.588 Time taken: 409.83 secs

    Epoch: 4 Validation Loss 0.538 Time taken: 2.22 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.504 Time taken: 411.62 secs

    Epoch: 5 Validation Loss 0.418 Time taken: 2.25 secs| Accuracy 80.795

Epoch: 6 Train Loss 0.42 Time taken: 414.03 secs

    Epoch: 6 Validation Loss 0.376 Time taken: 2.18 secs| Accuracy 83.113
T\P      0       1
0       0156        0042        
1       0048        0208        

Accuracy 80.176
Time taken: 2.64 secs
3248.658 secs

Tue Apr 19 07:22:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=406
Tue Apr 19 07:22:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 11.053 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 406,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 0.018 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.223 Time taken: 633.36 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.5 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.073 Time taken: 631.6 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 23.58 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.047 Time taken: 631.43 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 23.57 secs| Accuracy 98.813
T\P      0       1
0       5255        0054        
1       0056        5252        

Accuracy 98.964
Time taken: 34.62 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.675 Time taken: 416.84 secs

    Epoch: 4 Validation Loss 0.495 Time taken: 2.41 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.426 Time taken: 407.34 secs

    Epoch: 5 Validation Loss 0.352 Time taken: 2.64 secs| Accuracy 85.099

Epoch: 6 Train Loss 0.304 Time taken: 405.42 secs

    Epoch: 6 Validation Loss 0.325 Time taken: 2.45 secs| Accuracy 85.762
T\P      0       1
0       0176        0022        
1       0047        0209        

Accuracy 84.802
Time taken: 3.02 secs
3280.455 secs

Tue Apr 19 08:18:22 2022




======================================================================
----------------------------------------------------------------------
                run_ID=423
Tue Apr 19 12:59:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.272 GB
41 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 423,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 15.52 GB
32 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.212 Time taken: 697.25 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 23.96 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.068 Time taken: 581.07 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 22.31 secs| Accuracy 98.926

Epoch: 3 Train Loss 0.049 Time taken: 570.34 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 22.28 secs| Accuracy 98.658
T\P      0       1
0       5295        0014        
1       0130        5178        

Accuracy 98.644
Time taken: 33.15 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.623 Time taken: 382.81 secs

    Epoch: 4 Validation Loss 0.481 Time taken: 1.4 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.421 Time taken: 381.7 secs

    Epoch: 5 Validation Loss 0.395 Time taken: 1.4 secs| Accuracy 82.45

Epoch: 6 Train Loss 0.384 Time taken: 376.3 secs

    Epoch: 6 Validation Loss 0.411 Time taken: 1.47 secs| Accuracy 80.795
T\P      0       1
0       0122        0076        
1       0012        0244        

Accuracy 80.617
Time taken: 1.92 secs
3121.489 secs

Tue Apr 19 13:52:55 2022


======================================================================
----------------------------------------------------------------------
                run_ID=424
Tue Apr 19 13:52:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.525 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 424,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 9.961 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.225 Time taken: 573.11 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.4 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.083 Time taken: 577.15 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.33 secs| Accuracy 98.545

Epoch: 3 Train Loss 0.059 Time taken: 576.29 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 22.27 secs| Accuracy 98.757
T\P      0       1
0       5266        0043        
1       0068        5240        

Accuracy 98.955
Time taken: 33.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.563 Time taken: 375.42 secs

    Epoch: 4 Validation Loss 0.447 Time taken: 1.43 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.375 Time taken: 384.97 secs

    Epoch: 5 Validation Loss 0.393 Time taken: 1.51 secs| Accuracy 84.106

Epoch: 6 Train Loss 0.257 Time taken: 446.49 secs

    Epoch: 6 Validation Loss 0.3 Time taken: 2.02 secs| Accuracy 85.099
T\P      0       1
0       0161        0037        
1       0025        0231        

Accuracy 86.344
Time taken: 2.39 secs
3064.461 secs

Tue Apr 19 14:45:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=425
Tue Apr 19 14:45:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.189 GB
35 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 425,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 2.119 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 710.13 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 24.47 secs| Accuracy 98.192

Epoch: 2 Train Loss 0.08 Time taken: 746.39 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 24.7 secs| Accuracy 98.743

Epoch: 3 Train Loss 0.053 Time taken: 754.92 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 24.45 secs| Accuracy 98.036
T\P      0       1
0       5299        0010        
1       0183        5125        

Accuracy 98.182
Time taken: 36.38 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.574 Time taken: 533.16 secs

    Epoch: 4 Validation Loss 0.437 Time taken: 1.75 secs| Accuracy 81.457

Epoch: 5 Train Loss 0.388 Time taken: 536.37 secs

    Epoch: 5 Validation Loss 0.464 Time taken: 2.11 secs| Accuracy 80.464

Epoch: 6 Train Loss 0.348 Time taken: 410.57 secs

    Epoch: 6 Validation Loss 0.36 Time taken: 1.59 secs| Accuracy 83.775
T\P      0       1
0       0182        0016        
1       0059        0197        

Accuracy 83.48
Time taken: 1.92 secs
3834.492 secs

Tue Apr 19 15:50:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=426
Tue Apr 19 15:50:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.602 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 426,
    "message": "396 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 11.103 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 573.31 secs

    Epoch: 1 Validation Loss 0.138 Time taken: 22.33 secs| Accuracy 96.426

Epoch: 2 Train Loss 0.118 Time taken: 571.67 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 22.45 secs| Accuracy 97.584

Epoch: 3 Train Loss 0.076 Time taken: 575.37 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 22.41 secs| Accuracy 98.545
T\P      0       1
0       5242        0067        
1       0060        5248        

Accuracy 98.804
Time taken: 33.38 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.598 Time taken: 382.87 secs

    Epoch: 4 Validation Loss 0.478 Time taken: 1.47 secs| Accuracy 78.146

Epoch: 5 Train Loss 0.461 Time taken: 382.16 secs

    Epoch: 5 Validation Loss 0.445 Time taken: 1.47 secs| Accuracy 80.795

Epoch: 6 Train Loss 0.374 Time taken: 381.51 secs

    Epoch: 6 Validation Loss 0.377 Time taken: 1.5 secs| Accuracy 83.775
T\P      0       1
0       0158        0040        
1       0044        0212        

Accuracy 81.498
Time taken: 1.93 secs
2998.765 secs

Tue Apr 19 16:41:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=427
Tue Apr 19 16:41:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.786 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 427,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.352 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.271 Time taken: 602.78 secs

    Epoch: 1 Validation Loss 0.108 Time taken: 23.27 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.124 Time taken: 604.06 secs

    Epoch: 2 Validation Loss 0.071 Time taken: 23.29 secs| Accuracy 97.94

Epoch: 3 Train Loss 0.074 Time taken: 604.56 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 23.33 secs| Accuracy 98.32

Epoch: 4 Train Loss 0.055 Time taken: 600.04 secs

    Epoch: 4 Validation Loss 0.053 Time taken: 23.5 secs| Accuracy 98.564

Epoch: 5 Train Loss 0.042 Time taken: 600.63 secs

    Epoch: 5 Validation Loss 0.059 Time taken: 23.34 secs| Accuracy 98.482
T\P      0       1
0       5363        0039        
1       0279        5390        

Accuracy 97.128
Time taken: 34.63 secs
3163.891 secs

Tue Apr 19 17:34:59 2022


======================================================================
----------------------------------------------------------------------
                run_ID=428
Tue Apr 19 17:35:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.042 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 428,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.356 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.25 Time taken: 602.4 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.29 secs| Accuracy 97.818

Epoch: 2 Train Loss 0.092 Time taken: 602.38 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 23.48 secs| Accuracy 98.415

Epoch: 3 Train Loss 0.06 Time taken: 600.94 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.4 secs| Accuracy 98.726

Epoch: 4 Train Loss 0.04 Time taken: 600.03 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 23.44 secs| Accuracy 98.862

Epoch: 5 Train Loss 0.034 Time taken: 600.54 secs

    Epoch: 5 Validation Loss 0.046 Time taken: 23.28 secs| Accuracy 98.631
T\P      0       1
0       5265        0137        
1       0074        5595        

Accuracy 98.094
Time taken: 34.69 secs
3158.163 secs

Tue Apr 19 18:28:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=429
Tue Apr 19 21:06:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.092 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 429,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.354 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.194 Time taken: 603.48 secs

    Epoch: 1 Validation Loss 0.078 Time taken: 23.2 secs| Accuracy 97.967

Epoch: 2 Train Loss 0.078 Time taken: 601.96 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 23.18 secs| Accuracy 98.171

Epoch: 3 Train Loss 0.052 Time taken: 602.18 secs

    Epoch: 3 Validation Loss 0.04 Time taken: 23.41 secs| Accuracy 98.808

Epoch: 4 Train Loss 0.04 Time taken: 607.02 secs

    Epoch: 4 Validation Loss 0.053 Time taken: 23.31 secs| Accuracy 98.415

Epoch: 5 Train Loss 0.031 Time taken: 601.5 secs

    Epoch: 5 Validation Loss 0.035 Time taken: 23.15 secs| Accuracy 98.835
T\P      0       1
0       5286        0116        
1       0102        5567        

Accuracy 98.031
Time taken: 34.66 secs
3167.312 secs

Tue Apr 19 22:00:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=430
Tue Apr 19 22:00:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.087 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 430,
    "message": "381 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.311 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.213 Time taken: 603.47 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 23.26 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.081 Time taken: 608.36 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.29 secs| Accuracy 98.469

Epoch: 3 Train Loss 0.059 Time taken: 600.8 secs

    Epoch: 3 Validation Loss 0.045 Time taken: 23.36 secs| Accuracy 98.645

Epoch: 4 Train Loss 0.042 Time taken: 601.44 secs

    Epoch: 4 Validation Loss 0.047 Time taken: 23.46 secs| Accuracy 98.659

Epoch: 5 Train Loss 0.039 Time taken: 602.29 secs

    Epoch: 5 Validation Loss 0.047 Time taken: 23.43 secs| Accuracy 98.848
T\P      0       1
0       5325        0077        
1       0156        5513        

Accuracy 97.895
Time taken: 34.93 secs
3168.365 secs

Tue Apr 19 22:54:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=447
Tue Apr 19 22:54:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.061 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 447,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.407 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.247 Time taken: 575.61 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 22.45 secs| Accuracy 97.584

Epoch: 2 Train Loss 0.078 Time taken: 577.67 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.42 secs| Accuracy 98.559

Epoch: 3 Train Loss 0.05 Time taken: 573.01 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.5 secs| Accuracy 98.601
T\P      0       1
0       5230        0079        
1       0036        5272        

Accuracy 98.917
Time taken: 33.49 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.706 Time taken: 379.46 secs

    Epoch: 4 Validation Loss 0.581 Time taken: 1.46 secs| Accuracy 67.55

Epoch: 5 Train Loss 0.469 Time taken: 375.46 secs

    Epoch: 5 Validation Loss 0.38 Time taken: 1.48 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.323 Time taken: 374.77 secs

    Epoch: 6 Validation Loss 0.328 Time taken: 1.52 secs| Accuracy 88.742
T\P      0       1
0       0175        0023        
1       0038        0218        

Accuracy 86.564
Time taken: 1.91 secs
2987.955 secs

Tue Apr 19 23:45:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=448
Tue Apr 19 23:45:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.05 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 448,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.357 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.298 Time taken: 574.76 secs

    Epoch: 1 Validation Loss 0.08 Time taken: 22.43 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.111 Time taken: 573.26 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.44 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.077 Time taken: 576.4 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 22.39 secs| Accuracy 98.502
T\P      0       1
0       5263        0046        
1       0081        5227        

Accuracy 98.804
Time taken: 33.38 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.554 Time taken: 380.36 secs

    Epoch: 4 Validation Loss 0.473 Time taken: 1.59 secs| Accuracy 79.47

Epoch: 5 Train Loss 0.401 Time taken: 375.5 secs

    Epoch: 5 Validation Loss 0.427 Time taken: 1.47 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.325 Time taken: 374.8 secs

    Epoch: 6 Validation Loss 0.376 Time taken: 1.42 secs| Accuracy 81.126
T\P      0       1
0       0137        0061        
1       0016        0240        

Accuracy 83.04
Time taken: 1.92 secs
2987.836 secs

Wed Apr 20 00:36:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=449
Wed Apr 20 00:36:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.051 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 449,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.367 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.3 Time taken: 574.33 secs

    Epoch: 1 Validation Loss 0.098 Time taken: 22.38 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.101 Time taken: 571.88 secs

    Epoch: 2 Validation Loss 0.067 Time taken: 22.39 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.084 Time taken: 578.11 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 22.36 secs| Accuracy 98.46
T\P      0       1
0       5210        0099        
1       0060        5248        

Accuracy 98.502
Time taken: 33.33 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.634 Time taken: 381.34 secs

    Epoch: 4 Validation Loss 0.501 Time taken: 1.44 secs| Accuracy 78.146

Epoch: 5 Train Loss 0.441 Time taken: 374.77 secs

    Epoch: 5 Validation Loss 0.511 Time taken: 1.41 secs| Accuracy 79.139

Epoch: 6 Train Loss 0.418 Time taken: 385.55 secs

    Epoch: 6 Validation Loss 0.475 Time taken: 1.48 secs| Accuracy 82.781
T\P      0       1
0       0164        0034        
1       0041        0215        

Accuracy 83.48
Time taken: 1.9 secs
2999.812 secs

Wed Apr 20 01:27:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=450
Wed Apr 20 01:27:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 24.502 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 450,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.757 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.262 Time taken: 673.55 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 23.25 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.071 Time taken: 672.31 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.53 secs| Accuracy 98.757

Epoch: 3 Train Loss 0.048 Time taken: 669.15 secs

    Epoch: 3 Validation Loss 0.039 Time taken: 23.55 secs| Accuracy 98.856
T\P      0       1
0       5261        0048        
1       0057        5251        

Accuracy 99.011
Time taken: 35.06 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.666 Time taken: 477.28 secs

    Epoch: 4 Validation Loss 0.516 Time taken: 1.63 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.45 Time taken: 466.28 secs

    Epoch: 5 Validation Loss 0.528 Time taken: 1.83 secs| Accuracy 82.119

Epoch: 6 Train Loss 0.345 Time taken: 466.5 secs

    Epoch: 6 Validation Loss 0.34 Time taken: 1.7 secs| Accuracy 85.099
T\P      0       1
0       0166        0032        
1       0043        0213        

Accuracy 83.48
Time taken: 2.03 secs
3563.181 secs

Wed Apr 20 02:27:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=451
Wed Apr 20 02:27:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.085 GB
34 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 451,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.372 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.252 Time taken: 704.07 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 24.63 secs| Accuracy 98.144

Epoch: 2 Train Loss 0.073 Time taken: 702.98 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 24.44 secs| Accuracy 98.076

Epoch: 3 Train Loss 0.056 Time taken: 713.05 secs

    Epoch: 3 Validation Loss 0.043 Time taken: 24.38 secs| Accuracy 98.726

Epoch: 4 Train Loss 0.037 Time taken: 713.09 secs

    Epoch: 4 Validation Loss 0.05 Time taken: 24.66 secs| Accuracy 98.767

Epoch: 5 Train Loss 0.03 Time taken: 705.4 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 24.54 secs| Accuracy 98.902
T\P      0       1
0       5353        0049        
1       0259        5410        

Accuracy 97.218
Time taken: 36.76 secs
3698.73 secs

Wed Apr 20 03:30:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=452
Wed Apr 20 03:30:44 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.239 GB
34 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 452,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.457 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.232 Time taken: 709.76 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 23.83 secs| Accuracy 98.252

Epoch: 2 Train Loss 0.067 Time taken: 710.33 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 24.25 secs| Accuracy 98.74

Epoch: 3 Train Loss 0.045 Time taken: 705.06 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 24.38 secs| Accuracy 98.537

Epoch: 4 Train Loss 0.033 Time taken: 695.59 secs

    Epoch: 4 Validation Loss 0.041 Time taken: 24.62 secs| Accuracy 98.875

Epoch: 5 Train Loss 0.028 Time taken: 695.4 secs

    Epoch: 5 Validation Loss 0.047 Time taken: 23.52 secs| Accuracy 98.388
T\P      0       1
0       5257        0145        
1       0104        5565        

Accuracy 97.751
Time taken: 35.74 secs
3672.887 secs

Wed Apr 20 04:33:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=453
Wed Apr 20 04:33:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.19 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 453,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.437 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.334 Time taken: 688.31 secs

    Epoch: 1 Validation Loss 0.119 Time taken: 24.5 secs| Accuracy 97.168

Epoch: 2 Train Loss 0.14 Time taken: 685.99 secs

    Epoch: 2 Validation Loss 0.077 Time taken: 23.82 secs| Accuracy 97.954

Epoch: 3 Train Loss 0.108 Time taken: 691.49 secs

    Epoch: 3 Validation Loss 0.089 Time taken: 23.58 secs| Accuracy 97.371

Epoch: 4 Train Loss 0.077 Time taken: 690.42 secs

    Epoch: 4 Validation Loss 0.044 Time taken: 24.24 secs| Accuracy 98.686

Epoch: 5 Train Loss 0.064 Time taken: 687.44 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 24.07 secs| Accuracy 98.604
T\P      0       1
0       5307        0095        
1       0144        5525        

Accuracy 97.841
Time taken: 35.77 secs
3599.86 secs

Wed Apr 20 05:34:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=454
Wed Apr 20 05:34:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 18451 | Validation_set: 7380 | Test_set: 11071
Train_batches: 577 | Validation_batches: 231 | Test_batches: 346
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.146 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 454,
    "message": "XXX with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "with inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.171 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.294 Time taken: 694.08 secs

    Epoch: 1 Validation Loss 0.096 Time taken: 23.44 secs| Accuracy 97.385

Epoch: 2 Train Loss 0.1 Time taken: 685.56 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 23.91 secs| Accuracy 97.913

Epoch: 3 Train Loss 0.07 Time taken: 693.03 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 24.2 secs| Accuracy 98.469

Epoch: 4 Train Loss 0.053 Time taken: 681.09 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 25.01 secs| Accuracy 98.794

Epoch: 5 Train Loss 0.039 Time taken: 689.61 secs

    Epoch: 5 Validation Loss 0.049 Time taken: 24.06 secs| Accuracy 98.659
T\P      0       1
0       5247        0155        
1       0063        5606        

Accuracy 98.031
Time taken: 35.85 secs
3600.126 secs

Wed Apr 20 06:35:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=461
Wed Apr 20 12:06:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.947 GB
24 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 461,
    "message": "just using asha_sentsim data with EL0 frozen",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.822 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.687 Time taken: 472.49 secs

    Epoch: 1 Validation Loss 0.677 Time taken: 1.99 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.663 Time taken: 473.83 secs

    Epoch: 2 Validation Loss 0.663 Time taken: 1.62 secs| Accuracy 58.94

Epoch: 3 Train Loss 0.615 Time taken: 465.19 secs

    Epoch: 3 Validation Loss 0.531 Time taken: 1.75 secs| Accuracy 58.94

Epoch: 4 Train Loss 0.539 Time taken: 465.08 secs

    Epoch: 4 Validation Loss 0.45 Time taken: 2.09 secs| Accuracy 86.093

Epoch: 5 Train Loss 0.461 Time taken: 460.74 secs

    Epoch: 5 Validation Loss 0.425 Time taken: 1.85 secs| Accuracy 80.795
T\P      0       1
0       0193        0005        
1       0104        0152        

Accuracy 75.991
Time taken: 2.11 secs
2348.787 secs



======================================================================
----------------------------------------------------------------------
                run_ID=462
Wed Apr 20 13:16:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.301 GB
25 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 462,
    "message": "461 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.42 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.683 Time taken: 464.52 secs

    Epoch: 1 Validation Loss 0.666 Time taken: 1.75 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.657 Time taken: 472.35 secs

    Epoch: 2 Validation Loss 0.638 Time taken: 1.48 secs| Accuracy 58.94

Epoch: 3 Train Loss 0.634 Time taken: 472.3 secs

    Epoch: 3 Validation Loss 0.581 Time taken: 1.43 secs| Accuracy 58.94

Epoch: 4 Train Loss 0.562 Time taken: 472.83 secs

    Epoch: 4 Validation Loss 0.488 Time taken: 1.63 secs| Accuracy 81.788

Epoch: 5 Train Loss 0.484 Time taken: 462.82 secs

    Epoch: 5 Validation Loss 0.42 Time taken: 1.77 secs| Accuracy 86.424
T\P      0       1
0       0163        0035        
1       0029        0227        

Accuracy 85.903
Time taken: 1.99 secs
2354.896 secs

Wed Apr 20 13:56:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=463
Wed Apr 20 13:56:48 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.212 GB
26 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 463,
    "message": "461 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "asha_sentsim",
    "include_asha_data": "no",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.423 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.697 Time taken: 466.7 secs

    Epoch: 1 Validation Loss 0.678 Time taken: 1.53 secs| Accuracy 58.94

Epoch: 2 Train Loss 0.669 Time taken: 461.59 secs

    Epoch: 2 Validation Loss 0.641 Time taken: 1.74 secs| Accuracy 70.861

Epoch: 3 Train Loss 0.598 Time taken: 462.14 secs

    Epoch: 3 Validation Loss 0.547 Time taken: 1.74 secs| Accuracy 78.808

Epoch: 4 Train Loss 0.537 Time taken: 454.16 secs

    Epoch: 4 Validation Loss 0.521 Time taken: 1.62 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.487 Time taken: 460.77 secs

    Epoch: 5 Validation Loss 0.447 Time taken: 1.67 secs| Accuracy 78.808
T\P      0       1
0       0111        0087        
1       0000        0256        

Accuracy 80.837
Time taken: 2.0 secs
2315.676 secs

Wed Apr 20 14:36:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=498
Wed Apr 20 16:49:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 9.452 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 498,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 142,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 6.461 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.181 Time taken: 689.2 secs

    Epoch: 1 Validation Loss 0.052 Time taken: 23.26 secs| Accuracy 98.432

Epoch: 2 Train Loss 0.067 Time taken: 702.19 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 23.84 secs| Accuracy 97.739

Epoch: 3 Train Loss 0.051 Time taken: 681.13 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 23.66 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.035 Time taken: 683.03 secs

    Epoch: 4 Validation Loss 0.03 Time taken: 23.07 secs| Accuracy 99.082

Epoch: 5 Train Loss 0.022 Time taken: 680.0 secs

    Epoch: 5 Validation Loss 0.045 Time taken: 23.6 secs| Accuracy 99.096
T\P      0       1
0       5255        0054        
1       0051        5257        

Accuracy 99.011
Time taken: 34.71 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.74 Time taken: 468.86 secs

    Epoch: 6 Validation Loss 0.563 Time taken: 1.9 secs| Accuracy 72.185

Epoch: 7 Train Loss 0.442 Time taken: 455.06 secs

    Epoch: 7 Validation Loss 0.367 Time taken: 1.52 secs| Accuracy 83.444
T\P      0       1
0       0178        0020        
1       0074        0182        

Accuracy 79.295
Time taken: 2.19 secs
4563.988 secs

Wed Apr 20 18:06:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=499
Wed Apr 20 18:06:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.101 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 499,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 143,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.417 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 678.0 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 23.08 secs| Accuracy 98.163

Epoch: 2 Train Loss 0.067 Time taken: 682.74 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.11 secs| Accuracy 98.63

Epoch: 3 Train Loss 0.047 Time taken: 683.32 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 23.13 secs| Accuracy 98.813

Epoch: 4 Train Loss 0.036 Time taken: 676.79 secs

    Epoch: 4 Validation Loss 0.051 Time taken: 23.19 secs| Accuracy 98.474

Epoch: 5 Train Loss 0.028 Time taken: 679.8 secs

    Epoch: 5 Validation Loss 0.055 Time taken: 23.34 secs| Accuracy 98.658
T\P      0       1
0       5219        0090        
1       0044        5264        

Accuracy 98.738
Time taken: 34.97 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.684 Time taken: 460.27 secs

    Epoch: 6 Validation Loss 0.556 Time taken: 1.78 secs| Accuracy 77.152

Epoch: 7 Train Loss 0.459 Time taken: 468.89 secs

    Epoch: 7 Validation Loss 0.374 Time taken: 1.66 secs| Accuracy 83.775
T\P      0       1
0       0171        0027        
1       0063        0193        

Accuracy 80.176
Time taken: 2.03 secs
4527.011 secs

Wed Apr 20 19:23:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=500
Wed Apr 20 19:23:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.06 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 500,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 144,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 9.261 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.156 Time taken: 674.77 secs

    Epoch: 1 Validation Loss 0.057 Time taken: 23.02 secs| Accuracy 98.347

Epoch: 2 Train Loss 0.063 Time taken: 674.99 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 22.91 secs| Accuracy 97.98

Epoch: 3 Train Loss 0.042 Time taken: 674.81 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.97 secs| Accuracy 98.531

Epoch: 4 Train Loss 0.031 Time taken: 671.03 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 23.24 secs| Accuracy 98.432

Epoch: 5 Train Loss 0.024 Time taken: 671.32 secs

    Epoch: 5 Validation Loss 0.037 Time taken: 23.24 secs| Accuracy 98.912
T\P      0       1
0       5260        0049        
1       0041        5267        

Accuracy 99.152
Time taken: 34.4 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.578 Time taken: 468.13 secs

    Epoch: 6 Validation Loss 0.402 Time taken: 1.8 secs| Accuracy 82.45

Epoch: 7 Train Loss 0.329 Time taken: 451.14 secs

    Epoch: 7 Validation Loss 0.333 Time taken: 1.73 secs| Accuracy 86.424
T\P      0       1
0       0173        0025        
1       0044        0212        

Accuracy 84.802
Time taken: 2.2 secs
4490.401 secs

Wed Apr 20 20:39:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=501
Wed Apr 20 20:39:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.46 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 501,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 145,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 9.44 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.161 Time taken: 666.26 secs

    Epoch: 1 Validation Loss 0.054 Time taken: 22.43 secs| Accuracy 98.404

Epoch: 2 Train Loss 0.062 Time taken: 698.32 secs

    Epoch: 2 Validation Loss 0.072 Time taken: 22.73 secs| Accuracy 97.98

Epoch: 3 Train Loss 0.044 Time taken: 696.4 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 23.47 secs| Accuracy 98.163

Epoch: 4 Train Loss 0.029 Time taken: 700.82 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 23.57 secs| Accuracy 98.658

Epoch: 5 Train Loss 0.023 Time taken: 691.3 secs

    Epoch: 5 Validation Loss 0.044 Time taken: 23.88 secs| Accuracy 98.785
T\P      0       1
0       5263        0046        
1       0046        5262        

Accuracy 99.133
Time taken: 34.87 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.542 Time taken: 459.87 secs

    Epoch: 6 Validation Loss 0.369 Time taken: 1.52 secs| Accuracy 84.437

Epoch: 7 Train Loss 0.329 Time taken: 460.74 secs

    Epoch: 7 Validation Loss 0.321 Time taken: 1.74 secs| Accuracy 86.424
T\P      0       1
0       0182        0016        
1       0049        0207        

Accuracy 85.683
Time taken: 2.28 secs
4570.489 secs

Wed Apr 20 21:56:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=502
Wed Apr 20 21:56:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.2 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 502,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 146,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.964 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.173 Time taken: 727.24 secs

    Epoch: 1 Validation Loss 0.043 Time taken: 23.83 secs| Accuracy 98.601

Epoch: 2 Train Loss 0.065 Time taken: 715.61 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 23.77 secs| Accuracy 98.955

Epoch: 3 Train Loss 0.041 Time taken: 717.05 secs

    Epoch: 3 Validation Loss 0.032 Time taken: 23.78 secs| Accuracy 99.082

Epoch: 4 Train Loss 0.032 Time taken: 710.56 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 24.02 secs| Accuracy 98.969

Epoch: 5 Train Loss 0.02 Time taken: 710.72 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 24.05 secs| Accuracy 99.082
T\P      0       1
0       5270        0039        
1       0053        5255        

Accuracy 99.133
Time taken: 36.02 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.592 Time taken: 493.5 secs

    Epoch: 6 Validation Loss 0.41 Time taken: 1.68 secs| Accuracy 82.781

Epoch: 7 Train Loss 0.369 Time taken: 481.39 secs

    Epoch: 7 Validation Loss 0.323 Time taken: 1.96 secs| Accuracy 86.424
T\P      0       1
0       0165        0033        
1       0047        0209        

Accuracy 82.379
Time taken: 2.07 secs
4762.711 secs

Wed Apr 20 23:17:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=503
Wed Apr 20 23:17:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.519 GB
35 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 503,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 147,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 8.998 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.172 Time taken: 713.78 secs

    Epoch: 1 Validation Loss 0.055 Time taken: 23.99 secs| Accuracy 98.474

Epoch: 2 Train Loss 0.072 Time taken: 705.62 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 23.85 secs| Accuracy 98.658

Epoch: 3 Train Loss 0.045 Time taken: 688.31 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 22.38 secs| Accuracy 98.969

Epoch: 4 Train Loss 0.036 Time taken: 589.9 secs

    Epoch: 4 Validation Loss 0.032 Time taken: 22.39 secs| Accuracy 99.096

Epoch: 5 Train Loss 0.023 Time taken: 589.96 secs

    Epoch: 5 Validation Loss 0.033 Time taken: 22.55 secs| Accuracy 99.294
T\P      0       1
0       5262        0047        
1       0054        5254        

Accuracy 99.049
Time taken: 33.3 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.58 Time taken: 380.18 secs

    Epoch: 6 Validation Loss 0.343 Time taken: 1.41 secs| Accuracy 85.099

Epoch: 7 Train Loss 0.345 Time taken: 379.3 secs

    Epoch: 7 Validation Loss 0.266 Time taken: 1.4 secs| Accuracy 88.079
T\P      0       1
0       0149        0049        
1       0030        0226        

Accuracy 82.599
Time taken: 1.85 secs
4247.8 secs

Thu Apr 21 00:29:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=504
Thu Apr 21 00:29:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.007 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 504,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 148,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM 5.363 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.182 Time taken: 593.78 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 22.48 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 595.85 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 22.41 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.038 Time taken: 588.73 secs

    Epoch: 3 Validation Loss 0.084 Time taken: 22.46 secs| Accuracy 97.443

Epoch: 4 Train Loss 0.026 Time taken: 588.87 secs

    Epoch: 4 Validation Loss 0.039 Time taken: 22.47 secs| Accuracy 98.87

Epoch: 5 Train Loss 0.019 Time taken: 591.46 secs

    Epoch: 5 Validation Loss 0.042 Time taken: 23.04 secs| Accuracy 98.912
T\P      0       1
0       5252        0057        
1       0044        5264        

Accuracy 99.049
Time taken: 33.7 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.694 Time taken: 447.6 secs

    Epoch: 6 Validation Loss 0.436 Time taken: 2.0 secs| Accuracy 81.788

Epoch: 7 Train Loss 0.405 Time taken: 419.7 secs

    Epoch: 7 Validation Loss 0.346 Time taken: 1.59 secs| Accuracy 84.768
T\P      0       1
0       0170        0028        
1       0064        0192        

Accuracy 79.736
Time taken: 2.06 secs
4024.033 secs

Thu Apr 21 01:37:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=505
Thu Apr 21 01:37:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 14.264 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 505,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 149,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.036 GB
Memory taken on RAM -11.755 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.167 Time taken: 643.12 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.35 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.067 Time taken: 595.67 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.3 secs| Accuracy 98.714

Epoch: 3 Train Loss 0.043 Time taken: 598.83 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 22.39 secs| Accuracy 98.785

Epoch: 4 Train Loss 0.034 Time taken: 594.42 secs

    Epoch: 4 Validation Loss 0.033 Time taken: 22.37 secs| Accuracy 99.11

Epoch: 5 Train Loss 0.022 Time taken: 589.02 secs

    Epoch: 5 Validation Loss 0.034 Time taken: 22.44 secs| Accuracy 99.011
T\P      0       1
0       5262        0047        
1       0052        5256        

Accuracy 99.068
Time taken: 33.31 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.66 Time taken: 374.12 secs

    Epoch: 6 Validation Loss 0.541 Time taken: 1.46 secs| Accuracy 75.166

Epoch: 7 Train Loss 0.465 Time taken: 380.41 secs

    Epoch: 7 Validation Loss 0.365 Time taken: 1.49 secs| Accuracy 83.775
T\P      0       1
0       0186        0012        
1       0090        0166        

Accuracy 77.533
Time taken: 1.88 secs
3974.638 secs

Thu Apr 21 02:44:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=506
Thu Apr 21 02:44:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.049 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 506,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 140,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.374 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.306 Time taken: 591.53 secs

    Epoch: 1 Validation Loss 0.09 Time taken: 22.45 secs| Accuracy 97.641

Epoch: 2 Train Loss 0.084 Time taken: 589.0 secs

    Epoch: 2 Validation Loss 0.069 Time taken: 22.47 secs| Accuracy 98.135

Epoch: 3 Train Loss 0.061 Time taken: 593.89 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 22.52 secs| Accuracy 98.658

Epoch: 4 Train Loss 0.044 Time taken: 595.01 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 22.47 secs| Accuracy 98.94

Epoch: 5 Train Loss 0.031 Time taken: 593.52 secs

    Epoch: 5 Validation Loss 0.046 Time taken: 22.45 secs| Accuracy 98.785
T\P      0       1
0       5228        0081        
1       0033        5275        

Accuracy 98.926
Time taken: 33.3 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.662 Time taken: 379.79 secs

    Epoch: 6 Validation Loss 0.524 Time taken: 1.69 secs| Accuracy 78.146

Epoch: 7 Train Loss 0.422 Time taken: 374.58 secs

    Epoch: 7 Validation Loss 0.467 Time taken: 1.62 secs| Accuracy 81.126
T\P      0       1
0       0157        0041        
1       0050        0206        

Accuracy 79.956
Time taken: 2.12 secs
3917.113 secs

Thu Apr 21 03:50:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=507
Thu Apr 21 03:51:02 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.068 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 507,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 141,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -0.239 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.283 Time taken: 590.69 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.43 secs| Accuracy 98.163

Epoch: 2 Train Loss 0.092 Time taken: 596.8 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 22.34 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.069 Time taken: 592.35 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.31 secs| Accuracy 98.615

Epoch: 4 Train Loss 0.05 Time taken: 588.11 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 22.48 secs| Accuracy 98.771

Epoch: 5 Train Loss 0.035 Time taken: 588.86 secs

    Epoch: 5 Validation Loss 0.054 Time taken: 22.46 secs| Accuracy 98.404
T\P      0       1
0       5183        0126        
1       0035        5273        

Accuracy 98.484
Time taken: 33.61 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.663 Time taken: 382.29 secs

    Epoch: 6 Validation Loss 0.454 Time taken: 1.54 secs| Accuracy 79.139

Epoch: 7 Train Loss 0.504 Time taken: 375.25 secs

    Epoch: 7 Validation Loss 0.391 Time taken: 1.56 secs| Accuracy 82.781
T\P      0       1
0       0153        0045        
1       0036        0220        

Accuracy 82.159
Time taken: 2.07 secs
3908.026 secs

Thu Apr 21 04:57:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=508
Thu Apr 21 04:57:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.063 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 508,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 142,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.375 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.23 Time taken: 591.19 secs

    Epoch: 1 Validation Loss 0.05 Time taken: 22.35 secs| Accuracy 98.615

Epoch: 2 Train Loss 0.067 Time taken: 588.25 secs

    Epoch: 2 Validation Loss 0.039 Time taken: 22.31 secs| Accuracy 98.955

Epoch: 3 Train Loss 0.046 Time taken: 588.36 secs

    Epoch: 3 Validation Loss 0.033 Time taken: 22.41 secs| Accuracy 99.053

Epoch: 4 Train Loss 0.031 Time taken: 588.19 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 22.43 secs| Accuracy 99.138

Epoch: 5 Train Loss 0.024 Time taken: 588.75 secs

    Epoch: 5 Validation Loss 0.025 Time taken: 22.42 secs| Accuracy 99.336
T\P      0       1
0       5263        0046        
1       0047        5261        

Accuracy 99.124
Time taken: 33.36 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.612 Time taken: 379.49 secs

    Epoch: 6 Validation Loss 0.371 Time taken: 1.5 secs| Accuracy 85.762

Epoch: 7 Train Loss 0.37 Time taken: 378.84 secs

    Epoch: 7 Validation Loss 0.318 Time taken: 1.52 secs| Accuracy 86.424
T\P      0       1
0       0189        0009        
1       0063        0193        

Accuracy 84.141
Time taken: 1.95 secs
3910.793 secs

Thu Apr 21 06:03:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=509
Thu Apr 21 06:03:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.101 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 509,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 143,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.315 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 590.41 secs

    Epoch: 1 Validation Loss 0.105 Time taken: 22.22 secs| Accuracy 97.217

Epoch: 2 Train Loss 0.078 Time taken: 592.05 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 22.18 secs| Accuracy 98.587

Epoch: 3 Train Loss 0.047 Time taken: 588.49 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 22.24 secs| Accuracy 98.926

Epoch: 4 Train Loss 0.037 Time taken: 596.63 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 22.1 secs| Accuracy 98.771

Epoch: 5 Train Loss 0.03 Time taken: 587.57 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 22.23 secs| Accuracy 98.87
T\P      0       1
0       5275        0034        
1       0066        5242        

Accuracy 99.058
Time taken: 33.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.554 Time taken: 378.25 secs

    Epoch: 6 Validation Loss 0.384 Time taken: 1.44 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.342 Time taken: 374.48 secs

    Epoch: 7 Validation Loss 0.38 Time taken: 1.4 secs| Accuracy 82.781
T\P      0       1
0       0194        0004        
1       0087        0169        

Accuracy 79.956
Time taken: 1.9 secs
3894.918 secs

Thu Apr 21 07:09:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=510
Thu Apr 21 07:09:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.054 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 510,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 144,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.355 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.255 Time taken: 590.18 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.26 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.081 Time taken: 591.96 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 22.29 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.055 Time taken: 592.7 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 22.28 secs| Accuracy 98.644

Epoch: 4 Train Loss 0.041 Time taken: 588.96 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 22.33 secs| Accuracy 98.517

Epoch: 5 Train Loss 0.034 Time taken: 594.79 secs

    Epoch: 5 Validation Loss 0.038 Time taken: 22.26 secs| Accuracy 98.983
T\P      0       1
0       5286        0023        
1       0069        5239        

Accuracy 99.133
Time taken: 33.15 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.588 Time taken: 379.04 secs

    Epoch: 6 Validation Loss 0.384 Time taken: 1.49 secs| Accuracy 81.788

Epoch: 7 Train Loss 0.367 Time taken: 374.33 secs

    Epoch: 7 Validation Loss 0.406 Time taken: 1.46 secs| Accuracy 83.444
T\P      0       1
0       0150        0048        
1       0030        0226        

Accuracy 82.819
Time taken: 1.99 secs
3904.954 secs

Thu Apr 21 08:15:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=511
Thu Apr 21 08:15:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.046 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)...


{
    "run_ID": 511,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 145,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.246 Time taken: 591.25 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 22.26 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.072 Time taken: 588.85 secs

    Epoch: 2 Validation Loss 0.043 Time taken: 22.32 secs| Accuracy 98.771

Epoch: 3 Train Loss 0.048 Time taken: 588.73 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 22.32 secs| Accuracy 98.601

Epoch: 4 Train Loss 0.037 Time taken: 589.05 secs

    Epoch: 4 Validation Loss 0.034 Time taken: 22.41 secs| Accuracy 99.053

Epoch: 5 Train Loss 0.028 Time taken: 589.21 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 22.49 secs| Accuracy 98.898
T\P      0       1
0       5250        0059        
1       0058        5250        

Accuracy 98.898
Time taken: 33.44 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.585 Time taken: 379.18 secs

    Epoch: 6 Validation Loss 0.412 Time taken: 1.45 secs| Accuracy 80.795

Epoch: 7 Train Loss 0.405 Time taken: 374.46 secs

    Epoch: 7 Validation Loss 0.362 Time taken: 1.45 secs| Accuracy 84.437
T\P      0       1
0       0185        0013        
1       0066        0190        

Accuracy 82.599
Time taken: 1.93 secs
3890.249 secs

Thu Apr 21 09:21:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=512
Thu Apr 21 09:21:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.048 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 512,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 146,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.382 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.245 Time taken: 591.52 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 22.24 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.07 Time taken: 593.15 secs

    Epoch: 2 Validation Loss 0.038 Time taken: 22.25 secs| Accuracy 98.785

Epoch: 3 Train Loss 0.045 Time taken: 589.56 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 22.32 secs| Accuracy 98.884

Epoch: 4 Train Loss 0.037 Time taken: 595.54 secs

    Epoch: 4 Validation Loss 0.03 Time taken: 22.23 secs| Accuracy 99.138

Epoch: 5 Train Loss 0.022 Time taken: 587.42 secs

    Epoch: 5 Validation Loss 0.043 Time taken: 22.28 secs| Accuracy 99.025
T\P      0       1
0       5285        0024        
1       0075        5233        

Accuracy 99.068
Time taken: 33.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.601 Time taken: 381.46 secs

    Epoch: 6 Validation Loss 0.433 Time taken: 1.45 secs| Accuracy 82.45

Epoch: 7 Train Loss 0.38 Time taken: 378.33 secs

    Epoch: 7 Validation Loss 0.351 Time taken: 1.41 secs| Accuracy 84.106
T\P      0       1
0       0189        0009        
1       0088        0168        

Accuracy 78.634
Time taken: 1.9 secs
3913.06 secs

Thu Apr 21 10:27:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=513
Thu Apr 21 10:27:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.062 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 513,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 147,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.35 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.251 Time taken: 590.22 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.21 secs| Accuracy 97.853

Epoch: 2 Train Loss 0.091 Time taken: 592.41 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.18 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.065 Time taken: 588.79 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 22.26 secs| Accuracy 98.46

Epoch: 4 Train Loss 0.042 Time taken: 588.33 secs

    Epoch: 4 Validation Loss 0.043 Time taken: 22.33 secs| Accuracy 98.827

Epoch: 5 Train Loss 0.035 Time taken: 589.52 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 22.28 secs| Accuracy 99.11
T\P      0       1
0       5254        0055        
1       0053        5255        

Accuracy 98.983
Time taken: 33.23 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.638 Time taken: 379.95 secs

    Epoch: 6 Validation Loss 0.469 Time taken: 1.43 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.387 Time taken: 375.98 secs

    Epoch: 7 Validation Loss 0.324 Time taken: 1.4 secs| Accuracy 86.755
T\P      0       1
0       0157        0041        
1       0046        0210        

Accuracy 80.837
Time taken: 1.91 secs
3894.382 secs

Thu Apr 21 11:33:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=514
Thu Apr 21 11:33:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.045 GB
29 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 514,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 148,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.346 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.243 Time taken: 593.83 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.36 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.075 Time taken: 590.85 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 22.34 secs| Accuracy 98.022

Epoch: 3 Train Loss 0.044 Time taken: 590.98 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 22.36 secs| Accuracy 98.94

Epoch: 4 Train Loss 0.034 Time taken: 591.37 secs

    Epoch: 4 Validation Loss 0.04 Time taken: 22.46 secs| Accuracy 98.771

Epoch: 5 Train Loss 0.024 Time taken: 591.73 secs

    Epoch: 5 Validation Loss 0.039 Time taken: 22.45 secs| Accuracy 98.955
T\P      0       1
0       5255        0054        
1       0049        5259        

Accuracy 99.03
Time taken: 33.38 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.603 Time taken: 379.3 secs

    Epoch: 6 Validation Loss 0.415 Time taken: 1.44 secs| Accuracy 80.795

Epoch: 7 Train Loss 0.354 Time taken: 375.16 secs

    Epoch: 7 Validation Loss 0.336 Time taken: 1.43 secs| Accuracy 84.106
T\P      0       1
0       0154        0044        
1       0038        0218        

Accuracy 81.938
Time taken: 1.95 secs
3911.063 secs

Thu Apr 21 12:39:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=515
Thu Apr 21 12:39:49 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.078 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 515,
    "message": "xxx with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 5,
    "rseed": 149,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data with best model",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.359 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.291 Time taken: 597.41 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 22.75 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.085 Time taken: 603.21 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.34 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.063 Time taken: 592.88 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 22.34 secs| Accuracy 98.714

Epoch: 4 Train Loss 0.045 Time taken: 592.77 secs

    Epoch: 4 Validation Loss 0.038 Time taken: 22.38 secs| Accuracy 98.884

Epoch: 5 Train Loss 0.038 Time taken: 588.55 secs

    Epoch: 5 Validation Loss 0.036 Time taken: 22.36 secs| Accuracy 98.884
T\P      0       1
0       5268        0041        
1       0056        5252        

Accuracy 99.086
Time taken: 33.27 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 6 Train Loss 0.529 Time taken: 378.48 secs

    Epoch: 6 Validation Loss 0.357 Time taken: 1.51 secs| Accuracy 81.457

Epoch: 7 Train Loss 0.337 Time taken: 425.57 secs

    Epoch: 7 Validation Loss 0.294 Time taken: 1.9 secs| Accuracy 87.417
T\P      0       1
0       0132        0066        
1       0017        0239        

Accuracy 81.718
Time taken: 2.15 secs
3982.631 secs

Thu Apr 21 13:47:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=529
Thu Apr 21 15:14:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 7.447 GB
29 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 529,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 124,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 8.203 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.495 Time taken: 570.88 secs

    Epoch: 1 Validation Loss 0.253 Time taken: 22.28 secs| Accuracy 89.503

Epoch: 2 Train Loss 0.202 Time taken: 569.5 secs

    Epoch: 2 Validation Loss 0.156 Time taken: 22.3 secs| Accuracy 94.575

Epoch: 3 Train Loss 0.123 Time taken: 569.1 secs

    Epoch: 3 Validation Loss 0.14 Time taken: 22.28 secs| Accuracy 94.928
T\P      0       1
0       4835        0474        
1       0099        5209        

Accuracy 94.603
Time taken: 33.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.743 Time taken: 380.47 secs

    Epoch: 4 Validation Loss 0.612 Time taken: 1.51 secs| Accuracy 64.238

Epoch: 5 Train Loss 0.554 Time taken: 380.23 secs

    Epoch: 5 Validation Loss 0.463 Time taken: 1.5 secs| Accuracy 80.464
T\P      0       1
0       0117        0081        
1       0034        0222        

Accuracy 74.67
Time taken: 1.95 secs
2600.691 secs

Thu Apr 21 15:58:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=530
Thu Apr 21 15:59:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.085 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 530,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 125,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.289 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.451 Time taken: 577.1 secs

    Epoch: 1 Validation Loss 0.19 Time taken: 22.58 secs| Accuracy 92.922

Epoch: 2 Train Loss 0.173 Time taken: 762.79 secs

    Epoch: 2 Validation Loss 0.142 Time taken: 23.34 secs| Accuracy 94.49

Epoch: 3 Train Loss 0.111 Time taken: 674.94 secs

    Epoch: 3 Validation Loss 0.141 Time taken: 22.49 secs| Accuracy 95.083
T\P      0       1
0       5110        0199        
1       0316        4992        

Accuracy 95.149
Time taken: 33.29 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.837 Time taken: 379.67 secs

    Epoch: 4 Validation Loss 0.628 Time taken: 1.54 secs| Accuracy 59.272

Epoch: 5 Train Loss 0.585 Time taken: 383.8 secs

    Epoch: 5 Validation Loss 0.495 Time taken: 2.14 secs| Accuracy 76.821
T\P      0       1
0       0121        0077        
1       0046        0210        

Accuracy 72.907
Time taken: 2.7 secs
2910.181 secs

Thu Apr 21 16:48:28 2022


======================================================================
----------------------------------------------------------------------
                run_ID=531
Thu Apr 21 16:48:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.06 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 531,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 10.761 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.586 Time taken: 681.15 secs

    Epoch: 1 Validation Loss 0.373 Time taken: 23.74 secs| Accuracy 88.033

Epoch: 2 Train Loss 0.258 Time taken: 670.45 secs

    Epoch: 2 Validation Loss 0.165 Time taken: 24.04 secs| Accuracy 93.769

Epoch: 3 Train Loss 0.138 Time taken: 726.58 secs

    Epoch: 3 Validation Loss 0.134 Time taken: 23.89 secs| Accuracy 95.083
T\P      0       1
0       5103        0206        
1       0280        5028        

Accuracy 95.422
Time taken: 35.54 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.819 Time taken: 512.53 secs

    Epoch: 4 Validation Loss 0.524 Time taken: 2.01 secs| Accuracy 78.146

Epoch: 5 Train Loss 0.474 Time taken: 494.33 secs

    Epoch: 5 Validation Loss 0.427 Time taken: 1.96 secs| Accuracy 81.788
T\P      0       1
0       0150        0048        
1       0056        0200        

Accuracy 77.093
Time taken: 2.54 secs
3223.588 secs

Thu Apr 21 17:43:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=532
Thu Apr 21 17:43:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.745 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 532,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.349 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.521 Time taken: 672.51 secs

    Epoch: 1 Validation Loss 0.344 Time taken: 23.32 secs| Accuracy 84.162

Epoch: 2 Train Loss 0.225 Time taken: 630.91 secs

    Epoch: 2 Validation Loss 0.151 Time taken: 23.13 secs| Accuracy 94.165

Epoch: 3 Train Loss 0.131 Time taken: 590.62 secs

    Epoch: 3 Validation Loss 0.129 Time taken: 22.37 secs| Accuracy 95.196
T\P      0       1
0       4915        0394        
1       0152        5156        

Accuracy 94.857
Time taken: 33.31 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.837 Time taken: 386.59 secs

    Epoch: 4 Validation Loss 0.62 Time taken: 1.62 secs| Accuracy 58.94

Epoch: 5 Train Loss 0.54 Time taken: 385.39 secs

    Epoch: 5 Validation Loss 0.471 Time taken: 1.56 secs| Accuracy 76.821
T\P      0       1
0       0161        0037        
1       0071        0185        

Accuracy 76.211
Time taken: 2.02 secs
2797.208 secs

Thu Apr 21 18:31:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=533
Thu Apr 21 18:31:29 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.762 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 533,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.362 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.471 Time taken: 588.67 secs

    Epoch: 1 Validation Loss 0.245 Time taken: 22.28 secs| Accuracy 89.941

Epoch: 2 Train Loss 0.204 Time taken: 760.84 secs

    Epoch: 2 Validation Loss 0.17 Time taken: 24.4 secs| Accuracy 93.19

Epoch: 3 Train Loss 0.124 Time taken: 765.09 secs

    Epoch: 3 Validation Loss 0.143 Time taken: 26.11 secs| Accuracy 94.674
T\P      0       1
0       5128        0181        
1       0364        4944        

Accuracy 94.867
Time taken: 38.62 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.85 Time taken: 601.15 secs

    Epoch: 4 Validation Loss 0.659 Time taken: 2.3 secs| Accuracy 58.94

Epoch: 5 Train Loss 0.655 Time taken: 557.13 secs

    Epoch: 5 Validation Loss 0.646 Time taken: 2.04 secs| Accuracy 58.94
T\P      0       1
0       0000        0198        
1       0000        0256        

Accuracy 56.388
Time taken: 2.64 secs
3420.93 secs

Thu Apr 21 19:29:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=534
Thu Apr 21 19:29:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.279 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 534,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.377 GB
28 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.465 Time taken: 708.95 secs

    Epoch: 1 Validation Loss 0.2 Time taken: 23.68 secs| Accuracy 92.314

Epoch: 2 Train Loss 0.179 Time taken: 779.15 secs

    Epoch: 2 Validation Loss 0.148 Time taken: 24.86 secs| Accuracy 94.306

Epoch: 3 Train Loss 0.111 Time taken: 643.39 secs

    Epoch: 3 Validation Loss 0.122 Time taken: 22.57 secs| Accuracy 95.352
T\P      0       1
0       5027        0282        
1       0168        5140        

Accuracy 95.762
Time taken: 33.45 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.881 Time taken: 385.53 secs

    Epoch: 4 Validation Loss 0.586 Time taken: 1.6 secs| Accuracy 69.205

Epoch: 5 Train Loss 0.525 Time taken: 379.05 secs

    Epoch: 5 Validation Loss 0.444 Time taken: 1.64 secs| Accuracy 81.457
T\P      0       1
0       0162        0036        
1       0063        0193        

Accuracy 78.194
Time taken: 2.1 secs
3030.115 secs

Thu Apr 21 20:21:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=535
Thu Apr 21 20:21:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.929 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 535,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.345 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.516 Time taken: 573.4 secs

    Epoch: 1 Validation Loss 0.243 Time taken: 22.31 secs| Accuracy 90.619

Epoch: 2 Train Loss 0.215 Time taken: 570.17 secs

    Epoch: 2 Validation Loss 0.145 Time taken: 22.86 secs| Accuracy 94.377

Epoch: 3 Train Loss 0.124 Time taken: 574.39 secs

    Epoch: 3 Validation Loss 0.131 Time taken: 22.46 secs| Accuracy 94.984
T\P      0       1
0       5109        0200        
1       0290        5018        

Accuracy 95.385
Time taken: 33.32 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.777 Time taken: 445.88 secs

    Epoch: 4 Validation Loss 0.582 Time taken: 1.58 secs| Accuracy 72.185

Epoch: 5 Train Loss 0.521 Time taken: 483.9 secs

    Epoch: 5 Validation Loss 0.493 Time taken: 1.82 secs| Accuracy 77.815
T\P      0       1
0       0132        0066        
1       0057        0199        

Accuracy 72.907
Time taken: 2.29 secs
2778.478 secs

Thu Apr 21 21:08:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=536
Sat Apr 23 16:02:04 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.068 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 536,
    "message": "401 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 2,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.439 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 515.53 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 22.34 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.097 Time taken: 518.27 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.32 secs| Accuracy 98.248


======================================================================
----------------------------------------------------------------------
                run_ID=536
Sat Apr 23 16:24:59 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.171 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 536,
    "message": "401 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.205 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 523.78 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 23.55 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.097 Time taken: 518.35 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 23.71 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.067 Time taken: 519.04 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 23.64 secs| Accuracy 97.867
T\P      0       1
0       5301        0008        
1       0166        5142        

Accuracy 98.361
Time taken: 34.44 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.553 Time taken: 376.09 secs

    Epoch: 4 Validation Loss 0.421 Time taken: 2.48 secs| Accuracy 82.781

Epoch: 5 Train Loss 0.398 Time taken: 371.91 secs

    Epoch: 5 Validation Loss 0.316 Time taken: 2.41 secs| Accuracy 87.417
T\P      0       1
0       0166        0032        
1       0045        0211        

Accuracy 83.04
Time taken: 2.81 secs
2465.15 secs

Sat Apr 23 17:07:04 2022


======================================================================
----------------------------------------------------------------------
                run_ID=536
Sat Apr 23 17:32:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.062 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 536,
    "message": "401 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 8,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.339 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.292 Time taken: 579.34 secs

    Epoch: 1 Validation Loss 0.085 Time taken: 23.18 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.095 Time taken: 571.04 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 23.3 secs| Accuracy 98.192

Epoch: 3 Train Loss 0.065 Time taken: 576.21 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 23.27 secs| Accuracy 98.658
T\P      0       1
0       5256        0053        
1       0082        5226        

Accuracy 98.728
Time taken: 33.92 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.7 Time taken: 381.72 secs

    Epoch: 4 Validation Loss 0.421 Time taken: 2.48 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.413 Time taken: 377.09 secs

    Epoch: 5 Validation Loss 0.326 Time taken: 2.52 secs| Accuracy 87.086
T\P      0       1
0       0160        0038        
1       0041        0215        

Accuracy 82.599
Time taken: 2.88 secs
2647.085 secs

Sat Apr 23 18:17:42 2022


======================================================================
----------------------------------------------------------------------
                run_ID=526
Mon Apr 25 17:29:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.962 GB
35 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 526,
    "message": "rand sents off",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 121,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 11.235 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.593 Time taken: 625.58 secs

    Epoch: 1 Validation Loss 0.463 Time taken: 23.29 secs| Accuracy 72.718

Epoch: 2 Train Loss 0.286 Time taken: 573.51 secs

    Epoch: 2 Validation Loss 0.18 Time taken: 22.72 secs| Accuracy 92.879

Epoch: 3 Train Loss 0.155 Time taken: 571.98 secs

    Epoch: 3 Validation Loss 0.134 Time taken: 22.87 secs| Accuracy 94.942
T\P      0       1
0       5112        0197        
1       0324        4984        

Accuracy 95.093
Time taken: 34.12 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.715 Time taken: 413.59 secs

    Epoch: 4 Validation Loss 0.557 Time taken: 2.39 secs| Accuracy 68.874

Epoch: 5 Train Loss 0.487 Time taken: 406.45 secs

    Epoch: 5 Validation Loss 0.436 Time taken: 1.79 secs| Accuracy 81.457

Epoch: 6 Train Loss 0.344 Time taken: 428.28 secs

    Epoch: 6 Validation Loss 0.406 Time taken: 2.47 secs| Accuracy 80.132

Epoch: 7 Train Loss 0.289 Time taken: 459.86 secs

    Epoch: 7 Validation Loss 0.323 Time taken: 2.19 secs| Accuracy 85.099

Epoch: 8 Train Loss 0.195 Time taken: 461.82 secs

    Epoch: 8 Validation Loss 0.315 Time taken: 2.36 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.147 Time taken: 461.59 secs

    Epoch: 9 Validation Loss 0.301 Time taken: 2.23 secs| Accuracy 88.079
T\P      0       1
0       0182        0016        
1       0028        0228        

Accuracy 90.308
Time taken: 2.58 secs
4553.131 secs

Mon Apr 25 18:46:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=527
Mon Apr 25 18:46:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.769 GB
35 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 527,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 122,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.507 GB
28 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.444 Time taken: 672.43 secs

    Epoch: 1 Validation Loss 0.201 Time taken: 23.76 secs| Accuracy 92.102

Epoch: 2 Train Loss 0.166 Time taken: 665.27 secs

    Epoch: 2 Validation Loss 0.132 Time taken: 24.11 secs| Accuracy 94.999

Epoch: 3 Train Loss 0.107 Time taken: 666.87 secs

    Epoch: 3 Validation Loss 0.117 Time taken: 24.02 secs| Accuracy 95.535
T\P      0       1
0       5119        0190        
1       0258        5050        

Accuracy 95.78
Time taken: 35.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.729 Time taken: 469.71 secs

    Epoch: 4 Validation Loss 0.532 Time taken: 2.13 secs| Accuracy 70.53

Epoch: 5 Train Loss 0.447 Time taken: 475.04 secs

    Epoch: 5 Validation Loss 0.446 Time taken: 2.4 secs| Accuracy 81.457

Epoch: 6 Train Loss 0.359 Time taken: 462.36 secs

    Epoch: 6 Validation Loss 0.356 Time taken: 1.94 secs| Accuracy 83.775

Epoch: 7 Train Loss 0.256 Time taken: 464.96 secs

    Epoch: 7 Validation Loss 0.33 Time taken: 2.53 secs| Accuracy 85.43

Epoch: 8 Train Loss 0.192 Time taken: 461.28 secs

    Epoch: 8 Validation Loss 0.342 Time taken: 2.59 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.162 Time taken: 457.94 secs

    Epoch: 9 Validation Loss 0.339 Time taken: 2.48 secs| Accuracy 88.079
T\P      0       1
0       0158        0040        
1       0015        0241        

Accuracy 87.885
Time taken: 2.96 secs
4950.98 secs

Mon Apr 25 20:10:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=528
Mon Apr 25 20:10:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.935 GB
46 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 528,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 123,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.375 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.577 Time taken: 660.04 secs

    Epoch: 1 Validation Loss 0.343 Time taken: 23.78 secs| Accuracy 87.313

Epoch: 2 Train Loss 0.247 Time taken: 660.32 secs

    Epoch: 2 Validation Loss 0.18 Time taken: 24.04 secs| Accuracy 93.6

Epoch: 3 Train Loss 0.148 Time taken: 662.7 secs

    Epoch: 3 Validation Loss 0.129 Time taken: 24.04 secs| Accuracy 94.928
T\P      0       1
0       5110        0199        
1       0314        4994        

Accuracy 95.168
Time taken: 35.04 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.81 Time taken: 465.15 secs

    Epoch: 4 Validation Loss 0.643 Time taken: 1.86 secs| Accuracy 63.245

Epoch: 5 Train Loss 0.557 Time taken: 478.57 secs

    Epoch: 5 Validation Loss 0.487 Time taken: 2.22 secs| Accuracy 77.483

Epoch: 6 Train Loss 0.442 Time taken: 453.91 secs

    Epoch: 6 Validation Loss 0.413 Time taken: 1.9 secs| Accuracy 82.781

Epoch: 7 Train Loss 0.332 Time taken: 459.63 secs

    Epoch: 7 Validation Loss 0.342 Time taken: 2.06 secs| Accuracy 85.099

Epoch: 8 Train Loss 0.259 Time taken: 480.48 secs

    Epoch: 8 Validation Loss 0.354 Time taken: 2.21 secs| Accuracy 83.775

Epoch: 9 Train Loss 0.207 Time taken: 459.59 secs

    Epoch: 9 Validation Loss 0.358 Time taken: 2.2 secs| Accuracy 85.43
T\P      0       1
0       0165        0033        
1       0026        0230        

Accuracy 87.004
Time taken: 2.39 secs
4931.421 secs

Mon Apr 25 21:34:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=529
Mon Apr 25 21:34:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.154 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 529,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 124,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.428 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.495 Time taken: 688.87 secs

    Epoch: 1 Validation Loss 0.253 Time taken: 27.64 secs| Accuracy 89.503

Epoch: 2 Train Loss 0.202 Time taken: 694.54 secs

    Epoch: 2 Validation Loss 0.156 Time taken: 24.58 secs| Accuracy 94.575

Epoch: 3 Train Loss 0.123 Time taken: 666.14 secs

    Epoch: 3 Validation Loss 0.14 Time taken: 24.13 secs| Accuracy 94.928
T\P      0       1
0       4835        0474        
1       0099        5209        

Accuracy 94.603
Time taken: 35.66 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.743 Time taken: 491.51 secs

    Epoch: 4 Validation Loss 0.612 Time taken: 2.24 secs| Accuracy 64.238

Epoch: 5 Train Loss 0.554 Time taken: 478.1 secs

    Epoch: 5 Validation Loss 0.463 Time taken: 1.96 secs| Accuracy 80.464

Epoch: 6 Train Loss 0.376 Time taken: 476.76 secs

    Epoch: 6 Validation Loss 0.35 Time taken: 2.73 secs| Accuracy 84.106

Epoch: 7 Train Loss 0.287 Time taken: 462.73 secs

    Epoch: 7 Validation Loss 0.308 Time taken: 2.55 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.233 Time taken: 462.11 secs

    Epoch: 8 Validation Loss 0.312 Time taken: 2.75 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.164 Time taken: 464.19 secs

    Epoch: 9 Validation Loss 0.342 Time taken: 2.55 secs| Accuracy 86.424
T\P      0       1
0       0177        0021        
1       0044        0212        

Accuracy 85.683
Time taken: 2.58 secs
5045.487 secs

Mon Apr 25 22:59:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=530
Mon Apr 25 22:59:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.997 GB
42 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 530,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 125,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.379 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.451 Time taken: 672.82 secs

    Epoch: 1 Validation Loss 0.19 Time taken: 24.05 secs| Accuracy 92.922

Epoch: 2 Train Loss 0.173 Time taken: 684.84 secs

    Epoch: 2 Validation Loss 0.142 Time taken: 23.92 secs| Accuracy 94.49

Epoch: 3 Train Loss 0.111 Time taken: 666.34 secs

    Epoch: 3 Validation Loss 0.141 Time taken: 24.4 secs| Accuracy 95.083
T\P      0       1
0       5110        0199        
1       0316        4992        

Accuracy 95.149
Time taken: 35.4 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.837 Time taken: 487.14 secs

    Epoch: 4 Validation Loss 0.628 Time taken: 2.04 secs| Accuracy 59.272

Epoch: 5 Train Loss 0.585 Time taken: 478.84 secs

    Epoch: 5 Validation Loss 0.495 Time taken: 2.44 secs| Accuracy 76.821

Epoch: 6 Train Loss 0.442 Time taken: 481.46 secs

    Epoch: 6 Validation Loss 0.45 Time taken: 2.83 secs| Accuracy 79.47

Epoch: 7 Train Loss 0.39 Time taken: 465.43 secs

    Epoch: 7 Validation Loss 0.384 Time taken: 2.24 secs| Accuracy 84.106

Epoch: 8 Train Loss 0.288 Time taken: 462.96 secs

    Epoch: 8 Validation Loss 0.354 Time taken: 2.32 secs| Accuracy 85.43

Epoch: 9 Train Loss 0.209 Time taken: 466.02 secs

    Epoch: 9 Validation Loss 0.356 Time taken: 2.4 secs| Accuracy 87.086
T\P      0       1
0       0179        0019        
1       0055        0201        

Accuracy 83.7
Time taken: 2.78 secs
5018.643 secs

Tue Apr 26 00:24:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=531
Tue Apr 26 00:24:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.306 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 531,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.387 GB
36 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.586 Time taken: 659.85 secs

    Epoch: 1 Validation Loss 0.373 Time taken: 24.01 secs| Accuracy 88.033

Epoch: 2 Train Loss 0.258 Time taken: 655.54 secs

    Epoch: 2 Validation Loss 0.165 Time taken: 24.45 secs| Accuracy 93.769

Epoch: 3 Train Loss 0.138 Time taken: 686.68 secs

    Epoch: 3 Validation Loss 0.134 Time taken: 24.31 secs| Accuracy 95.083
T\P      0       1
0       5103        0206        
1       0280        5028        

Accuracy 95.422
Time taken: 35.43 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.819 Time taken: 470.11 secs

    Epoch: 4 Validation Loss 0.524 Time taken: 2.35 secs| Accuracy 78.146

Epoch: 5 Train Loss 0.474 Time taken: 453.57 secs

    Epoch: 5 Validation Loss 0.427 Time taken: 2.56 secs| Accuracy 81.788

Epoch: 6 Train Loss 0.347 Time taken: 469.86 secs

    Epoch: 6 Validation Loss 0.314 Time taken: 2.42 secs| Accuracy 85.099

Epoch: 7 Train Loss 0.258 Time taken: 458.18 secs

    Epoch: 7 Validation Loss 0.297 Time taken: 2.65 secs| Accuracy 85.43

Epoch: 8 Train Loss 0.218 Time taken: 456.4 secs

    Epoch: 8 Validation Loss 0.313 Time taken: 2.58 secs| Accuracy 88.742

Epoch: 9 Train Loss 0.174 Time taken: 455.55 secs

    Epoch: 9 Validation Loss 0.298 Time taken: 2.22 secs| Accuracy 88.742
T\P      0       1
0       0168        0030        
1       0032        0224        

Accuracy 86.344
Time taken: 2.76 secs
4918.06 secs

Tue Apr 26 01:47:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=532
Tue Apr 26 01:48:03 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.149 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 532,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.261 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.521 Time taken: 662.22 secs

    Epoch: 1 Validation Loss 0.344 Time taken: 24.09 secs| Accuracy 84.162

Epoch: 2 Train Loss 0.225 Time taken: 659.44 secs

    Epoch: 2 Validation Loss 0.151 Time taken: 24.05 secs| Accuracy 94.165

Epoch: 3 Train Loss 0.131 Time taken: 687.22 secs

    Epoch: 3 Validation Loss 0.129 Time taken: 23.9 secs| Accuracy 95.196
T\P      0       1
0       4915        0394        
1       0152        5156        

Accuracy 94.857
Time taken: 35.49 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.837 Time taken: 470.63 secs

    Epoch: 4 Validation Loss 0.62 Time taken: 2.49 secs| Accuracy 58.94

Epoch: 5 Train Loss 0.54 Time taken: 466.39 secs

    Epoch: 5 Validation Loss 0.471 Time taken: 2.37 secs| Accuracy 76.821

Epoch: 6 Train Loss 0.392 Time taken: 471.38 secs

    Epoch: 6 Validation Loss 0.379 Time taken: 2.32 secs| Accuracy 83.444

Epoch: 7 Train Loss 0.302 Time taken: 455.01 secs

    Epoch: 7 Validation Loss 0.329 Time taken: 2.57 secs| Accuracy 86.755

Epoch: 8 Train Loss 0.23 Time taken: 456.64 secs

    Epoch: 8 Validation Loss 0.296 Time taken: 2.38 secs| Accuracy 88.742

Epoch: 9 Train Loss 0.186 Time taken: 411.97 secs

    Epoch: 9 Validation Loss 0.305 Time taken: 1.93 secs| Accuracy 87.748
T\P      0       1
0       0178        0020        
1       0048        0208        

Accuracy 85.022
Time taken: 2.37 secs
4891.581 secs

Tue Apr 26 03:10:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=533
Tue Apr 26 10:05:39 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.005 GB
30 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 533,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.285 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.471 Time taken: 668.52 secs

    Epoch: 1 Validation Loss 0.245 Time taken: 24.44 secs| Accuracy 89.941

Epoch: 2 Train Loss 0.204 Time taken: 666.07 secs

    Epoch: 2 Validation Loss 0.17 Time taken: 23.54 secs| Accuracy 93.19

Epoch: 3 Train Loss 0.124 Time taken: 669.29 secs

    Epoch: 3 Validation Loss 0.143 Time taken: 23.92 secs| Accuracy 94.674
T\P      0       1
0       5128        0181        
1       0364        4944        

Accuracy 94.867
Time taken: 35.27 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.85 Time taken: 459.01 secs

    Epoch: 4 Validation Loss 0.659 Time taken: 2.12 secs| Accuracy 58.94

Epoch: 5 Train Loss 0.655 Time taken: 470.34 secs

    Epoch: 5 Validation Loss 0.646 Time taken: 2.84 secs| Accuracy 58.94

Epoch: 6 Train Loss 0.63 Time taken: 472.46 secs

    Epoch: 6 Validation Loss 0.593 Time taken: 2.51 secs| Accuracy 64.901

Epoch: 7 Train Loss 0.52 Time taken: 469.85 secs

    Epoch: 7 Validation Loss 0.544 Time taken: 3.29 secs| Accuracy 71.192

Epoch: 8 Train Loss 0.394 Time taken: 475.75 secs

    Epoch: 8 Validation Loss 0.33 Time taken: 1.91 secs| Accuracy 85.43

Epoch: 9 Train Loss 0.253 Time taken: 466.4 secs

    Epoch: 9 Validation Loss 0.313 Time taken: 2.25 secs| Accuracy 86.424
T\P      0       1
0       0157        0041        
1       0022        0234        

Accuracy 86.123
Time taken: 2.64 secs
4966.39 secs

Tue Apr 26 11:29:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=534
Tue Apr 26 11:29:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.069 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 534,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.361 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.465 Time taken: 671.41 secs

    Epoch: 1 Validation Loss 0.2 Time taken: 23.8 secs| Accuracy 92.314

Epoch: 2 Train Loss 0.179 Time taken: 664.42 secs

    Epoch: 2 Validation Loss 0.148 Time taken: 24.22 secs| Accuracy 94.306

Epoch: 3 Train Loss 0.111 Time taken: 667.1 secs

    Epoch: 3 Validation Loss 0.122 Time taken: 23.82 secs| Accuracy 95.352
T\P      0       1
0       5027        0282        
1       0168        5140        

Accuracy 95.762
Time taken: 35.66 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.881 Time taken: 474.3 secs

    Epoch: 4 Validation Loss 0.586 Time taken: 2.39 secs| Accuracy 69.205

Epoch: 5 Train Loss 0.525 Time taken: 476.79 secs

    Epoch: 5 Validation Loss 0.444 Time taken: 2.29 secs| Accuracy 81.457

Epoch: 6 Train Loss 0.39 Time taken: 461.14 secs

    Epoch: 6 Validation Loss 0.385 Time taken: 2.16 secs| Accuracy 85.43

Epoch: 7 Train Loss 0.281 Time taken: 473.36 secs

    Epoch: 7 Validation Loss 0.44 Time taken: 2.36 secs| Accuracy 82.45

Epoch: 8 Train Loss 0.242 Time taken: 464.52 secs

    Epoch: 8 Validation Loss 0.419 Time taken: 2.1 secs| Accuracy 84.437

Epoch: 9 Train Loss 0.186 Time taken: 462.59 secs

    Epoch: 9 Validation Loss 0.369 Time taken: 2.47 secs| Accuracy 85.43
T\P      0       1
0       0183        0015        
1       0035        0221        

Accuracy 88.987
Time taken: 2.64 secs
4965.873 secs

Tue Apr 26 12:53:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=535
Tue Apr 26 12:53:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.103 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 535,
    "message": "526 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.314 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.516 Time taken: 675.0 secs

    Epoch: 1 Validation Loss 0.243 Time taken: 23.6 secs| Accuracy 90.619

Epoch: 2 Train Loss 0.215 Time taken: 668.93 secs

    Epoch: 2 Validation Loss 0.145 Time taken: 24.15 secs| Accuracy 94.377

Epoch: 3 Train Loss 0.124 Time taken: 667.95 secs

    Epoch: 3 Validation Loss 0.131 Time taken: 23.35 secs| Accuracy 94.984
T\P      0       1
0       5109        0200        
1       0290        5018        

Accuracy 95.385
Time taken: 35.19 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.777 Time taken: 474.6 secs

    Epoch: 4 Validation Loss 0.582 Time taken: 1.84 secs| Accuracy 72.185

Epoch: 5 Train Loss 0.521 Time taken: 463.39 secs

    Epoch: 5 Validation Loss 0.493 Time taken: 1.78 secs| Accuracy 77.815

Epoch: 6 Train Loss 0.429 Time taken: 464.45 secs

    Epoch: 6 Validation Loss 0.38 Time taken: 1.85 secs| Accuracy 82.781

Epoch: 7 Train Loss 0.329 Time taken: 470.76 secs

    Epoch: 7 Validation Loss 0.363 Time taken: 2.48 secs| Accuracy 85.099

Epoch: 8 Train Loss 0.273 Time taken: 466.16 secs

    Epoch: 8 Validation Loss 0.305 Time taken: 2.01 secs| Accuracy 86.093

Epoch: 9 Train Loss 0.191 Time taken: 458.16 secs

    Epoch: 9 Validation Loss 0.355 Time taken: 1.9 secs| Accuracy 86.424
T\P      0       1
0       0172        0026        
1       0042        0214        

Accuracy 85.022
Time taken: 2.69 secs
4954.956 secs

Tue Apr 26 14:17:19 2022




======================================================================
----------------------------------------------------------------------
                run_ID=537
Wed Apr 27 10:16:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.047 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 537,
    "message": "188 with include asha data after inshorts",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.359 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.312 Time taken: 596.13 secs

    Epoch: 1 Validation Loss 0.104 Time taken: 22.46 secs| Accuracy 97.457


======================================================================
----------------------------------------------------------------------
                run_ID=537
Wed Apr 27 10:35:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.063 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 537,
    "message": "188 with include asha data after inshorts",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.36 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.312 Time taken: 602.94 secs

    Epoch: 1 Validation Loss 0.104 Time taken: 22.48 secs| Accuracy 97.457

Epoch: 2 Train Loss 0.118 Time taken: 598.99 secs

    Epoch: 2 Validation Loss 0.076 Time taken: 22.47 secs| Accuracy 97.739

Epoch: 3 Train Loss 0.082 Time taken: 604.33 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 22.41 secs| Accuracy 98.248
T\P      0       1
0       5205        0104        
1       0067        5241        

Accuracy 98.389
Time taken: 34.55 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.633 Time taken: 380.55 secs

    Epoch: 4 Validation Loss 0.448 Time taken: 1.52 secs| Accuracy 79.139

Epoch: 5 Train Loss 0.442 Time taken: 378.07 secs

    Epoch: 5 Validation Loss 0.365 Time taken: 1.55 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.387 Time taken: 442.53 secs

    Epoch: 6 Validation Loss 0.342 Time taken: 2.13 secs| Accuracy 85.762

Epoch: 7 Train Loss 0.293 Time taken: 446.21 secs

    Epoch: 7 Validation Loss 0.317 Time taken: 1.58 secs| Accuracy 88.079

Epoch: 8 Train Loss 0.252 Time taken: 410.77 secs

    Epoch: 8 Validation Loss 0.3 Time taken: 1.88 secs| Accuracy 90.397

Epoch: 9 Train Loss 0.219 Time taken: 414.78 secs

    Epoch: 9 Validation Loss 0.338 Time taken: 1.97 secs| Accuracy 87.748
T\P      0       1
0       0181        0017        
1       0044        0212        

Accuracy 86.564
Time taken: 2.3 secs
4425.79 secs

Wed Apr 27 11:50:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=538
Wed Apr 27 11:50:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.898 GB
37 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 538,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.355 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.322 Time taken: 664.16 secs

    Epoch: 1 Validation Loss 0.122 Time taken: 22.96 secs| Accuracy 97.344

Epoch: 2 Train Loss 0.141 Time taken: 614.96 secs

    Epoch: 2 Validation Loss 0.08 Time taken: 24.36 secs| Accuracy 97.669

Epoch: 3 Train Loss 0.113 Time taken: 599.18 secs

    Epoch: 3 Validation Loss 0.143 Time taken: 22.67 secs| Accuracy 95.521
T\P      0       1
0       5296        0013        
1       0434        4874        

Accuracy 95.79
Time taken: 33.67 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.588 Time taken: 522.98 secs

    Epoch: 4 Validation Loss 0.538 Time taken: 3.2 secs| Accuracy 78.808

Epoch: 5 Train Loss 0.504 Time taken: 554.86 secs

    Epoch: 5 Validation Loss 0.418 Time taken: 2.84 secs| Accuracy 80.795

Epoch: 6 Train Loss 0.42 Time taken: 528.13 secs

    Epoch: 6 Validation Loss 0.376 Time taken: 2.62 secs| Accuracy 83.113

Epoch: 7 Train Loss 0.378 Time taken: 504.23 secs

    Epoch: 7 Validation Loss 0.338 Time taken: 2.02 secs| Accuracy 84.106

Epoch: 8 Train Loss 0.317 Time taken: 477.33 secs

    Epoch: 8 Validation Loss 0.33 Time taken: 2.23 secs| Accuracy 84.106

Epoch: 9 Train Loss 0.291 Time taken: 465.62 secs

    Epoch: 9 Validation Loss 0.295 Time taken: 2.33 secs| Accuracy 85.762
T\P      0       1
0       0151        0047        
1       0026        0230        

Accuracy 83.921
Time taken: 2.49 secs
5082.23 secs

Wed Apr 27 13:16:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=539
Wed Apr 27 13:16:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.346 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 539,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.892 GB
32 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.223 Time taken: 720.71 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 24.41 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.073 Time taken: 726.32 secs

    Epoch: 2 Validation Loss 0.041 Time taken: 25.98 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.047 Time taken: 714.96 secs

    Epoch: 3 Validation Loss 0.047 Time taken: 24.51 secs| Accuracy 98.813
T\P      0       1
0       5255        0054        
1       0056        5252        

Accuracy 98.964
Time taken: 36.93 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.675 Time taken: 484.68 secs

    Epoch: 4 Validation Loss 0.495 Time taken: 2.29 secs| Accuracy 82.119

Epoch: 5 Train Loss 0.426 Time taken: 484.06 secs

    Epoch: 5 Validation Loss 0.352 Time taken: 2.15 secs| Accuracy 85.099

Epoch: 6 Train Loss 0.304 Time taken: 486.85 secs

    Epoch: 6 Validation Loss 0.325 Time taken: 2.39 secs| Accuracy 85.762

Epoch: 7 Train Loss 0.232 Time taken: 488.12 secs

    Epoch: 7 Validation Loss 0.299 Time taken: 2.51 secs| Accuracy 85.099

Epoch: 8 Train Loss 0.183 Time taken: 490.39 secs

    Epoch: 8 Validation Loss 0.303 Time taken: 3.54 secs| Accuracy 88.079

Epoch: 9 Train Loss 0.129 Time taken: 489.07 secs

    Epoch: 9 Validation Loss 0.303 Time taken: 2.83 secs| Accuracy 89.073
T\P      0       1
0       0173        0025        
1       0024        0232        

Accuracy 89.207
Time taken: 2.36 secs
5250.033 secs

Wed Apr 27 14:45:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=540
Wed Apr 27 14:45:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.197 GB
38 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 540,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.291 GB
34 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.28 Time taken: 680.71 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 23.98 secs| Accuracy 98.333

Epoch: 2 Train Loss 0.081 Time taken: 687.43 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.94 secs| Accuracy 98.672

Epoch: 3 Train Loss 0.053 Time taken: 689.2 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 24.02 secs| Accuracy 98.177
T\P      0       1
0       5191        0118        
1       0041        5267        

Accuracy 98.502
Time taken: 36.08 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.647 Time taken: 463.19 secs

    Epoch: 4 Validation Loss 0.465 Time taken: 2.28 secs| Accuracy 80.464

Epoch: 5 Train Loss 0.389 Time taken: 466.87 secs

    Epoch: 5 Validation Loss 0.473 Time taken: 2.09 secs| Accuracy 82.781

Epoch: 6 Train Loss 0.324 Time taken: 466.21 secs

    Epoch: 6 Validation Loss 0.451 Time taken: 2.72 secs| Accuracy 83.775

Epoch: 7 Train Loss 0.241 Time taken: 461.01 secs

    Epoch: 7 Validation Loss 0.353 Time taken: 2.21 secs| Accuracy 85.762

Epoch: 8 Train Loss 0.186 Time taken: 461.55 secs

    Epoch: 8 Validation Loss 0.371 Time taken: 2.16 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.162 Time taken: 467.61 secs

    Epoch: 9 Validation Loss 0.427 Time taken: 2.47 secs| Accuracy 86.424
T\P      0       1
0       0182        0016        
1       0041        0215        

Accuracy 87.445
Time taken: 2.96 secs
5003.161 secs

Wed Apr 27 16:10:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=541
Wed Apr 27 16:10:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.468 GB
36 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 541,
    "message": "404 with_ different random seed",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 7.502 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.255 Time taken: 687.19 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 24.23 secs| Accuracy 98.319

Epoch: 2 Train Loss 0.073 Time taken: 688.47 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 25.09 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.049 Time taken: 685.85 secs

    Epoch: 3 Validation Loss 0.038 Time taken: 24.21 secs| Accuracy 98.94
T\P      0       1
0       5248        0061        
1       0054        5254        

Accuracy 98.917
Time taken: 35.78 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.672 Time taken: 464.32 secs

    Epoch: 4 Validation Loss 0.495 Time taken: 2.36 secs| Accuracy 79.47

Epoch: 5 Train Loss 0.415 Time taken: 461.96 secs

    Epoch: 5 Validation Loss 0.4 Time taken: 2.49 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.322 Time taken: 466.53 secs

    Epoch: 6 Validation Loss 0.346 Time taken: 2.43 secs| Accuracy 87.417

Epoch: 7 Train Loss 0.23 Time taken: 466.27 secs

    Epoch: 7 Validation Loss 0.388 Time taken: 2.17 secs| Accuracy 84.437

Epoch: 8 Train Loss 0.207 Time taken: 464.3 secs

    Epoch: 8 Validation Loss 0.327 Time taken: 2.66 secs| Accuracy 86.755

Epoch: 9 Train Loss 0.15 Time taken: 464.95 secs

    Epoch: 9 Validation Loss 0.346 Time taken: 1.97 secs| Accuracy 86.755
T\P      0       1
0       0186        0012        
1       0044        0212        

Accuracy 87.665
Time taken: 2.68 secs
5001.265 secs

Wed Apr 27 17:34:43 2022


======================================================================
----------------------------------------------------------------------
                run_ID=566
Thu Apr 28 11:58:32 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.894 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 566,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.523 GB
29 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.244 Time taken: 692.69 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 24.74 secs| Accuracy 98.192

Epoch: 2 Train Loss 0.08 Time taken: 700.27 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.9 secs| Accuracy 98.573

Epoch: 3 Train Loss 0.049 Time taken: 691.98 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 24.24 secs| Accuracy 98.474
T\P      0       1
0       5200        0109        
1       0040        5268        

Accuracy 98.597
Time taken: 35.53 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.674 Time taken: 471.88 secs

    Epoch: 4 Validation Loss 0.504 Time taken: 2.11 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.4 Time taken: 478.93 secs

    Epoch: 5 Validation Loss 0.266 Time taken: 2.28 secs| Accuracy 87.748

Epoch: 6 Train Loss 0.26 Time taken: 490.53 secs

    Epoch: 6 Validation Loss 0.251 Time taken: 2.79 secs| Accuracy 90.066

Epoch: 7 Train Loss 0.176 Time taken: 503.52 secs

    Epoch: 7 Validation Loss 0.219 Time taken: 2.31 secs| Accuracy 92.053

Epoch: 8 Train Loss 0.135 Time taken: 517.72 secs

    Epoch: 8 Validation Loss 0.24 Time taken: 2.03 secs| Accuracy 92.053

Epoch: 9 Train Loss 0.097 Time taken: 509.24 secs

    Epoch: 9 Validation Loss 0.269 Time taken: 2.53 secs| Accuracy 91.06
T\P      0       1
0       0169        0029        
1       0019        0237        

Accuracy 89.427
Time taken: 3.26 secs
5211.038 secs

Thu Apr 28 13:26:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=567
Thu Apr 28 13:26:42 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.224 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 567,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 3.998 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.246 Time taken: 704.09 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 24.58 secs| Accuracy 98.389

Epoch: 2 Train Loss 0.072 Time taken: 705.17 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 25.0 secs| Accuracy 98.672

Epoch: 3 Train Loss 0.049 Time taken: 718.77 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 24.73 secs| Accuracy 97.994
T\P      0       1
0       5167        0142        
1       0034        5274        

Accuracy 98.342
Time taken: 35.94 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.557 Time taken: 485.53 secs

    Epoch: 4 Validation Loss 0.405 Time taken: 2.86 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.392 Time taken: 474.72 secs

    Epoch: 5 Validation Loss 0.351 Time taken: 2.09 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.232 Time taken: 465.8 secs

    Epoch: 6 Validation Loss 0.309 Time taken: 1.88 secs| Accuracy 89.073

Epoch: 7 Train Loss 0.172 Time taken: 463.9 secs

    Epoch: 7 Validation Loss 0.296 Time taken: 2.05 secs| Accuracy 89.735

Epoch: 8 Train Loss 0.131 Time taken: 462.7 secs

    Epoch: 8 Validation Loss 0.318 Time taken: 2.43 secs| Accuracy 88.742

Epoch: 9 Train Loss 0.085 Time taken: 463.57 secs

    Epoch: 9 Validation Loss 0.406 Time taken: 1.89 secs| Accuracy 89.073
T\P      0       1
0       0172        0026        
1       0028        0228        

Accuracy 88.106
Time taken: 2.4 secs
5096.139 secs

Thu Apr 28 14:52:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=568
Thu Apr 28 14:52:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.209 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 568,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.749 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.23 Time taken: 678.61 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 24.09 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.074 Time taken: 689.13 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 23.99 secs| Accuracy 98.573

Epoch: 3 Train Loss 0.044 Time taken: 686.54 secs

    Epoch: 3 Validation Loss 0.036 Time taken: 24.2 secs| Accuracy 98.884
T\P      0       1
0       5271        0038        
1       0056        5252        

Accuracy 99.115
Time taken: 34.85 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.664 Time taken: 464.3 secs

    Epoch: 4 Validation Loss 0.508 Time taken: 2.05 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.463 Time taken: 472.97 secs

    Epoch: 5 Validation Loss 0.382 Time taken: 1.89 secs| Accuracy 82.781

Epoch: 6 Train Loss 0.315 Time taken: 469.48 secs

    Epoch: 6 Validation Loss 0.34 Time taken: 2.3 secs| Accuracy 87.748

Epoch: 7 Train Loss 0.223 Time taken: 466.87 secs

    Epoch: 7 Validation Loss 0.28 Time taken: 1.89 secs| Accuracy 89.073

Epoch: 8 Train Loss 0.168 Time taken: 469.99 secs

    Epoch: 8 Validation Loss 0.318 Time taken: 2.16 secs| Accuracy 90.397

Epoch: 9 Train Loss 0.131 Time taken: 470.68 secs

    Epoch: 9 Validation Loss 0.274 Time taken: 1.83 secs| Accuracy 88.411
T\P      0       1
0       0170        0028        
1       0016        0240        

Accuracy 90.308
Time taken: 2.9 secs
5023.487 secs

Thu Apr 28 16:17:46 2022


======================================================================
----------------------------------------------------------------------
                run_ID=569
Thu Apr 28 16:17:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.172 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 569,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.983 GB
26 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.247 Time taken: 689.02 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 23.67 secs| Accuracy 98.446

Epoch: 2 Train Loss 0.072 Time taken: 686.87 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 24.18 secs| Accuracy 98.728

Epoch: 3 Train Loss 0.05 Time taken: 685.7 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 23.97 secs| Accuracy 98.658
T\P      0       1
0       5237        0072        
1       0060        5248        

Accuracy 98.757
Time taken: 35.51 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.584 Time taken: 467.01 secs

    Epoch: 4 Validation Loss 0.455 Time taken: 1.87 secs| Accuracy 79.47

Epoch: 5 Train Loss 0.338 Time taken: 472.44 secs

    Epoch: 5 Validation Loss 0.378 Time taken: 2.29 secs| Accuracy 85.762

Epoch: 6 Train Loss 0.225 Time taken: 464.82 secs

    Epoch: 6 Validation Loss 0.439 Time taken: 2.54 secs| Accuracy 86.093

Epoch: 7 Train Loss 0.213 Time taken: 459.03 secs

    Epoch: 7 Validation Loss 0.339 Time taken: 2.14 secs| Accuracy 85.099

Epoch: 8 Train Loss 0.192 Time taken: 467.43 secs

    Epoch: 8 Validation Loss 0.358 Time taken: 2.23 secs| Accuracy 86.755

Epoch: 9 Train Loss 0.151 Time taken: 467.38 secs

    Epoch: 9 Validation Loss 0.382 Time taken: 2.03 secs| Accuracy 86.424
T\P      0       1
0       0168        0030        
1       0026        0230        

Accuracy 87.665
Time taken: 3.19 secs
5009.399 secs

Thu Apr 28 17:42:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=570
Thu Apr 28 17:42:38 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.681 GB
35 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 570,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 6.037 GB
28 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.249 Time taken: 695.7 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 24.3 secs| Accuracy 98.121

Epoch: 2 Train Loss 0.08 Time taken: 697.84 secs

    Epoch: 2 Validation Loss 0.046 Time taken: 24.56 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.049 Time taken: 692.87 secs

    Epoch: 3 Validation Loss 0.041 Time taken: 23.84 secs| Accuracy 98.63
T\P      0       1
0       5226        0083        
1       0037        5271        

Accuracy 98.87
Time taken: 35.25 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.656 Time taken: 469.39 secs

    Epoch: 4 Validation Loss 0.512 Time taken: 2.68 secs| Accuracy 80.132

Epoch: 5 Train Loss 0.414 Time taken: 471.44 secs

    Epoch: 5 Validation Loss 0.451 Time taken: 2.45 secs| Accuracy 80.132

Epoch: 6 Train Loss 0.307 Time taken: 471.5 secs

    Epoch: 6 Validation Loss 0.341 Time taken: 1.94 secs| Accuracy 84.437

Epoch: 7 Train Loss 0.214 Time taken: 475.26 secs

    Epoch: 7 Validation Loss 0.326 Time taken: 2.28 secs| Accuracy 86.093

Epoch: 8 Train Loss 0.17 Time taken: 472.49 secs

    Epoch: 8 Validation Loss 0.368 Time taken: 1.8 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.121 Time taken: 478.31 secs

    Epoch: 9 Validation Loss 0.387 Time taken: 1.94 secs| Accuracy 87.086
T\P      0       1
0       0186        0012        
1       0039        0217        

Accuracy 88.767
Time taken: 2.28 secs
5074.499 secs

Thu Apr 28 19:08:25 2022


======================================================================
----------------------------------------------------------------------
                run_ID=571
Thu Apr 28 19:08:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.822 GB
34 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 571,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.516 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.23 Time taken: 692.85 secs

    Epoch: 1 Validation Loss 0.059 Time taken: 23.9 secs| Accuracy 98.432

Epoch: 2 Train Loss 0.068 Time taken: 697.18 secs

    Epoch: 2 Validation Loss 0.044 Time taken: 23.57 secs| Accuracy 98.686

Epoch: 3 Train Loss 0.047 Time taken: 701.17 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 24.23 secs| Accuracy 98.319
T\P      0       1
0       5182        0127        
1       0034        5274        

Accuracy 98.484
Time taken: 35.36 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.713 Time taken: 473.87 secs

    Epoch: 4 Validation Loss 0.613 Time taken: 1.94 secs| Accuracy 58.94

Epoch: 5 Train Loss 0.511 Time taken: 480.76 secs

    Epoch: 5 Validation Loss 0.527 Time taken: 1.93 secs| Accuracy 76.821

Epoch: 6 Train Loss 0.367 Time taken: 476.95 secs

    Epoch: 6 Validation Loss 0.337 Time taken: 2.51 secs| Accuracy 86.424

Epoch: 7 Train Loss 0.235 Time taken: 477.05 secs

    Epoch: 7 Validation Loss 0.342 Time taken: 2.07 secs| Accuracy 86.755

Epoch: 8 Train Loss 0.178 Time taken: 484.8 secs

    Epoch: 8 Validation Loss 0.442 Time taken: 2.19 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.159 Time taken: 488.01 secs

    Epoch: 9 Validation Loss 0.359 Time taken: 2.17 secs| Accuracy 88.742
T\P      0       1
0       0165        0033        
1       0030        0226        

Accuracy 86.123
Time taken: 2.95 secs
5122.498 secs

Thu Apr 28 20:35:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=572
Thu Apr 28 20:35:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.527 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 572,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM -0.25 GB
34 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.225 Time taken: 712.35 secs

    Epoch: 1 Validation Loss 0.056 Time taken: 23.93 secs| Accuracy 98.177

Epoch: 2 Train Loss 0.075 Time taken: 709.46 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.85 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.054 Time taken: 702.28 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 23.93 secs| Accuracy 98.813
T\P      0       1
0       5266        0043        
1       0077        5231        

Accuracy 98.87
Time taken: 35.77 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.582 Time taken: 480.14 secs

    Epoch: 4 Validation Loss 0.45 Time taken: 1.78 secs| Accuracy 81.457

Epoch: 5 Train Loss 0.361 Time taken: 487.64 secs

    Epoch: 5 Validation Loss 0.385 Time taken: 2.21 secs| Accuracy 83.775

Epoch: 6 Train Loss 0.25 Time taken: 492.6 secs

    Epoch: 6 Validation Loss 0.472 Time taken: 2.63 secs| Accuracy 83.113

Epoch: 7 Train Loss 0.165 Time taken: 491.89 secs

    Epoch: 7 Validation Loss 0.358 Time taken: 2.11 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.149 Time taken: 489.73 secs

    Epoch: 8 Validation Loss 0.391 Time taken: 2.44 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.108 Time taken: 492.86 secs

    Epoch: 9 Validation Loss 0.469 Time taken: 2.0 secs| Accuracy 83.775
T\P      0       1
0       0159        0039        
1       0017        0239        

Accuracy 87.665
Time taken: 2.72 secs
5208.697 secs

Thu Apr 28 22:03:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=573
Thu Apr 28 22:03:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.898 GB
35 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 573,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.253 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.226 Time taken: 706.93 secs

    Epoch: 1 Validation Loss 0.061 Time taken: 24.37 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.069 Time taken: 708.34 secs

    Epoch: 2 Validation Loss 0.042 Time taken: 24.77 secs| Accuracy 98.799

Epoch: 3 Train Loss 0.049 Time taken: 704.83 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 24.12 secs| Accuracy 98.601
T\P      0       1
0       5292        0017        
1       0137        5171        

Accuracy 98.549
Time taken: 35.85 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.517 Time taken: 480.48 secs

    Epoch: 4 Validation Loss 0.401 Time taken: 2.37 secs| Accuracy 84.768

Epoch: 5 Train Loss 0.317 Time taken: 483.14 secs

    Epoch: 5 Validation Loss 0.318 Time taken: 2.16 secs| Accuracy 84.768

Epoch: 6 Train Loss 0.246 Time taken: 487.49 secs

    Epoch: 6 Validation Loss 0.285 Time taken: 2.01 secs| Accuracy 87.748

Epoch: 7 Train Loss 0.163 Time taken: 478.4 secs

    Epoch: 7 Validation Loss 0.263 Time taken: 2.04 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.117 Time taken: 480.49 secs

    Epoch: 8 Validation Loss 0.313 Time taken: 2.14 secs| Accuracy 88.411

Epoch: 9 Train Loss 0.082 Time taken: 483.26 secs

    Epoch: 9 Validation Loss 0.29 Time taken: 2.08 secs| Accuracy 89.073
T\P      0       1
0       0182        0016        
1       0038        0218        

Accuracy 88.106
Time taken: 2.78 secs
5162.502 secs

Thu Apr 28 23:30:34 2022


======================================================================
----------------------------------------------------------------------
                run_ID=574
Thu Apr 28 23:30:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.04 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 574,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.364 GB
27 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.229 Time taken: 713.85 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 24.02 secs| Accuracy 98.064

Epoch: 2 Train Loss 0.087 Time taken: 711.82 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 24.27 secs| Accuracy 98.644

Epoch: 3 Train Loss 0.056 Time taken: 711.36 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.85 secs| Accuracy 98.771
T\P      0       1
0       5230        0079        
1       0052        5256        

Accuracy 98.766
Time taken: 35.03 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.672 Time taken: 478.72 secs

    Epoch: 4 Validation Loss 0.438 Time taken: 2.04 secs| Accuracy 80.464

Epoch: 5 Train Loss 0.382 Time taken: 482.58 secs

    Epoch: 5 Validation Loss 0.361 Time taken: 2.12 secs| Accuracy 83.113

Epoch: 6 Train Loss 0.305 Time taken: 491.24 secs

    Epoch: 6 Validation Loss 0.352 Time taken: 2.14 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.239 Time taken: 477.27 secs

    Epoch: 7 Validation Loss 0.298 Time taken: 2.06 secs| Accuracy 86.755

Epoch: 8 Train Loss 0.196 Time taken: 482.78 secs

    Epoch: 8 Validation Loss 0.299 Time taken: 2.57 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.141 Time taken: 479.8 secs

    Epoch: 9 Validation Loss 0.323 Time taken: 2.63 secs| Accuracy 87.417
T\P      0       1
0       0177        0021        
1       0047        0209        

Accuracy 85.022
Time taken: 2.85 secs
5179.348 secs

Fri Apr 29 00:58:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=575
Fri Apr 29 00:58:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.307 GB
34 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 575,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.243 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.279 Time taken: 691.72 secs

    Epoch: 1 Validation Loss 0.087 Time taken: 23.79 secs| Accuracy 97.824

Epoch: 2 Train Loss 0.099 Time taken: 701.05 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 24.09 secs| Accuracy 98.446

Epoch: 3 Train Loss 0.065 Time taken: 712.23 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.88 secs| Accuracy 98.601
T\P      0       1
0       5267        0042        
1       0094        5214        

Accuracy 98.719
Time taken: 35.42 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.591 Time taken: 479.27 secs

    Epoch: 4 Validation Loss 0.443 Time taken: 2.23 secs| Accuracy 80.795

Epoch: 5 Train Loss 0.39 Time taken: 486.34 secs

    Epoch: 5 Validation Loss 0.371 Time taken: 2.19 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.283 Time taken: 483.16 secs

    Epoch: 6 Validation Loss 0.343 Time taken: 2.29 secs| Accuracy 87.417

Epoch: 7 Train Loss 0.215 Time taken: 474.55 secs

    Epoch: 7 Validation Loss 0.295 Time taken: 1.8 secs| Accuracy 89.404

Epoch: 8 Train Loss 0.166 Time taken: 478.1 secs

    Epoch: 8 Validation Loss 0.419 Time taken: 2.03 secs| Accuracy 83.775

Epoch: 9 Train Loss 0.181 Time taken: 478.14 secs

    Epoch: 9 Validation Loss 0.304 Time taken: 2.03 secs| Accuracy 91.06
T\P      0       1
0       0178        0020        
1       0027        0229        

Accuracy 89.648
Time taken: 2.83 secs
5131.57 secs

Fri Apr 29 02:25:00 2022


======================================================================
----------------------------------------------------------------------
                run_ID=578
Fri Apr 29 10:05:27 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.893 GB
37 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 578,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.854 GB
28 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.311 Time taken: 687.01 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 23.67 secs| Accuracy 97.612

Epoch: 2 Train Loss 0.138 Time taken: 687.71 secs

    Epoch: 2 Validation Loss 0.098 Time taken: 24.2 secs| Accuracy 97.429

Epoch: 3 Train Loss 0.098 Time taken: 686.68 secs

    Epoch: 3 Validation Loss 0.068 Time taken: 23.97 secs| Accuracy 97.909
T\P      0       1
0       5277        0032        
1       0152        5156        

Accuracy 98.267
Time taken: 35.85 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.624 Time taken: 485.88 secs

    Epoch: 4 Validation Loss 0.561 Time taken: 1.72 secs| Accuracy 75.166
T\P      0       1
0       0157        0041        
1       0074        0182        

Accuracy 74.67
Time taken: 2.49 secs
2685.858 secs

Fri Apr 29 10:51:27 2022


======================================================================
----------------------------------------------------------------------
                run_ID=579
Fri Apr 29 10:51:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.698 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 579,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "xlm-roberta-base",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCEwithLogits",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): XLMRoberta xlm-roberta-base )lementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Identity()
  (criterion): BCEWithLogitsLoss()
)
Memory taken on GPU 1.038 GB
Memory taken on RAM 5.555 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.302 Time taken: 697.63 secs

    Epoch: 1 Validation Loss 0.099 Time taken: 23.62 secs| Accuracy 97.358

Epoch: 2 Train Loss 0.105 Time taken: 683.92 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 24.03 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.07 Time taken: 687.41 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.77 secs| Accuracy 98.587
T\P      0       1
0       5246        0063        
1       0072        5236        

Accuracy 98.728
Time taken: 35.07 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.683 Time taken: 475.59 secs

    Epoch: 4 Validation Loss 0.577 Time taken: 2.34 secs| Accuracy 69.205
T\P      0       1
0       0193        0005        
1       0140        0116        

Accuracy 68.062
Time taken: 2.67 secs
2682.155 secs

Fri Apr 29 11:37:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=608
Fri Apr 29 17:26:15 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 7.678 GB
35 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 608,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.028 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 566.31 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.29 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.061 Time taken: 564.5 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 22.38 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.032 Time taken: 563.74 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.4 secs| Accuracy 98.347
T\P      0       1
0       5218        0091        
1       0055        5253        

Accuracy 98.625
Time taken: 33.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.669 Time taken: 373.72 secs

    Epoch: 4 Validation Loss 0.365 Time taken: 1.68 secs| Accuracy 85.43

Epoch: 5 Train Loss 0.294 Time taken: 373.19 secs

    Epoch: 5 Validation Loss 0.277 Time taken: 1.63 secs| Accuracy 85.43

Epoch: 6 Train Loss 0.205 Time taken: 371.08 secs

    Epoch: 6 Validation Loss 0.268 Time taken: 1.59 secs| Accuracy 87.417

Epoch: 7 Train Loss 0.148 Time taken: 371.18 secs

    Epoch: 7 Validation Loss 0.321 Time taken: 1.62 secs| Accuracy 87.748

Epoch: 8 Train Loss 0.092 Time taken: 370.97 secs

    Epoch: 8 Validation Loss 0.364 Time taken: 1.63 secs| Accuracy 86.755

Epoch: 9 Train Loss 0.062 Time taken: 370.58 secs

    Epoch: 9 Validation Loss 0.347 Time taken: 1.64 secs| Accuracy 88.411
T\P      0       1
0       0175        0023        
1       0031        0225        

Accuracy 88.106
Time taken: 1.99 secs
4058.683 secs

Fri Apr 29 18:34:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=609
Fri Apr 29 18:35:00 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.961 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 609,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM -8.543 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.168 Time taken: 562.74 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 22.39 secs| Accuracy 98.234

Epoch: 2 Train Loss 0.057 Time taken: 563.49 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.45 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.031 Time taken: 563.5 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 22.43 secs| Accuracy 98.559
T\P      0       1
0       5260        0049        
1       0064        5244        

Accuracy 98.936
Time taken: 33.25 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.491 Time taken: 369.93 secs

    Epoch: 4 Validation Loss 0.307 Time taken: 1.7 secs| Accuracy 87.086

Epoch: 5 Train Loss 0.24 Time taken: 370.06 secs

    Epoch: 5 Validation Loss 0.301 Time taken: 1.77 secs| Accuracy 88.079

Epoch: 6 Train Loss 0.15 Time taken: 370.06 secs

    Epoch: 6 Validation Loss 0.329 Time taken: 1.67 secs| Accuracy 89.073

Epoch: 7 Train Loss 0.094 Time taken: 370.02 secs

    Epoch: 7 Validation Loss 0.322 Time taken: 1.75 secs| Accuracy 89.073

Epoch: 8 Train Loss 0.059 Time taken: 370.62 secs

    Epoch: 8 Validation Loss 0.413 Time taken: 1.81 secs| Accuracy 87.086

Epoch: 9 Train Loss 0.049 Time taken: 370.6 secs

    Epoch: 9 Validation Loss 0.452 Time taken: 1.78 secs| Accuracy 88.079
T\P      0       1
0       0180        0018        
1       0035        0221        

Accuracy 88.326
Time taken: 2.19 secs
4045.457 secs

Fri Apr 29 19:43:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=610
Fri Apr 29 19:43:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.045 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 610,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.977 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.186 Time taken: 579.05 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 22.46 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.055 Time taken: 579.13 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 22.41 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.032 Time taken: 578.71 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.46 secs| Accuracy 98.375
T\P      0       1
0       5210        0099        
1       0052        5256        

Accuracy 98.578
Time taken: 33.43 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.678 Time taken: 371.06 secs

    Epoch: 4 Validation Loss 0.467 Time taken: 1.69 secs| Accuracy 84.768

Epoch: 5 Train Loss 0.328 Time taken: 371.17 secs

    Epoch: 5 Validation Loss 0.298 Time taken: 1.72 secs| Accuracy 87.086

Epoch: 6 Train Loss 0.204 Time taken: 371.03 secs

    Epoch: 6 Validation Loss 0.356 Time taken: 1.71 secs| Accuracy 87.748

Epoch: 7 Train Loss 0.134 Time taken: 371.15 secs

    Epoch: 7 Validation Loss 0.316 Time taken: 1.67 secs| Accuracy 89.404

Epoch: 8 Train Loss 0.083 Time taken: 371.35 secs

    Epoch: 8 Validation Loss 0.381 Time taken: 1.75 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.086 Time taken: 371.37 secs

    Epoch: 9 Validation Loss 0.436 Time taken: 1.68 secs| Accuracy 87.086
T\P      0       1
0       0169        0029        
1       0021        0235        

Accuracy 88.987
Time taken: 2.15 secs
4098.496 secs

Fri Apr 29 20:52:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=611
Fri Apr 29 20:52:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.868 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 611,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.98 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.184 Time taken: 579.72 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.37 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.061 Time taken: 578.81 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 22.36 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.034 Time taken: 578.53 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.53 secs| Accuracy 98.389
T\P      0       1
0       5278        0031        
1       0134        5174        

Accuracy 98.446
Time taken: 33.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.497 Time taken: 370.36 secs

    Epoch: 4 Validation Loss 0.295 Time taken: 1.78 secs| Accuracy 87.748

Epoch: 5 Train Loss 0.218 Time taken: 370.98 secs

    Epoch: 5 Validation Loss 0.273 Time taken: 1.76 secs| Accuracy 88.079

Epoch: 6 Train Loss 0.168 Time taken: 370.85 secs

    Epoch: 6 Validation Loss 0.284 Time taken: 1.81 secs| Accuracy 87.086

Epoch: 7 Train Loss 0.088 Time taken: 370.76 secs

    Epoch: 7 Validation Loss 0.309 Time taken: 1.83 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.078 Time taken: 370.68 secs

    Epoch: 8 Validation Loss 0.409 Time taken: 1.78 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.059 Time taken: 370.83 secs

    Epoch: 9 Validation Loss 0.357 Time taken: 1.72 secs| Accuracy 88.411
T\P      0       1
0       0160        0038        
1       0019        0237        

Accuracy 87.445
Time taken: 2.2 secs
4096.466 secs

Fri Apr 29 22:01:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=612
Fri Apr 29 22:01:55 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.856 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 612,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.004 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.179 Time taken: 579.08 secs

    Epoch: 1 Validation Loss 0.089 Time taken: 22.27 secs| Accuracy 97.245

Epoch: 2 Train Loss 0.053 Time taken: 579.13 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.25 secs| Accuracy 98.46

Epoch: 3 Train Loss 0.031 Time taken: 578.31 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.45 secs| Accuracy 98.545
T\P      0       1
0       5280        0029        
1       0093        5215        

Accuracy 98.851
Time taken: 33.3 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.515 Time taken: 369.92 secs

    Epoch: 4 Validation Loss 0.311 Time taken: 1.74 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.265 Time taken: 370.11 secs

    Epoch: 5 Validation Loss 0.268 Time taken: 1.8 secs| Accuracy 87.086

Epoch: 6 Train Loss 0.201 Time taken: 370.31 secs

    Epoch: 6 Validation Loss 0.31 Time taken: 1.84 secs| Accuracy 88.079

Epoch: 7 Train Loss 0.161 Time taken: 370.33 secs

    Epoch: 7 Validation Loss 0.29 Time taken: 1.81 secs| Accuracy 87.086

Epoch: 8 Train Loss 0.169 Time taken: 370.18 secs

    Epoch: 8 Validation Loss 0.355 Time taken: 1.8 secs| Accuracy 88.742

Epoch: 9 Train Loss 0.088 Time taken: 370.4 secs

    Epoch: 9 Validation Loss 0.367 Time taken: 1.78 secs| Accuracy 87.086
T\P      0       1
0       0155        0043        
1       0021        0235        

Accuracy 85.903
Time taken: 2.21 secs
4094.218 secs

Fri Apr 29 23:11:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=613
Fri Apr 29 23:11:06 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.867 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 613,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.983 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.179 Time taken: 577.94 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.26 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.057 Time taken: 578.2 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.28 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.031 Time taken: 577.84 secs

    Epoch: 3 Validation Loss 0.072 Time taken: 22.46 secs| Accuracy 97.838
T\P      0       1
0       5212        0097        
1       0088        5220        

Accuracy 98.258
Time taken: 33.27 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.631 Time taken: 370.06 secs

    Epoch: 4 Validation Loss 0.502 Time taken: 1.68 secs| Accuracy 81.126

Epoch: 5 Train Loss 0.308 Time taken: 370.32 secs

    Epoch: 5 Validation Loss 0.354 Time taken: 1.73 secs| Accuracy 86.755

Epoch: 6 Train Loss 0.219 Time taken: 370.38 secs

    Epoch: 6 Validation Loss 0.416 Time taken: 1.81 secs| Accuracy 84.768

Epoch: 7 Train Loss 0.142 Time taken: 370.3 secs

    Epoch: 7 Validation Loss 0.385 Time taken: 1.69 secs| Accuracy 84.768

Epoch: 8 Train Loss 0.091 Time taken: 370.5 secs

    Epoch: 8 Validation Loss 0.439 Time taken: 1.75 secs| Accuracy 86.093

Epoch: 9 Train Loss 0.043 Time taken: 369.99 secs

    Epoch: 9 Validation Loss 0.516 Time taken: 1.67 secs| Accuracy 85.762
T\P      0       1
0       0181        0017        
1       0030        0226        

Accuracy 89.648
Time taken: 2.15 secs
4090.557 secs

Sat Apr 30 00:20:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=614
Sat Apr 30 00:20:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.524 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 614,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.989 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.183 Time taken: 578.96 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 22.3 secs| Accuracy 97.994

Epoch: 2 Train Loss 0.059 Time taken: 578.58 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 22.37 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.034 Time taken: 578.89 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.47 secs| Accuracy 98.404
T\P      0       1
0       5271        0038        
1       0093        5215        

Accuracy 98.766
Time taken: 33.26 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.461 Time taken: 369.74 secs

    Epoch: 4 Validation Loss 0.383 Time taken: 1.71 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.253 Time taken: 370.12 secs

    Epoch: 5 Validation Loss 0.297 Time taken: 1.7 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.153 Time taken: 370.79 secs

    Epoch: 6 Validation Loss 0.33 Time taken: 1.69 secs| Accuracy 87.086

Epoch: 7 Train Loss 0.11 Time taken: 370.09 secs

    Epoch: 7 Validation Loss 0.399 Time taken: 1.7 secs| Accuracy 86.093

Epoch: 8 Train Loss 0.092 Time taken: 369.79 secs

    Epoch: 8 Validation Loss 0.343 Time taken: 1.72 secs| Accuracy 86.424

Epoch: 9 Train Loss 0.059 Time taken: 369.71 secs

    Epoch: 9 Validation Loss 0.374 Time taken: 1.74 secs| Accuracy 86.093
T\P      0       1
0       0174        0024        
1       0028        0228        

Accuracy 88.546
Time taken: 2.18 secs
4090.553 secs

Sat Apr 30 01:29:18 2022


======================================================================
----------------------------------------------------------------------
                run_ID=615
Sat Apr 30 01:29:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.876 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 615,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.985 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.175 Time taken: 579.3 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.36 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.05 Time taken: 578.45 secs

    Epoch: 2 Validation Loss 0.064 Time taken: 22.5 secs| Accuracy 98.192

Epoch: 3 Train Loss 0.028 Time taken: 579.09 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.38 secs| Accuracy 98.474
T\P      0       1
0       5222        0087        
1       0056        5252        

Accuracy 98.653
Time taken: 33.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.59 Time taken: 370.9 secs

    Epoch: 4 Validation Loss 0.308 Time taken: 1.72 secs| Accuracy 85.762

Epoch: 5 Train Loss 0.236 Time taken: 370.66 secs

    Epoch: 5 Validation Loss 0.284 Time taken: 1.78 secs| Accuracy 88.411

Epoch: 6 Train Loss 0.167 Time taken: 370.97 secs

    Epoch: 6 Validation Loss 0.29 Time taken: 1.68 secs| Accuracy 88.079

Epoch: 7 Train Loss 0.103 Time taken: 370.81 secs

    Epoch: 7 Validation Loss 0.356 Time taken: 1.68 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.089 Time taken: 370.26 secs

    Epoch: 8 Validation Loss 0.359 Time taken: 1.71 secs| Accuracy 88.079

Epoch: 9 Train Loss 0.055 Time taken: 369.64 secs

    Epoch: 9 Validation Loss 0.334 Time taken: 1.69 secs| Accuracy 89.404
T\P      0       1
0       0167        0031        
1       0020        0236        

Accuracy 88.767
Time taken: 2.1 secs
4094.512 secs

Sat Apr 30 02:38:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=616
Sat Apr 30 02:38:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.783 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 616,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.981 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.192 Time taken: 577.92 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.37 secs| Accuracy 98.036

Epoch: 2 Train Loss 0.059 Time taken: 577.35 secs

    Epoch: 2 Validation Loss 0.045 Time taken: 22.35 secs| Accuracy 98.615

Epoch: 3 Train Loss 0.033 Time taken: 577.57 secs

    Epoch: 3 Validation Loss 0.051 Time taken: 22.33 secs| Accuracy 98.404
T\P      0       1
0       5214        0095        
1       0055        5253        

Accuracy 98.587
Time taken: 33.13 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.561 Time taken: 369.18 secs

    Epoch: 4 Validation Loss 0.399 Time taken: 1.68 secs| Accuracy 85.099

Epoch: 5 Train Loss 0.274 Time taken: 369.35 secs

    Epoch: 5 Validation Loss 0.323 Time taken: 1.66 secs| Accuracy 84.106

Epoch: 6 Train Loss 0.186 Time taken: 369.78 secs

    Epoch: 6 Validation Loss 0.415 Time taken: 1.69 secs| Accuracy 85.099

Epoch: 7 Train Loss 0.112 Time taken: 369.64 secs

    Epoch: 7 Validation Loss 0.449 Time taken: 1.77 secs| Accuracy 84.768

Epoch: 8 Train Loss 0.086 Time taken: 369.84 secs

    Epoch: 8 Validation Loss 0.524 Time taken: 1.66 secs| Accuracy 83.775

Epoch: 9 Train Loss 0.08 Time taken: 369.56 secs

    Epoch: 9 Validation Loss 0.523 Time taken: 1.73 secs| Accuracy 84.106
T\P      0       1
0       0191        0007        
1       0067        0189        

Accuracy 83.7
Time taken: 2.11 secs
4085.687 secs

Sat Apr 30 03:47:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=617
Sat Apr 30 03:47:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.865 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 617,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.983 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.177 Time taken: 577.01 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 22.33 secs| Accuracy 98.29

Epoch: 2 Train Loss 0.056 Time taken: 577.57 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 22.38 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.033 Time taken: 577.67 secs

    Epoch: 3 Validation Loss 0.044 Time taken: 22.38 secs| Accuracy 98.728
T\P      0       1
0       5233        0076        
1       0060        5248        

Accuracy 98.719
Time taken: 33.19 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.497 Time taken: 369.55 secs

    Epoch: 4 Validation Loss 0.315 Time taken: 1.85 secs| Accuracy 86.093

Epoch: 5 Train Loss 0.239 Time taken: 369.73 secs

    Epoch: 5 Validation Loss 0.285 Time taken: 1.77 secs| Accuracy 89.735

Epoch: 6 Train Loss 0.155 Time taken: 370.53 secs

    Epoch: 6 Validation Loss 0.264 Time taken: 1.78 secs| Accuracy 90.728

Epoch: 7 Train Loss 0.107 Time taken: 370.38 secs

    Epoch: 7 Validation Loss 0.288 Time taken: 1.68 secs| Accuracy 89.735

Epoch: 8 Train Loss 0.056 Time taken: 369.7 secs

    Epoch: 8 Validation Loss 0.311 Time taken: 1.68 secs| Accuracy 89.735

Epoch: 9 Train Loss 0.032 Time taken: 369.74 secs

    Epoch: 9 Validation Loss 0.452 Time taken: 1.72 secs| Accuracy 89.735
T\P      0       1
0       0193        0005        
1       0050        0206        

Accuracy 87.885
Time taken: 2.13 secs
4088.373 secs

Sat Apr 30 04:56:40 2022


======================================================================
----------------------------------------------------------------------
                run_ID=618
Sat Apr 30 04:56:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.858 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 618,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.983 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.187 Time taken: 578.99 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 22.46 secs| Accuracy 97.994

Epoch: 2 Train Loss 0.059 Time taken: 578.03 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.45 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.031 Time taken: 577.62 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.51 secs| Accuracy 98.587
T\P      0       1
0       5244        0065        
1       0051        5257        

Accuracy 98.907
Time taken: 33.35 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.637 Time taken: 369.2 secs

    Epoch: 4 Validation Loss 0.317 Time taken: 1.95 secs| Accuracy 86.424

Epoch: 5 Train Loss 0.274 Time taken: 370.71 secs

    Epoch: 5 Validation Loss 0.271 Time taken: 2.0 secs| Accuracy 88.742

Epoch: 6 Train Loss 0.194 Time taken: 371.01 secs

    Epoch: 6 Validation Loss 0.249 Time taken: 1.98 secs| Accuracy 88.411

Epoch: 7 Train Loss 0.137 Time taken: 371.07 secs

    Epoch: 7 Validation Loss 0.313 Time taken: 1.96 secs| Accuracy 89.404

Epoch: 8 Train Loss 0.093 Time taken: 371.35 secs

    Epoch: 8 Validation Loss 0.311 Time taken: 2.0 secs| Accuracy 90.397

Epoch: 9 Train Loss 0.083 Time taken: 370.94 secs

    Epoch: 9 Validation Loss 0.369 Time taken: 2.02 secs| Accuracy 88.079
T\P      0       1
0       0189        0009        
1       0050        0206        

Accuracy 87.004
Time taken: 2.46 secs
4095.846 secs

Sat Apr 30 06:05:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=619
Sat Apr 30 06:05:56 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.874 GB
27 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 619,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.974 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.169 Time taken: 577.84 secs

    Epoch: 1 Validation Loss 0.06 Time taken: 22.32 secs| Accuracy 98.418

Epoch: 2 Train Loss 0.055 Time taken: 576.65 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 22.38 secs| Accuracy 98.46

Epoch: 3 Train Loss 0.03 Time taken: 576.74 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.36 secs| Accuracy 98.46
T\P      0       1
0       5217        0092        
1       0041        5267        

Accuracy 98.747
Time taken: 33.12 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.52 Time taken: 368.97 secs

    Epoch: 4 Validation Loss 0.327 Time taken: 1.63 secs| Accuracy 85.762

Epoch: 5 Train Loss 0.24 Time taken: 369.32 secs

    Epoch: 5 Validation Loss 0.323 Time taken: 1.69 secs| Accuracy 89.073

Epoch: 6 Train Loss 0.172 Time taken: 369.76 secs

    Epoch: 6 Validation Loss 0.328 Time taken: 1.59 secs| Accuracy 89.404

Epoch: 7 Train Loss 0.115 Time taken: 369.19 secs

    Epoch: 7 Validation Loss 0.298 Time taken: 1.67 secs| Accuracy 88.742

Epoch: 8 Train Loss 0.064 Time taken: 369.26 secs

    Epoch: 8 Validation Loss 0.371 Time taken: 1.59 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.048 Time taken: 369.24 secs

    Epoch: 9 Validation Loss 0.456 Time taken: 1.61 secs| Accuracy 89.404
T\P      0       1
0       0182        0016        
1       0030        0226        

Accuracy 89.868
Time taken: 2.02 secs
4080.658 secs

Sat Apr 30 07:14:52 2022


======================================================================
----------------------------------------------------------------------
                run_ID=640
Sat Apr 30 22:37:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.529 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 640,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.981 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.181 Time taken: 567.16 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.19 secs| Accuracy 98.022

Epoch: 2 Train Loss 0.065 Time taken: 639.73 secs

    Epoch: 2 Validation Loss 0.147 Time taken: 23.22 secs| Accuracy 96.101

Epoch: 3 Train Loss 0.036 Time taken: 614.48 secs

    Epoch: 3 Validation Loss 0.055 Time taken: 22.64 secs| Accuracy 98.46
T\P      0       1
0       5242        0067        
1       0067        5241        

Accuracy 98.738
Time taken: 33.76 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.533 Time taken: 381.55 secs

    Epoch: 4 Validation Loss 0.316 Time taken: 1.45 secs| Accuracy 86.424

Epoch: 5 Train Loss 0.255 Time taken: 436.76 secs

    Epoch: 5 Validation Loss 0.252 Time taken: 1.78 secs| Accuracy 88.742

Epoch: 6 Train Loss 0.179 Time taken: 453.84 secs

    Epoch: 6 Validation Loss 0.311 Time taken: 1.61 secs| Accuracy 86.424

Epoch: 7 Train Loss 0.137 Time taken: 444.31 secs

    Epoch: 7 Validation Loss 0.266 Time taken: 1.62 secs| Accuracy 89.073

Epoch: 8 Train Loss 0.081 Time taken: 448.77 secs

    Epoch: 8 Validation Loss 0.318 Time taken: 1.63 secs| Accuracy 90.728

Epoch: 9 Train Loss 0.058 Time taken: 413.49 secs

    Epoch: 9 Validation Loss 0.347 Time taken: 1.56 secs| Accuracy 89.073
T\P      0       1
0       0175        0023        
1       0038        0218        

Accuracy 86.564
Time taken: 2.09 secs
4536.509 secs

Sat Apr 30 23:53:42 2022


======================================================================
----------------------------------------------------------------------
                run_ID=641
Sat Apr 30 23:53:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.972 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 641,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.047 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.134 Time taken: 584.85 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 22.2 secs| Accuracy 98.276

Epoch: 2 Train Loss 0.049 Time taken: 569.6 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.27 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.027 Time taken: 570.06 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 22.33 secs| Accuracy 98.389
T\P      0       1
0       5215        0094        
1       0055        5253        

Accuracy 98.597
Time taken: 33.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.564 Time taken: 373.67 secs

    Epoch: 4 Validation Loss 0.317 Time taken: 1.48 secs| Accuracy 86.093

Epoch: 5 Train Loss 0.266 Time taken: 373.7 secs

    Epoch: 5 Validation Loss 0.273 Time taken: 1.5 secs| Accuracy 86.093

Epoch: 6 Train Loss 0.189 Time taken: 373.27 secs

    Epoch: 6 Validation Loss 0.304 Time taken: 1.49 secs| Accuracy 86.755

Epoch: 7 Train Loss 0.145 Time taken: 373.57 secs

    Epoch: 7 Validation Loss 0.338 Time taken: 1.5 secs| Accuracy 86.424

Epoch: 8 Train Loss 0.084 Time taken: 373.56 secs

    Epoch: 8 Validation Loss 0.426 Time taken: 1.49 secs| Accuracy 85.099

Epoch: 9 Train Loss 0.048 Time taken: 372.72 secs

    Epoch: 9 Validation Loss 0.433 Time taken: 1.49 secs| Accuracy 87.417
T\P      0       1
0       0179        0019        
1       0044        0212        

Accuracy 86.123
Time taken: 1.92 secs
4097.776 secs

Sun May  1 01:03:05 2022


======================================================================
----------------------------------------------------------------------
                run_ID=642
Sun May  1 01:03:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 642,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.991 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.153 Time taken: 572.89 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.38 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.051 Time taken: 570.53 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.42 secs| Accuracy 98.107

Epoch: 3 Train Loss 0.032 Time taken: 572.41 secs

    Epoch: 3 Validation Loss 0.069 Time taken: 22.48 secs| Accuracy 97.937
T\P      0       1
0       5175        0134        
1       0049        5259        

Accuracy 98.276
Time taken: 33.32 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.53 Time taken: 371.45 secs

    Epoch: 4 Validation Loss 0.371 Time taken: 1.5 secs| Accuracy 84.106

Epoch: 5 Train Loss 0.251 Time taken: 372.79 secs

    Epoch: 5 Validation Loss 0.329 Time taken: 1.46 secs| Accuracy 85.099

Epoch: 6 Train Loss 0.164 Time taken: 371.62 secs

    Epoch: 6 Validation Loss 0.358 Time taken: 1.46 secs| Accuracy 85.099

Epoch: 7 Train Loss 0.104 Time taken: 371.69 secs

    Epoch: 7 Validation Loss 0.391 Time taken: 1.41 secs| Accuracy 84.768

Epoch: 8 Train Loss 0.079 Time taken: 371.66 secs

    Epoch: 8 Validation Loss 0.53 Time taken: 1.48 secs| Accuracy 85.099

Epoch: 9 Train Loss 0.065 Time taken: 371.41 secs

    Epoch: 9 Validation Loss 0.505 Time taken: 1.49 secs| Accuracy 85.43
T\P      0       1
0       0177        0021        
1       0032        0224        

Accuracy 88.326
Time taken: 1.9 secs
4080.256 secs

Sun May  1 02:12:02 2022


======================================================================
----------------------------------------------------------------------
                run_ID=643
Sun May  1 02:12:05 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.872 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 643,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.99 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.148 Time taken: 576.51 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.34 secs| Accuracy 97.98

Epoch: 2 Train Loss 0.057 Time taken: 571.41 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.31 secs| Accuracy 98.432

Epoch: 3 Train Loss 0.033 Time taken: 573.77 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.36 secs| Accuracy 98.474
T\P      0       1
0       5241        0068        
1       0066        5242        

Accuracy 98.738
Time taken: 33.34 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.479 Time taken: 371.94 secs

    Epoch: 4 Validation Loss 0.324 Time taken: 1.57 secs| Accuracy 82.45

Epoch: 5 Train Loss 0.236 Time taken: 372.78 secs

    Epoch: 5 Validation Loss 0.306 Time taken: 1.51 secs| Accuracy 84.768

Epoch: 6 Train Loss 0.168 Time taken: 373.26 secs

    Epoch: 6 Validation Loss 0.331 Time taken: 1.49 secs| Accuracy 85.43

Epoch: 7 Train Loss 0.105 Time taken: 371.72 secs

    Epoch: 7 Validation Loss 0.509 Time taken: 1.46 secs| Accuracy 85.762

Epoch: 8 Train Loss 0.055 Time taken: 371.28 secs

    Epoch: 8 Validation Loss 0.469 Time taken: 1.49 secs| Accuracy 85.762

Epoch: 9 Train Loss 0.028 Time taken: 371.16 secs

    Epoch: 9 Validation Loss 0.62 Time taken: 1.49 secs| Accuracy 85.099
T\P      0       1
0       0176        0022        
1       0032        0224        

Accuracy 88.106
Time taken: 1.9 secs
4086.956 secs

Sun May  1 03:21:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=644
Sun May  1 03:21:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.865 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 644,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc2",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=1, bias=True)
  (activation2): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.986 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.162 Time taken: 568.83 secs

    Epoch: 1 Validation Loss 0.064 Time taken: 22.3 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.064 Time taken: 566.89 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.25 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.037 Time taken: 566.8 secs

    Epoch: 3 Validation Loss 0.048 Time taken: 22.27 secs| Accuracy 98.672
T\P      0       1
0       5257        0052        
1       0082        5226        

Accuracy 98.738
Time taken: 33.2 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.556 Time taken: 372.2 secs

    Epoch: 4 Validation Loss 0.325 Time taken: 1.44 secs| Accuracy 84.437

Epoch: 5 Train Loss 0.258 Time taken: 371.94 secs

    Epoch: 5 Validation Loss 0.263 Time taken: 1.43 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.189 Time taken: 371.96 secs

    Epoch: 6 Validation Loss 0.313 Time taken: 1.46 secs| Accuracy 84.106

Epoch: 7 Train Loss 0.122 Time taken: 371.88 secs

    Epoch: 7 Validation Loss 0.336 Time taken: 1.44 secs| Accuracy 87.417

Epoch: 8 Train Loss 0.07 Time taken: 371.72 secs

    Epoch: 8 Validation Loss 0.37 Time taken: 1.44 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.067 Time taken: 372.28 secs

    Epoch: 9 Validation Loss 0.46 Time taken: 1.42 secs| Accuracy 86.093
T\P      0       1
0       0186        0012        
1       0062        0194        

Accuracy 83.7
Time taken: 1.87 secs
4066.474 secs

Sun May  1 04:29:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=645
Sun May  1 04:29:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 645,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.981 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.13 Time taken: 567.59 secs

    Epoch: 1 Validation Loss 0.053 Time taken: 22.33 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.047 Time taken: 566.94 secs

    Epoch: 2 Validation Loss 0.048 Time taken: 22.24 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.023 Time taken: 565.64 secs

    Epoch: 3 Validation Loss 0.042 Time taken: 22.23 secs| Accuracy 98.672
T\P      0       1
0       5232        0077        
1       0057        5251        

Accuracy 98.738
Time taken: 33.07 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.479 Time taken: 371.19 secs

    Epoch: 4 Validation Loss 0.301 Time taken: 1.42 secs| Accuracy 86.424

Epoch: 5 Train Loss 0.241 Time taken: 371.38 secs

    Epoch: 5 Validation Loss 0.283 Time taken: 1.41 secs| Accuracy 88.411

Epoch: 6 Train Loss 0.14 Time taken: 371.59 secs

    Epoch: 6 Validation Loss 0.369 Time taken: 1.42 secs| Accuracy 86.093

Epoch: 7 Train Loss 0.098 Time taken: 371.62 secs

    Epoch: 7 Validation Loss 0.323 Time taken: 1.45 secs| Accuracy 89.735

Epoch: 8 Train Loss 0.05 Time taken: 372.34 secs

    Epoch: 8 Validation Loss 0.477 Time taken: 1.44 secs| Accuracy 87.417

Epoch: 9 Train Loss 0.067 Time taken: 372.17 secs

    Epoch: 9 Validation Loss 0.395 Time taken: 1.47 secs| Accuracy 89.073
T\P      0       1
0       0177        0021        
1       0032        0224        

Accuracy 88.326
Time taken: 1.97 secs
4062.437 secs

Sun May  1 05:38:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=646
Sun May  1 05:38:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.871 GB
28 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 646,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.982 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.129 Time taken: 568.37 secs

    Epoch: 1 Validation Loss 0.062 Time taken: 22.41 secs| Accuracy 97.838

Epoch: 2 Train Loss 0.051 Time taken: 566.91 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.4 secs| Accuracy 98.319

Epoch: 3 Train Loss 0.029 Time taken: 566.77 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.36 secs| Accuracy 98.305
T\P      0       1
0       5234        0075        
1       0054        5254        

Accuracy 98.785
Time taken: 33.28 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.65 Time taken: 371.71 secs

    Epoch: 4 Validation Loss 0.351 Time taken: 1.54 secs| Accuracy 83.113

Epoch: 5 Train Loss 0.286 Time taken: 371.74 secs

    Epoch: 5 Validation Loss 0.339 Time taken: 1.63 secs| Accuracy 84.437

Epoch: 6 Train Loss 0.214 Time taken: 371.31 secs

    Epoch: 6 Validation Loss 0.285 Time taken: 1.51 secs| Accuracy 85.762

Epoch: 7 Train Loss 0.151 Time taken: 371.1 secs

    Epoch: 7 Validation Loss 0.363 Time taken: 1.57 secs| Accuracy 84.106

Epoch: 8 Train Loss 0.093 Time taken: 370.57 secs

    Epoch: 8 Validation Loss 0.374 Time taken: 1.54 secs| Accuracy 85.762

Epoch: 9 Train Loss 0.059 Time taken: 371.92 secs

    Epoch: 9 Validation Loss 0.433 Time taken: 1.52 secs| Accuracy 87.748
T\P      0       1
0       0173        0025        
1       0034        0222        

Accuracy 87.004
Time taken: 1.92 secs
4065.181 secs

Sun May  1 06:47:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=647
Sun May  1 06:47:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.87 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 647,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.994 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.14 Time taken: 564.01 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.22 secs| Accuracy 97.853

Epoch: 2 Train Loss 0.055 Time taken: 564.92 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.15 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.031 Time taken: 565.13 secs

    Epoch: 3 Validation Loss 0.046 Time taken: 22.21 secs| Accuracy 98.615
T\P      0       1
0       5263        0046        
1       0079        5229        

Accuracy 98.823
Time taken: 33.0 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.472 Time taken: 370.38 secs

    Epoch: 4 Validation Loss 0.355 Time taken: 1.54 secs| Accuracy 85.099

Epoch: 5 Train Loss 0.254 Time taken: 370.78 secs

    Epoch: 5 Validation Loss 0.314 Time taken: 1.48 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.171 Time taken: 371.65 secs

    Epoch: 6 Validation Loss 0.321 Time taken: 1.53 secs| Accuracy 86.093

Epoch: 7 Train Loss 0.128 Time taken: 370.53 secs

    Epoch: 7 Validation Loss 0.317 Time taken: 1.5 secs| Accuracy 88.411

Epoch: 8 Train Loss 0.073 Time taken: 370.85 secs

    Epoch: 8 Validation Loss 0.38 Time taken: 1.45 secs| Accuracy 87.748

Epoch: 9 Train Loss 0.044 Time taken: 371.65 secs

    Epoch: 9 Validation Loss 0.503 Time taken: 1.5 secs| Accuracy 88.411
T\P      0       1
0       0165        0033        
1       0013        0243        

Accuracy 89.868
Time taken: 1.92 secs
4052.645 secs

Sun May  1 07:55:42 2022


======================================================================
----------------------------------------------------------------------
                run_ID=648
Sun May  1 07:55:45 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.873 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 648,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 4.981 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.139 Time taken: 566.33 secs

    Epoch: 1 Validation Loss 0.058 Time taken: 22.36 secs| Accuracy 98.248

Epoch: 2 Train Loss 0.046 Time taken: 564.55 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.27 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.026 Time taken: 563.97 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.28 secs| Accuracy 98.474
T\P      0       1
0       5228        0081        
1       0061        5247        

Accuracy 98.663
Time taken: 33.12 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.589 Time taken: 370.86 secs

    Epoch: 4 Validation Loss 0.382 Time taken: 1.48 secs| Accuracy 82.781

Epoch: 5 Train Loss 0.287 Time taken: 371.43 secs

    Epoch: 5 Validation Loss 0.316 Time taken: 1.46 secs| Accuracy 86.424

Epoch: 6 Train Loss 0.176 Time taken: 371.16 secs

    Epoch: 6 Validation Loss 0.288 Time taken: 1.49 secs| Accuracy 86.093

Epoch: 7 Train Loss 0.105 Time taken: 370.81 secs

    Epoch: 7 Validation Loss 0.453 Time taken: 1.44 secs| Accuracy 85.43

Epoch: 8 Train Loss 0.106 Time taken: 370.8 secs

    Epoch: 8 Validation Loss 0.361 Time taken: 1.52 secs| Accuracy 84.106

Epoch: 9 Train Loss 0.089 Time taken: 371.16 secs

    Epoch: 9 Validation Loss 0.455 Time taken: 1.45 secs| Accuracy 86.755
T\P      0       1
0       0190        0008        
1       0034        0222        

Accuracy 90.749
Time taken: 1.89 secs
4054.726 secs

Sun May  1 09:04:15 2022


======================================================================
----------------------------------------------------------------------
                run_ID=649
Sun May  1 09:04:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.87 GB
26 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 649,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 5.002 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.133 Time taken: 568.08 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.3 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.049 Time taken: 566.34 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.36 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.027 Time taken: 563.42 secs

    Epoch: 3 Validation Loss 0.071 Time taken: 22.4 secs| Accuracy 97.937
T\P      0       1
0       5136        0173        
1       0036        5272        

Accuracy 98.031
Time taken: 33.39 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.708 Time taken: 369.43 secs

    Epoch: 4 Validation Loss 0.432 Time taken: 1.56 secs| Accuracy 84.768


======================================================================
----------------------------------------------------------------------
                run_ID=649
Sun May  1 09:49:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.377 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 649,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc1",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=1, bias=True)
  (activation1): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.663 GB
Memory taken on RAM 5.16 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.133 Time taken: 563.71 secs

    Epoch: 1 Validation Loss 0.066 Time taken: 22.14 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.049 Time taken: 564.17 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.18 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.027 Time taken: 564.82 secs

    Epoch: 3 Validation Loss 0.071 Time taken: 22.18 secs| Accuracy 97.937
T\P      0       1
0       5136        0173        
1       0036        5272        

Accuracy 98.031
Time taken: 33.04 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.708 Time taken: 369.69 secs

    Epoch: 4 Validation Loss 0.432 Time taken: 1.41 secs| Accuracy 84.768
T\P      0       1
0       0148        0050        
1       0017        0239        

Accuracy 85.242
Time taken: 1.85 secs
2188.012 secs

Sun May  1 10:26:51 2022


======================================================================
----------------------------------------------------------------------
                run_ID=680
Sun May  1 10:26:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.904 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 680,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.357 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 557.4 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 22.26 secs| Accuracy 97.838

Epoch: 2 Train Loss 0.06 Time taken: 558.65 secs

    Epoch: 2 Validation Loss 0.071 Time taken: 22.25 secs| Accuracy 98.022

Epoch: 3 Train Loss 0.035 Time taken: 557.97 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.26 secs| Accuracy 98.432
T\P      0       1
0       5243        0066        
1       0077        5231        

Accuracy 98.653
Time taken: 33.18 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.587 Time taken: 370.09 secs

    Epoch: 4 Validation Loss 0.343 Time taken: 1.37 secs| Accuracy 86.093
T\P      0       1
0       0169        0029        
1       0038        0218        

Accuracy 85.242
Time taken: 1.86 secs
2168.4 secs

Sun May  1 11:03:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=681
Sun May  1 11:04:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.901 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 681,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.9 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.2 Time taken: 557.33 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 22.32 secs| Accuracy 97.994

Epoch: 2 Train Loss 0.062 Time taken: 566.45 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.49 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.033 Time taken: 564.96 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.4 secs| Accuracy 98.432
T\P      0       1
0       5271        0038        
1       0114        5194        

Accuracy 98.568
Time taken: 33.41 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.517 Time taken: 373.78 secs

    Epoch: 4 Validation Loss 0.317 Time taken: 1.69 secs| Accuracy 88.742
T\P      0       1
0       0171        0027        
1       0040        0216        

Accuracy 85.242
Time taken: 2.15 secs
2189.302 secs

Sun May  1 11:41:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=682
Sun May  1 11:41:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -4.125 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 682,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 7.373 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 566.26 secs

    Epoch: 1 Validation Loss 0.074 Time taken: 22.3 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.06 Time taken: 589.36 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.87 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.032 Time taken: 593.08 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 22.73 secs| Accuracy 98.29
T\P      0       1
0       5238        0071        
1       0061        5247        

Accuracy 98.757
Time taken: 33.82 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.539 Time taken: 401.3 secs

    Epoch: 4 Validation Loss 0.315 Time taken: 1.61 secs| Accuracy 85.099
T\P      0       1
0       0159        0039        
1       0029        0227        

Accuracy 85.022
Time taken: 2.11 secs
2277.102 secs

Sun May  1 12:20:22 2022


======================================================================
----------------------------------------------------------------------
                run_ID=683
Sun May  1 12:20:26 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.913 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 683,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.016 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 577.08 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 22.59 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.059 Time taken: 577.12 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.68 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.031 Time taken: 590.51 secs

    Epoch: 3 Validation Loss 0.083 Time taken: 22.87 secs| Accuracy 97.513
T\P      0       1
0       5269        0040        
1       0203        5105        

Accuracy 97.711
Time taken: 33.88 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.504 Time taken: 399.39 secs

    Epoch: 4 Validation Loss 0.39 Time taken: 1.58 secs| Accuracy 82.45
T\P      0       1
0       0180        0018        
1       0055        0201        

Accuracy 83.921
Time taken: 2.14 secs
2272.389 secs

Sun May  1 12:59:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=684
Sun May  1 12:59:23 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.832 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 684,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.045 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 619.23 secs

    Epoch: 1 Validation Loss 0.076 Time taken: 23.16 secs| Accuracy 97.966

Epoch: 2 Train Loss 0.057 Time taken: 605.99 secs

    Epoch: 2 Validation Loss 0.057 Time taken: 22.94 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.033 Time taken: 616.69 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 23.06 secs| Accuracy 98.446
T\P      0       1
0       5242        0067        
1       0076        5232        

Accuracy 98.653
Time taken: 34.33 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.569 Time taken: 424.77 secs

    Epoch: 4 Validation Loss 0.364 Time taken: 1.77 secs| Accuracy 84.106
T\P      0       1
0       0169        0029        
1       0034        0222        

Accuracy 86.123
Time taken: 2.29 secs
2397.346 secs

Sun May  1 13:40:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=685
Sun May  1 13:40:30 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.033 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 685,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.105 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.196 Time taken: 582.23 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.73 secs| Accuracy 97.824

Epoch: 2 Train Loss 0.059 Time taken: 576.32 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.66 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.036 Time taken: 582.66 secs

    Epoch: 3 Validation Loss 0.054 Time taken: 22.78 secs| Accuracy 98.446
T\P      0       1
0       5247        0062        
1       0079        5229        

Accuracy 98.672
Time taken: 33.87 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.579 Time taken: 401.43 secs

    Epoch: 4 Validation Loss 0.346 Time taken: 1.74 secs| Accuracy 86.424
T\P      0       1
0       0169        0029        
1       0033        0223        

Accuracy 86.344
Time taken: 2.16 secs
2270.902 secs

Sun May  1 14:19:23 2022


======================================================================
----------------------------------------------------------------------
                run_ID=686
Sun May  1 14:19:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.356 GB
37 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 686,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.132 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.199 Time taken: 583.38 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.92 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.062 Time taken: 583.75 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.71 secs| Accuracy 98.064

Epoch: 3 Train Loss 0.035 Time taken: 581.44 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 22.83 secs| Accuracy 98.22
T\P      0       1
0       5278        0031        
1       0129        5179        

Accuracy 98.493
Time taken: 33.7 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.552 Time taken: 400.02 secs

    Epoch: 4 Validation Loss 0.349 Time taken: 1.62 secs| Accuracy 84.437
T\P      0       1
0       0169        0029        
1       0050        0206        

Accuracy 82.599
Time taken: 2.06 secs
2276.569 secs

Sun May  1 14:58:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=687
Sun May  1 14:58:35 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.842 GB
33 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 687,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.137 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 584.65 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 22.86 secs| Accuracy 97.754

Epoch: 2 Train Loss 0.06 Time taken: 584.4 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.69 secs| Accuracy 98.135

Epoch: 3 Train Loss 0.035 Time taken: 576.55 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.81 secs| Accuracy 98.29
T\P      0       1
0       5271        0038        
1       0083        5225        

Accuracy 98.86
Time taken: 33.92 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.53 Time taken: 399.76 secs

    Epoch: 4 Validation Loss 0.327 Time taken: 1.63 secs| Accuracy 83.444
T\P      0       1
0       0152        0046        
1       0016        0240        

Accuracy 86.344
Time taken: 2.08 secs
2274.349 secs

Sun May  1 15:37:33 2022


======================================================================
----------------------------------------------------------------------
                run_ID=688
Sun May  1 15:37:37 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.926 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 688,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.866 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.186 Time taken: 575.64 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 22.72 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.057 Time taken: 580.58 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.65 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.034 Time taken: 573.06 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 22.75 secs| Accuracy 98.29
T\P      0       1
0       5258        0051        
1       0087        5221        

Accuracy 98.7
Time taken: 33.75 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.511 Time taken: 395.86 secs

    Epoch: 4 Validation Loss 0.404 Time taken: 1.63 secs| Accuracy 80.132
T\P      0       1
0       0182        0016        
1       0063        0193        

Accuracy 82.599
Time taken: 2.05 secs
2253.55 secs

Sun May  1 16:16:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=689
Sun May  1 16:16:19 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.933 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 689,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.0 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 580.21 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 22.78 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 582.46 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.59 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.035 Time taken: 575.4 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 22.72 secs| Accuracy 98.375
T\P      0       1
0       5216        0093        
1       0070        5238        

Accuracy 98.465
Time taken: 33.93 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.481 Time taken: 399.1 secs

    Epoch: 4 Validation Loss 0.36 Time taken: 1.61 secs| Accuracy 81.457
T\P      0       1
0       0141        0057        
1       0011        0245        

Accuracy 85.022
Time taken: 2.12 secs
2264.591 secs

Sun May  1 16:55:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=690
Sun May  1 16:55:11 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.027 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 690,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.059 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.196 Time taken: 553.93 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.77 secs| Accuracy 97.966

Epoch: 2 Train Loss 0.065 Time taken: 554.67 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 22.92 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.044 Time taken: 552.78 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.0 secs| Accuracy 98.404
T\P      0       1
0       5227        0082        
1       0071        5237        

Accuracy 98.559
Time taken: 34.0 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.625 Time taken: 386.84 secs

    Epoch: 4 Validation Loss 0.439 Time taken: 1.8 secs| Accuracy 82.781
T\P      0       1
0       0163        0035        
1       0045        0211        

Accuracy 82.379
Time taken: 2.31 secs
2176.777 secs

Sun May  1 17:32:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=691
Sun May  1 17:32:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.891 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 691,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.984 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 551.58 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.85 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.066 Time taken: 552.45 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.86 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.042 Time taken: 553.14 secs

    Epoch: 3 Validation Loss 0.065 Time taken: 22.89 secs| Accuracy 98.135
T\P      0       1
0       5266        0043        
1       0119        5189        

Accuracy 98.474
Time taken: 33.94 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.552 Time taken: 386.79 secs

    Epoch: 4 Validation Loss 0.344 Time taken: 1.93 secs| Accuracy 85.43
T\P      0       1
0       0165        0033        
1       0047        0209        

Accuracy 82.379
Time taken: 2.26 secs
2173.383 secs

Sun May  1 18:09:49 2022


======================================================================
----------------------------------------------------------------------
                run_ID=692
Sun May  1 18:09:54 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.22 GB
34 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 692,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.965 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 553.23 secs

    Epoch: 1 Validation Loss 0.078 Time taken: 22.83 secs| Accuracy 97.655

Epoch: 2 Train Loss 0.064 Time taken: 553.46 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 22.93 secs| Accuracy 98.149

Epoch: 3 Train Loss 0.041 Time taken: 555.15 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 22.97 secs| Accuracy 98.234
T\P      0       1
0       5267        0042        
1       0102        5206        

Accuracy 98.644
Time taken: 34.01 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.624 Time taken: 392.53 secs

    Epoch: 4 Validation Loss 0.452 Time taken: 1.47 secs| Accuracy 80.795
T\P      0       1
0       0135        0063        
1       0030        0226        

Accuracy 79.515
Time taken: 1.91 secs
2184.833 secs

Sun May  1 18:47:21 2022


======================================================================
----------------------------------------------------------------------
                run_ID=693
Sun May  1 18:47:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.163 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 693,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.491 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 578.42 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.91 secs| Accuracy 97.881

Epoch: 2 Train Loss 0.064 Time taken: 577.61 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.85 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.041 Time taken: 576.31 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.98 secs| Accuracy 98.347
T\P      0       1
0       5261        0048        
1       0087        5221        

Accuracy 98.728
Time taken: 33.94 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.519 Time taken: 416.35 secs

    Epoch: 4 Validation Loss 0.445 Time taken: 1.68 secs| Accuracy 79.47
T\P      0       1
0       0179        0019        
1       0071        0185        

Accuracy 80.176
Time taken: 2.24 secs
2277.111 secs

Sun May  1 19:26:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=694
Sun May  1 19:26:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -1.333 GB
29 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 694,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.221 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 550.9 secs

    Epoch: 1 Validation Loss 0.08 Time taken: 22.88 secs| Accuracy 97.739

Epoch: 2 Train Loss 0.064 Time taken: 553.54 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.7 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.044 Time taken: 554.28 secs

    Epoch: 3 Validation Loss 0.074 Time taken: 22.76 secs| Accuracy 97.98
T\P      0       1
0       5175        0134        
1       0057        5251        

Accuracy 98.201
Time taken: 33.73 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.517 Time taken: 393.04 secs

    Epoch: 4 Validation Loss 0.365 Time taken: 1.71 secs| Accuracy 83.775
T\P      0       1
0       0166        0032        
1       0027        0229        

Accuracy 87.004
Time taken: 2.13 secs
2179.763 secs

Sun May  1 20:03:38 2022


======================================================================
----------------------------------------------------------------------
                run_ID=676
Wed May  4 12:01:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.143 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 676,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.383 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 642.76 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.96 secs| Accuracy 98.036

Epoch: 2 Train Loss 0.062 Time taken: 643.04 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.9 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.034 Time taken: 637.05 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 23.04 secs| Accuracy 98.361
T\P      0       1
0       5271        0038        
1       0094        5214        

Accuracy 98.757
Time taken: 34.24 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.517 Time taken: 432.32 secs

    Epoch: 4 Validation Loss 0.293 Time taken: 1.72 secs| Accuracy 86.755
T\P      0       1
0       0163        0035        
1       0035        0221        

Accuracy 84.581
Time taken: 2.16 secs
2486.626 secs

Wed May  4 12:43:41 2022


======================================================================
----------------------------------------------------------------------
                run_ID=677
Wed May  4 12:43:46 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.917 GB
32 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 677,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.158 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 641.97 secs

    Epoch: 1 Validation Loss 0.077 Time taken: 22.95 secs| Accuracy 97.697

Epoch: 2 Train Loss 0.058 Time taken: 643.09 secs

    Epoch: 2 Validation Loss 0.065 Time taken: 22.96 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.028 Time taken: 641.08 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.05 secs| Accuracy 98.531
T\P      0       1
0       5264        0045        
1       0078        5230        

Accuracy 98.841
Time taken: 34.19 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.519 Time taken: 433.11 secs

    Epoch: 4 Validation Loss 0.312 Time taken: 1.7 secs| Accuracy 82.45
T\P      0       1
0       0150        0048        
1       0018        0238        

Accuracy 85.463
Time taken: 2.26 secs
2488.449 secs

Wed May  4 13:26:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=678
Wed May  4 13:26:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.094 GB
36 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 678,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.027 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 644.12 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.88 secs| Accuracy 97.966

Epoch: 2 Train Loss 0.06 Time taken: 643.41 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.99 secs| Accuracy 98.093

Epoch: 3 Train Loss 0.034 Time taken: 643.35 secs

    Epoch: 3 Validation Loss 0.068 Time taken: 23.11 secs| Accuracy 98.079
T\P      0       1
0       5199        0110        
1       0059        5249        

Accuracy 98.408
Time taken: 34.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.516 Time taken: 431.75 secs

    Epoch: 4 Validation Loss 0.379 Time taken: 1.89 secs| Accuracy 83.113
T\P      0       1
0       0154        0044        
1       0022        0234        

Accuracy 85.463
Time taken: 2.27 secs
2493.73 secs

Wed May  4 14:09:05 2022


======================================================================
----------------------------------------------------------------------
                run_ID=679
Wed May  4 14:09:10 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.026 GB
35 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 679,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "only embedding layer",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.075 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.194 Time taken: 644.16 secs

    Epoch: 1 Validation Loss 0.08 Time taken: 23.03 secs| Accuracy 97.739

Epoch: 2 Train Loss 0.061 Time taken: 641.32 secs

    Epoch: 2 Validation Loss 0.066 Time taken: 23.18 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.035 Time taken: 644.74 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 23.01 secs| Accuracy 98.333
T\P      0       1
0       5207        0102        
1       0061        5247        

Accuracy 98.465
Time taken: 34.01 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.562 Time taken: 437.6 secs

    Epoch: 4 Validation Loss 0.354 Time taken: 1.72 secs| Accuracy 84.106
T\P      0       1
0       0167        0031        
1       0029        0227        

Accuracy 86.784
Time taken: 2.16 secs
2497.628 secs

Wed May  4 14:51:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=726
Thu May  5 17:49:43 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.75 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 726,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 8.776 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.184 Time taken: 595.34 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.02 secs| Accuracy 97.895

Epoch: 2 Train Loss 0.054 Time taken: 591.68 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 23.03 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.03 Time taken: 595.25 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 22.71 secs| Accuracy 98.319
T\P      0       1
0       5245        0064        
1       0079        5229        

Accuracy 98.653
Time taken: 33.61 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.571 Time taken: 395.87 secs

    Epoch: 4 Validation Loss 0.301 Time taken: 1.98 secs| Accuracy 85.43
T\P      0       1
0       0160        0038        
1       0020        0236        

Accuracy 87.225
Time taken: 2.35 secs
2306.266 secs

Thu May  5 18:29:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=727
Thu May  5 18:29:18 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -2.687 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 727,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.949 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.2 Time taken: 584.15 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 23.08 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.058 Time taken: 588.95 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 23.33 secs| Accuracy 98.389

Epoch: 3 Train Loss 0.035 Time taken: 589.96 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.51 secs| Accuracy 98.502
T\P      0       1
0       5251        0058        
1       0076        5232        

Accuracy 98.738
Time taken: 34.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.523 Time taken: 397.06 secs

    Epoch: 4 Validation Loss 0.423 Time taken: 1.89 secs| Accuracy 82.45
T\P      0       1
0       0163        0035        
1       0024        0232        

Accuracy 87.004
Time taken: 2.38 secs
2291.477 secs

Thu May  5 19:08:32 2022


======================================================================
----------------------------------------------------------------------
                run_ID=728
Thu May  5 19:08:36 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.79 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 728,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.981 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.198 Time taken: 587.28 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.06 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.061 Time taken: 587.8 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.17 secs| Accuracy 98.361

Epoch: 3 Train Loss 0.036 Time taken: 586.76 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 23.21 secs| Accuracy 97.951
T\P      0       1
0       5141        0168        
1       0047        5261        

Accuracy 97.975
Time taken: 34.39 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.537 Time taken: 397.81 secs

    Epoch: 4 Validation Loss 0.36 Time taken: 2.09 secs| Accuracy 84.768
T\P      0       1
0       0183        0015        
1       0065        0191        

Accuracy 82.379
Time taken: 2.46 secs
2290.475 secs

Thu May  5 19:47:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=729
Thu May  5 19:47:53 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.957 GB
34 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 729,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.984 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 657.77 secs

    Epoch: 1 Validation Loss 0.084 Time taken: 23.18 secs| Accuracy 97.57

Epoch: 2 Train Loss 0.056 Time taken: 633.25 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.17 secs| Accuracy 98.29

Epoch: 3 Train Loss 0.029 Time taken: 588.1 secs

    Epoch: 3 Validation Loss 0.059 Time taken: 22.99 secs| Accuracy 98.206
T\P      0       1
0       5233        0076        
1       0065        5243        

Accuracy 98.672
Time taken: 34.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.627 Time taken: 396.85 secs

    Epoch: 4 Validation Loss 0.372 Time taken: 2.07 secs| Accuracy 84.437
T\P      0       1
0       0144        0054        
1       0019        0237        

Accuracy 83.921
Time taken: 2.54 secs
2408.407 secs

Thu May  5 20:29:08 2022


======================================================================
----------------------------------------------------------------------
                run_ID=730
Thu May  5 20:29:12 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.929 GB
33 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 730,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.058 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.19 Time taken: 587.06 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.16 secs| Accuracy 98.163

Epoch: 2 Train Loss 0.057 Time taken: 588.16 secs

    Epoch: 2 Validation Loss 0.065 Time taken: 23.25 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.035 Time taken: 587.85 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 23.34 secs| Accuracy 98.488
T\P      0       1
0       5237        0072        
1       0083        5225        

Accuracy 98.54
Time taken: 34.12 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.459 Time taken: 406.34 secs

    Epoch: 4 Validation Loss 0.296 Time taken: 2.05 secs| Accuracy 86.093
T\P      0       1
0       0176        0022        
1       0042        0214        

Accuracy 85.903
Time taken: 2.54 secs
2299.463 secs

Thu May  5 21:08:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=731
Thu May  5 21:08:40 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.839 GB
31 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 731,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.105 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.184 Time taken: 578.17 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.15 secs| Accuracy 97.994

Epoch: 2 Train Loss 0.056 Time taken: 578.69 secs

    Epoch: 2 Validation Loss 0.061 Time taken: 23.04 secs| Accuracy 98.121

Epoch: 3 Train Loss 0.032 Time taken: 578.29 secs

    Epoch: 3 Validation Loss 0.079 Time taken: 23.09 secs| Accuracy 98.079
T\P      0       1
0       5189        0120        
1       0056        5252        

Accuracy 98.342
Time taken: 34.21 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.623 Time taken: 395.99 secs

    Epoch: 4 Validation Loss 0.327 Time taken: 1.98 secs| Accuracy 83.113
T\P      0       1
0       0149        0049        
1       0019        0237        

Accuracy 85.022
Time taken: 2.36 secs
2262.533 secs

Thu May  5 21:47:24 2022


======================================================================
----------------------------------------------------------------------
                run_ID=732
Thu May  5 21:47:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.975 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 732,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.911 GB
25 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.199 Time taken: 580.84 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 23.1 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.063 Time taken: 579.82 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 23.14 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.036 Time taken: 584.69 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.21 secs| Accuracy 98.46
T\P      0       1
0       5245        0064        
1       0072        5236        

Accuracy 98.719
Time taken: 34.09 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.525 Time taken: 400.54 secs

    Epoch: 4 Validation Loss 0.416 Time taken: 1.85 secs| Accuracy 83.444
T\P      0       1
0       0174        0024        
1       0038        0218        

Accuracy 86.344
Time taken: 2.49 secs
2278.083 secs

Thu May  5 22:26:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=733
Thu May  5 22:26:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.874 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 733,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.019 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.202 Time taken: 582.74 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 23.23 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.063 Time taken: 582.76 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.11 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.036 Time taken: 581.48 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 23.13 secs| Accuracy 98.333
T\P      0       1
0       5211        0098        
1       0057        5251        

Accuracy 98.54
Time taken: 34.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.468 Time taken: 394.73 secs

    Epoch: 4 Validation Loss 0.31 Time taken: 2.04 secs| Accuracy 86.093
T\P      0       1
0       0186        0012        
1       0053        0203        

Accuracy 85.683
Time taken: 2.48 secs
2272.364 secs

Thu May  5 23:05:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=734
Thu May  5 23:05:31 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.943 GB
31 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 734,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.021 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.19 Time taken: 581.07 secs

    Epoch: 1 Validation Loss 0.083 Time taken: 23.14 secs| Accuracy 97.499

Epoch: 2 Train Loss 0.055 Time taken: 580.83 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.11 secs| Accuracy 98.305

Epoch: 3 Train Loss 0.031 Time taken: 581.37 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 22.86 secs| Accuracy 98.22
T\P      0       1
0       5230        0079        
1       0078        5230        

Accuracy 98.521
Time taken: 33.95 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.613 Time taken: 395.55 secs

    Epoch: 4 Validation Loss 0.343 Time taken: 1.89 secs| Accuracy 86.093
T\P      0       1
0       0168        0030        
1       0034        0222        

Accuracy 85.903
Time taken: 2.45 secs
2270.997 secs

Thu May  5 23:44:23 2022



======================================================================
----------------------------------------------------------------------
                run_ID=735
Thu May  5 23:56:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 5.142 GB
31 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 735,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0, layer1, layer2",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.964 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 578.81 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.93 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.062 Time taken: 583.63 secs

    Epoch: 2 Validation Loss 0.063 Time taken: 22.99 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.034 Time taken: 582.05 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 23.16 secs| Accuracy 98.248
T\P      0       1
0       5241        0068        
1       0072        5236        

Accuracy 98.681
Time taken: 34.32 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.468 Time taken: 396.46 secs

    Epoch: 4 Validation Loss 0.309 Time taken: 1.93 secs| Accuracy 85.43
T\P      0       1
0       0153        0045        
1       0020        0236        

Accuracy 85.683
Time taken: 2.37 secs
2271.46 secs

Fri May  6 00:35:16 2022


======================================================================
----------------------------------------------------------------------
                run_ID=736
Fri May  6 00:35:20 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.776 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 736,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.038 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.186 Time taken: 558.14 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.12 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.059 Time taken: 554.0 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 23.05 secs| Accuracy 98.163

Epoch: 3 Train Loss 0.038 Time taken: 556.56 secs

    Epoch: 3 Validation Loss 0.072 Time taken: 22.93 secs| Accuracy 98.036
T\P      0       1
0       5189        0120        
1       0053        5255        

Accuracy 98.371
Time taken: 33.94 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.637 Time taken: 395.88 secs

    Epoch: 4 Validation Loss 0.344 Time taken: 1.89 secs| Accuracy 82.119
T\P      0       1
0       0137        0061        
1       0019        0237        

Accuracy 82.379
Time taken: 2.25 secs
2194.372 secs

Fri May  6 01:12:54 2022


======================================================================
----------------------------------------------------------------------
                run_ID=737
Fri May  6 01:12:58 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.806 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 737,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 0.403 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.201 Time taken: 556.87 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 23.22 secs| Accuracy 98.177

Epoch: 2 Train Loss 0.068 Time taken: 556.86 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 23.08 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.044 Time taken: 555.84 secs

    Epoch: 3 Validation Loss 0.052 Time taken: 22.99 secs| Accuracy 98.531
T\P      0       1
0       5240        0069        
1       0079        5229        

Accuracy 98.606
Time taken: 34.06 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.567 Time taken: 395.41 secs

    Epoch: 4 Validation Loss 0.419 Time taken: 2.06 secs| Accuracy 81.126
T\P      0       1
0       0172        0026        
1       0049        0207        

Accuracy 83.48
Time taken: 2.62 secs
2196.785 secs

Fri May  6 01:50:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=738
Fri May  6 01:50:41 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.811 GB
33 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 738,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.036 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.198 Time taken: 553.64 secs

    Epoch: 1 Validation Loss 0.075 Time taken: 23.06 secs| Accuracy 97.725

Epoch: 2 Train Loss 0.069 Time taken: 557.97 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 23.1 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.045 Time taken: 556.3 secs

    Epoch: 3 Validation Loss 0.059 Time taken: 23.23 secs| Accuracy 98.163
T\P      0       1
0       5185        0124        
1       0056        5252        

Accuracy 98.305
Time taken: 34.01 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.5 Time taken: 395.36 secs

    Epoch: 4 Validation Loss 0.384 Time taken: 1.93 secs| Accuracy 83.444
T\P      0       1
0       0175        0023        
1       0054        0202        

Accuracy 83.04
Time taken: 2.56 secs
2195.675 secs

Fri May  6 02:28:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=739
Fri May  6 02:28:25 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.845 GB
32 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 739,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.982 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.193 Time taken: 570.82 secs

    Epoch: 1 Validation Loss 0.079 Time taken: 23.86 secs| Accuracy 97.683

Epoch: 2 Train Loss 0.063 Time taken: 555.43 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 23.31 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.041 Time taken: 572.45 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.55 secs| Accuracy 98.432
T\P      0       1
0       5245        0064        
1       0078        5230        

Accuracy 98.663
Time taken: 34.19 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.615 Time taken: 402.94 secs

    Epoch: 4 Validation Loss 0.389 Time taken: 2.19 secs| Accuracy 84.768
T\P      0       1
0       0174        0024        
1       0047        0209        

Accuracy 84.361
Time taken: 2.64 secs
2235.723 secs

Fri May  6 03:06:45 2022


======================================================================
----------------------------------------------------------------------
                run_ID=740
Fri May  6 03:06:50 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.939 GB
30 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 740,
    "message": "freeze half of bert",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "160",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.057 GB
21 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.191 Time taken: 555.49 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.86 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.067 Time taken: 554.32 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 22.78 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.041 Time taken: 557.43 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 23.05 secs| Accuracy 98.347
T\P      0       1
0       5229        0080        
1       0063        5245        

Accuracy 98.653
Time taken: 34.04 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.511 Time taken: 392.15 secs

    Epoch: 4 Validation Loss 0.368 Time taken: 1.81 secs| Accuracy 82.781
T\P      0       1
0       0168        0030        
1       0050        0206        

Accuracy 82.379
Time taken: 2.25 secs
2188.668 secs

Fri May  6 03:44:18 2022


======================================================================
----------------------------------------------------------------------
                run_ID=751
Fri May  6 03:44:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.606 GB
29 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 751,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.003 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.195 Time taken: 596.35 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.13 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.057 Time taken: 593.3 secs

    Epoch: 2 Validation Loss 0.06 Time taken: 22.97 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.034 Time taken: 593.57 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 22.92 secs| Accuracy 98.163

Epoch: 4 Train Loss 0.019 Time taken: 596.79 secs

    Epoch: 4 Validation Loss 0.078 Time taken: 23.19 secs| Accuracy 98.107
T\P      0       1
0       5221        0088        
1       0068        5240        

Accuracy 98.531
Time taken: 34.16 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.549 Time taken: 396.26 secs

    Epoch: 5 Validation Loss 0.34 Time taken: 2.06 secs| Accuracy 81.788
T\P      0       1
0       0123        0075        
1       0005        0251        

Accuracy 82.379
Time taken: 2.54 secs
2929.608 secs

Fri May  6 04:34:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=752
Fri May  6 04:34:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.864 GB
30 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 752,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.069 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 596.93 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 23.06 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.062 Time taken: 597.38 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.99 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.034 Time taken: 596.81 secs

    Epoch: 3 Validation Loss 0.07 Time taken: 23.05 secs| Accuracy 98.036

Epoch: 4 Train Loss 0.021 Time taken: 596.62 secs

    Epoch: 4 Validation Loss 0.06 Time taken: 23.03 secs| Accuracy 98.389
T\P      0       1
0       5231        0078        
1       0073        5235        

Accuracy 98.578
Time taken: 34.14 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.513 Time taken: 396.17 secs

    Epoch: 5 Validation Loss 0.285 Time taken: 1.98 secs| Accuracy 87.417
T\P      0       1
0       0169        0029        
1       0035        0221        

Accuracy 85.903
Time taken: 2.61 secs
2941.038 secs

Fri May  6 05:24:12 2022


======================================================================
----------------------------------------------------------------------
                run_ID=753
Fri May  6 05:24:16 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.857 GB
31 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)...


{
    "run_ID": 753,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.9 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.205 Time taken: 594.67 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 23.06 secs| Accuracy 97.923

Epoch: 2 Train Loss 0.06 Time taken: 601.1 secs

    Epoch: 2 Validation Loss 0.068 Time taken: 23.2 secs| Accuracy 98.22

Epoch: 3 Train Loss 0.032 Time taken: 603.01 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 23.11 secs| Accuracy 98.333

Epoch: 4 Train Loss 0.02 Time taken: 601.46 secs

    Epoch: 4 Validation Loss 0.068 Time taken: 23.22 secs| Accuracy 98.446
T\P      0       1
0       5209        0100        
1       0053        5255        

Accuracy 98.559
Time taken: 34.03 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.635 Time taken: 400.07 secs

    Epoch: 5 Validation Loss 0.343 Time taken: 2.05 secs| Accuracy 80.795
T\P      0       1
0       0147        0051        
1       0019        0237        

Accuracy 84.581
Time taken: 2.53 secs
2953.252 secs

Fri May  6 06:14:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=754
Fri May  6 06:14:33 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.797 GB
33 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 754,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.016 GB
22 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 599.35 secs

    Epoch: 1 Validation Loss 0.069 Time taken: 22.99 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.058 Time taken: 593.02 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 22.97 secs| Accuracy 98.177

Epoch: 3 Train Loss 0.033 Time taken: 584.7 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 23.02 secs| Accuracy 98.29

Epoch: 4 Train Loss 0.02 Time taken: 586.24 secs

    Epoch: 4 Validation Loss 0.061 Time taken: 23.04 secs| Accuracy 98.276
T\P      0       1
0       5249        0060        
1       0078        5230        

Accuracy 98.7
Time taken: 33.93 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.571 Time taken: 389.3 secs

    Epoch: 5 Validation Loss 0.355 Time taken: 1.8 secs| Accuracy 85.099
T\P      0       1
0       0175        0023        
1       0046        0210        

Accuracy 84.802
Time taken: 2.32 secs
2905.119 secs

Fri May  6 07:04:03 2022


======================================================================
----------------------------------------------------------------------
                run_ID=755
Fri May  6 07:04:07 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.859 GB
30 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 755,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.028 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.188 Time taken: 584.78 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 23.13 secs| Accuracy 97.937

Epoch: 2 Train Loss 0.056 Time taken: 587.9 secs

    Epoch: 2 Validation Loss 0.062 Time taken: 23.12 secs| Accuracy 98.206

Epoch: 3 Train Loss 0.032 Time taken: 587.16 secs

    Epoch: 3 Validation Loss 0.058 Time taken: 22.97 secs| Accuracy 98.474

Epoch: 4 Train Loss 0.02 Time taken: 587.49 secs

    Epoch: 4 Validation Loss 0.056 Time taken: 23.02 secs| Accuracy 98.46
T\P      0       1
0       5257        0052        
1       0085        5223        

Accuracy 98.71
Time taken: 34.12 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.51 Time taken: 388.66 secs

    Epoch: 5 Validation Loss 0.312 Time taken: 2.05 secs| Accuracy 85.099
T\P      0       1
0       0170        0028        
1       0034        0222        

Accuracy 86.344
Time taken: 2.37 secs
2889.99 secs

Fri May  6 07:53:14 2022


======================================================================
----------------------------------------------------------------------
                run_ID=766
Sat May  7 09:36:22 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 6.114 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 766,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.095 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 568.49 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.51 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.052 Time taken: 571.12 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.54 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.031 Time taken: 571.91 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 22.59 secs| Accuracy 98.446

Epoch: 4 Train Loss 0.02 Time taken: 570.96 secs

    Epoch: 4 Validation Loss 0.057 Time taken: 22.62 secs| Accuracy 98.446
T\P      0       1
0       5223        0086        
1       0058        5250        

Accuracy 98.644
Time taken: 33.52 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.502 Time taken: 374.89 secs

    Epoch: 5 Validation Loss 0.329 Time taken: 1.76 secs| Accuracy 82.45
T\P      0       1
0       0128        0070        
1       0013        0243        

Accuracy 81.718
Time taken: 2.2 secs
2808.832 secs

Sat May  7 10:24:05 2022


======================================================================
----------------------------------------------------------------------
                run_ID=767
Sat May  7 10:24:08 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.859 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 767,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.99 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.203 Time taken: 570.8 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 22.57 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.061 Time taken: 570.81 secs

    Epoch: 2 Validation Loss 0.05 Time taken: 22.59 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.034 Time taken: 569.24 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 22.63 secs| Accuracy 98.375

Epoch: 4 Train Loss 0.021 Time taken: 570.76 secs

    Epoch: 4 Validation Loss 0.045 Time taken: 22.56 secs| Accuracy 98.686
T\P      0       1
0       5244        0065        
1       0072        5236        

Accuracy 98.71
Time taken: 33.37 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.588 Time taken: 372.55 secs

    Epoch: 5 Validation Loss 0.411 Time taken: 1.74 secs| Accuracy 83.113
T\P      0       1
0       0169        0029        
1       0050        0206        

Accuracy 82.599
Time taken: 2.1 secs
2804.026 secs

Sat May  7 11:11:48 2022


======================================================================
----------------------------------------------------------------------
                run_ID=768
Sat May  7 11:11:52 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.969 GB
32 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 768,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.999 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.197 Time taken: 567.28 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.44 secs| Accuracy 98.079

Epoch: 2 Train Loss 0.063 Time taken: 567.77 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.74 secs| Accuracy 98.404

Epoch: 3 Train Loss 0.036 Time taken: 569.21 secs

    Epoch: 3 Validation Loss 0.057 Time taken: 22.52 secs| Accuracy 98.29

Epoch: 4 Train Loss 0.023 Time taken: 569.8 secs

    Epoch: 4 Validation Loss 0.064 Time taken: 22.79 secs| Accuracy 98.375
T\P      0       1
0       5202        0107        
1       0065        5243        

Accuracy 98.38
Time taken: 33.68 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.557 Time taken: 373.88 secs

    Epoch: 5 Validation Loss 0.313 Time taken: 1.77 secs| Accuracy 86.424
T\P      0       1
0       0176        0022        
1       0039        0217        

Accuracy 86.564
Time taken: 2.15 secs
2800.562 secs

Sat May  7 11:59:31 2022


======================================================================
----------------------------------------------------------------------
                run_ID=769
Sat May  7 11:59:34 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.874 GB
40 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 769,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.988 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.192 Time taken: 566.95 secs

    Epoch: 1 Validation Loss 0.084 Time taken: 22.62 secs| Accuracy 97.415

Epoch: 2 Train Loss 0.058 Time taken: 566.29 secs

    Epoch: 2 Validation Loss 0.049 Time taken: 22.58 secs| Accuracy 98.502

Epoch: 3 Train Loss 0.033 Time taken: 566.06 secs

    Epoch: 3 Validation Loss 0.056 Time taken: 22.6 secs| Accuracy 98.404

Epoch: 4 Train Loss 0.02 Time taken: 566.62 secs

    Epoch: 4 Validation Loss 0.063 Time taken: 22.52 secs| Accuracy 98.22
T\P      0       1
0       5213        0096        
1       0056        5252        

Accuracy 98.568
Time taken: 33.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.711 Time taken: 373.07 secs

    Epoch: 5 Validation Loss 0.372 Time taken: 1.79 secs| Accuracy 81.788
T\P      0       1
0       0145        0053        
1       0027        0229        

Accuracy 82.379
Time taken: 2.2 secs
2788.146 secs

Sat May  7 12:47:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=770
Sat May  7 12:47:13 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.873 GB
27 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 770,
    "message": "585 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.993 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.189 Time taken: 568.87 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 22.49 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.06 Time taken: 566.35 secs

    Epoch: 2 Validation Loss 0.056 Time taken: 22.53 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.035 Time taken: 566.38 secs

    Epoch: 3 Validation Loss 0.05 Time taken: 22.5 secs| Accuracy 98.333

Epoch: 4 Train Loss 0.023 Time taken: 566.87 secs

    Epoch: 4 Validation Loss 0.06 Time taken: 22.56 secs| Accuracy 98.474
T\P      0       1
0       5244        0065        
1       0069        5239        

Accuracy 98.738
Time taken: 33.47 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 5 Train Loss 0.532 Time taken: 372.87 secs

    Epoch: 5 Validation Loss 0.325 Time taken: 1.75 secs| Accuracy 85.099
T\P      0       1
0       0160        0038        
1       0027        0229        

Accuracy 85.683
Time taken: 2.2 secs
2791.105 secs

Sat May  7 13:34:37 2022


======================================================================
----------------------------------------------------------------------
                run_ID=776
Sun May  8 20:48:09 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.649 GB
27 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 776,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 10.854 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.223 Time taken: 572.18 secs

    Epoch: 1 Validation Loss 0.071 Time taken: 22.63 secs| Accuracy 97.881

Epoch: 2 Train Loss 0.059 Time taken: 574.69 secs

    Epoch: 2 Validation Loss 0.071 Time taken: 22.74 secs| Accuracy 98.05

Epoch: 3 Train Loss 0.029 Time taken: 574.09 secs

    Epoch: 3 Validation Loss 0.073 Time taken: 22.78 secs| Accuracy 98.361
T\P      0       1
0       5265        0044        
1       0091        5217        

Accuracy 98.728
Time taken: 33.62 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.588 Time taken: 377.17 secs

    Epoch: 4 Validation Loss 0.352 Time taken: 1.77 secs| Accuracy 81.126
T\P      0       1
0       0128        0070        
1       0005        0251        

Accuracy 83.48
Time taken: 2.13 secs
2226.879 secs

Sun May  8 21:26:11 2022


======================================================================
----------------------------------------------------------------------
                run_ID=777
Sun May  8 21:26:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.11 GB
28 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)...


{
    "run_ID": 777,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 132,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.719 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.228 Time taken: 570.61 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.62 secs| Accuracy 97.641

Epoch: 2 Train Loss 0.061 Time taken: 578.26 secs

    Epoch: 2 Validation Loss 0.052 Time taken: 22.64 secs| Accuracy 98.333

Epoch: 3 Train Loss 0.032 Time taken: 577.02 secs

    Epoch: 3 Validation Loss 0.053 Time taken: 22.56 secs| Accuracy 98.474
T\P      0       1
0       5266        0043        
1       0106        5202        

Accuracy 98.597
Time taken: 33.41 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.484 Time taken: 375.94 secs

    Epoch: 4 Validation Loss 0.381 Time taken: 1.52 secs| Accuracy 80.795
T\P      0       1
0       0179        0019        
1       0058        0198        

Accuracy 83.04
Time taken: 2.0 secs
2228.603 secs

Sun May  8 22:04:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=778
Sun May  8 22:04:21 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.729 GB
28 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 778,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 133,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 0.704 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.241 Time taken: 570.29 secs

    Epoch: 1 Validation Loss 0.073 Time taken: 22.63 secs| Accuracy 98.064

Epoch: 2 Train Loss 0.061 Time taken: 571.6 secs

    Epoch: 2 Validation Loss 0.084 Time taken: 22.67 secs| Accuracy 97.556

Epoch: 3 Train Loss 0.036 Time taken: 571.56 secs

    Epoch: 3 Validation Loss 0.071 Time taken: 22.61 secs| Accuracy 97.966
T\P      0       1
0       5196        0113        
1       0060        5248        

Accuracy 98.371
Time taken: 33.46 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.553 Time taken: 375.62 secs

    Epoch: 4 Validation Loss 0.316 Time taken: 1.74 secs| Accuracy 85.099
T\P      0       1
0       0184        0014        
1       0057        0199        

Accuracy 84.361
Time taken: 2.12 secs
2216.293 secs

Sun May  8 22:42:13 2022


======================================================================
----------------------------------------------------------------------
                run_ID=779
Sun May  8 22:42:17 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.613 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(1.)...


{
    "run_ID": 779,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 134,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.631 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.219 Time taken: 586.45 secs

    Epoch: 1 Validation Loss 0.07 Time taken: 23.3 secs| Accuracy 97.838

Epoch: 2 Train Loss 0.056 Time taken: 587.1 secs

    Epoch: 2 Validation Loss 0.051 Time taken: 23.35 secs| Accuracy 98.375

Epoch: 3 Train Loss 0.027 Time taken: 591.61 secs

    Epoch: 3 Validation Loss 0.049 Time taken: 23.05 secs| Accuracy 98.517
T\P      0       1
0       5263        0046        
1       0105        5203        

Accuracy 98.578
Time taken: 34.39 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.565 Time taken: 383.05 secs

    Epoch: 4 Validation Loss 0.293 Time taken: 1.82 secs| Accuracy 87.086
T\P      0       1
0       0166        0032        
1       0039        0217        

Accuracy 84.361
Time taken: 2.09 secs
2278.907 secs

Sun May  8 23:21:10 2022


======================================================================
----------------------------------------------------------------------
                run_ID=780
Sun May  8 23:21:14 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 1.562 GB
28 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(1.)tensor(1.)tensor(1.)tensor(0.)...


{
    "run_ID": 780,
    "message": "401",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 135,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc4",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=500, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=500, out_features=300, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=300, out_features=100, bias=True)
  (activation3): ReLU()
  (fc4): Linear(in_features=100, out_features=1, bias=True)
  (activation4): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.533 GB
20 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.226 Time taken: 567.85 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 22.73 secs| Accuracy 97.951

Epoch: 2 Train Loss 0.056 Time taken: 567.77 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 22.71 secs| Accuracy 98.276

Epoch: 3 Train Loss 0.033 Time taken: 568.03 secs

    Epoch: 3 Validation Loss 0.061 Time taken: 22.72 secs| Accuracy 97.909
T\P      0       1
0       5170        0139        
1       0042        5266        

Accuracy 98.295
Time taken: 33.59 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.547 Time taken: 372.58 secs

    Epoch: 4 Validation Loss 0.341 Time taken: 1.79 secs| Accuracy 85.43
T\P      0       1
0       0179        0019        
1       0051        0205        

Accuracy 84.581
Time taken: 2.26 secs
2203.575 secs

Sun May  8 23:58:53 2022



======================================================================
----------------------------------------------------------------------
                run_ID=881
Mon May 16 00:38:38 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.381 GB
21 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 881,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.082 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.523 Time taken: 44.62 secs

    Epoch: 1 Validation Loss 0.319 Time taken: 5.54 secs| Accuracy 90.0

Epoch: 2 Train Loss 0.221 Time taken: 45.19 secs

Epoch: 3 Train Loss 0.146 Time taken: 45.36 secs

    Epoch: 3 Validation Loss 0.252 Time taken: 5.61 secs| Accuracy 90.933

Epoch: 4 Train Loss 0.114 Time taken: 45.57 secs

    Epoch: 4 Validation Loss 0.254 Time taken: 5.61 secs| Accuracy 90.267
T\P      0       1
0       0413        0087        
1       0007        0393        

Accuracy 89.556
Time taken: 6.67 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
223.522 secs

Mon May 16 00:43:07 2022


======================================================================
----------------------------------------------------------------------
                run_ID=882
Mon May 16 00:43:10 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.411 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 882,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.988 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.489 Time taken: 45.18 secs

    Epoch: 1 Validation Loss 0.262 Time taken: 5.64 secs| Accuracy 93.6

Epoch: 2 Train Loss 0.227 Time taken: 45.67 secs

    Epoch: 2 Validation Loss 0.238 Time taken: 5.71 secs| Accuracy 92.0

Epoch: 3 Train Loss 0.134 Time taken: 45.77 secs

    Epoch: 3 Validation Loss 0.211 Time taken: 5.66 secs| Accuracy 91.733

Epoch: 4 Train Loss 0.095 Time taken: 45.64 secs

    Epoch: 4 Validation Loss 0.242 Time taken: 5.66 secs| Accuracy 92.4
T\P      0       1
0       0453        0047        
1       0020        0380        

Accuracy 92.556
Time taken: 6.75 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
227.609 secs

Mon May 16 00:47:44 2022


======================================================================
----------------------------------------------------------------------
                run_ID=883
Mon May 16 00:47:47 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.184 GB
23 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 883,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.961 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.473 Time taken: 44.88 secs

    Epoch: 1 Validation Loss 0.309 Time taken: 5.57 secs| Accuracy 89.733

Epoch: 2 Train Loss 0.23 Time taken: 45.25 secs

    Epoch: 2 Validation Loss 0.206 Time taken: 5.61 secs| Accuracy 92.933

Epoch: 3 Train Loss 0.142 Time taken: 45.37 secs

    Epoch: 3 Validation Loss 0.216 Time taken: 5.64 secs| Accuracy 92.0

Epoch: 4 Train Loss 0.097 Time taken: 45.5 secs

    Epoch: 4 Validation Loss 0.239 Time taken: 5.64 secs| Accuracy 91.333
T\P      0       1
0       0431        0069        
1       0008        0392        

Accuracy 91.444
Time taken: 6.65 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
224.057 secs

Mon May 16 00:52:19 2022


======================================================================
----------------------------------------------------------------------
                run_ID=884
Mon May 16 00:52:22 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.074 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 884,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.921 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.487 Time taken: 45.28 secs

    Epoch: 1 Validation Loss 0.271 Time taken: 5.66 secs| Accuracy 92.933

Epoch: 2 Train Loss 0.238 Time taken: 45.7 secs

    Epoch: 2 Validation Loss 0.215 Time taken: 5.67 secs| Accuracy 92.4

Epoch: 3 Train Loss 0.154 Time taken: 45.92 secs

    Epoch: 3 Validation Loss 0.219 Time taken: 5.7 secs| Accuracy 91.733

Epoch: 4 Train Loss 0.1 Time taken: 45.84 secs

    Epoch: 4 Validation Loss 0.287 Time taken: 5.72 secs| Accuracy 90.667
T\P      0       1
0       0460        0040        
1       0025        0375        

Accuracy 92.778
Time taken: 6.73 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
226.144 secs

Mon May 16 00:56:56 2022


======================================================================
----------------------------------------------------------------------
                run_ID=885
Mon May 16 00:56:59 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.104 GB
21 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 885,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.784 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.487 Time taken: 45.2 secs

    Epoch: 1 Validation Loss 0.291 Time taken: 5.6 secs| Accuracy 91.2

Epoch: 2 Train Loss 0.215 Time taken: 45.38 secs

    Epoch: 2 Validation Loss 0.222 Time taken: 5.66 secs| Accuracy 91.867

Epoch: 3 Train Loss 0.134 Time taken: 45.61 secs

    Epoch: 3 Validation Loss 0.206 Time taken: 5.67 secs| Accuracy 92.933

Epoch: 4 Train Loss 0.085 Time taken: 45.73 secs

    Epoch: 4 Validation Loss 0.231 Time taken: 5.7 secs| Accuracy 92.133
T\P      0       1
0       0444        0056        
1       0014        0386        

Accuracy 92.222
Time taken: 6.72 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
225.071 secs

Mon May 16 01:01:30 2022


======================================================================
----------------------------------------------------------------------
                run_ID=886
Mon May 16 01:01:33 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.184 GB
24 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 886,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.916 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.44 Time taken: 49.1 secs

    Epoch: 1 Validation Loss 0.181 Time taken: 5.65 secs| Accuracy 96.667

Epoch: 2 Train Loss 0.113 Time taken: 49.26 secs

    Epoch: 2 Validation Loss 0.078 Time taken: 5.66 secs| Accuracy 98.667

Epoch: 3 Train Loss 0.074 Time taken: 49.29 secs

    Epoch: 3 Validation Loss 0.082 Time taken: 5.64 secs| Accuracy 98.133

Epoch: 4 Train Loss 0.048 Time taken: 49.25 secs

    Epoch: 4 Validation Loss 0.055 Time taken: 5.68 secs| Accuracy 98.667
T\P      0       1
0       0328        0172        
1       0002        0398        

Accuracy 80.667
Time taken: 6.68 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
239.863 secs

Mon May 16 01:06:20 2022


======================================================================
----------------------------------------------------------------------
                run_ID=887
Mon May 16 01:06:24 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.418 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 887,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.866 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.41 Time taken: 48.76 secs

    Epoch: 1 Validation Loss 0.153 Time taken: 5.59 secs| Accuracy 98.4

Epoch: 2 Train Loss 0.095 Time taken: 48.85 secs

    Epoch: 2 Validation Loss 0.084 Time taken: 5.61 secs| Accuracy 98.533

Epoch: 3 Train Loss 0.051 Time taken: 49.01 secs

    Epoch: 3 Validation Loss 0.118 Time taken: 5.64 secs| Accuracy 97.067

Epoch: 4 Train Loss 0.051 Time taken: 49.0 secs

    Epoch: 4 Validation Loss 0.071 Time taken: 5.61 secs| Accuracy 98.533
T\P      0       1
0       0327        0173        
1       0004        0396        

Accuracy 80.333
Time taken: 6.65 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
238.425 secs

Mon May 16 01:11:09 2022


======================================================================
----------------------------------------------------------------------
                run_ID=888
Mon May 16 01:11:12 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM -0.173 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 888,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.965 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.394 Time taken: 49.03 secs

    Epoch: 1 Validation Loss 0.142 Time taken: 5.65 secs| Accuracy 98.267

Epoch: 2 Train Loss 0.112 Time taken: 49.33 secs

    Epoch: 2 Validation Loss 0.084 Time taken: 5.69 secs| Accuracy 98.267

Epoch: 3 Train Loss 0.061 Time taken: 49.31 secs

    Epoch: 3 Validation Loss 0.059 Time taken: 5.69 secs| Accuracy 98.667

Epoch: 4 Train Loss 0.041 Time taken: 49.57 secs

    Epoch: 4 Validation Loss 0.07 Time taken: 5.77 secs| Accuracy 98.4
T\P      0       1
0       0303        0197        
1       0003        0397        

Accuracy 77.778
Time taken: 6.8 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
240.774 secs

Mon May 16 01:15:58 2022


======================================================================
----------------------------------------------------------------------
                run_ID=889
Mon May 16 01:16:02 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.17 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 889,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.035 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.389 Time taken: 48.96 secs

    Epoch: 1 Validation Loss 0.152 Time taken: 5.68 secs| Accuracy 97.867

Epoch: 2 Train Loss 0.098 Time taken: 49.17 secs

    Epoch: 2 Validation Loss 0.077 Time taken: 5.62 secs| Accuracy 98.8

Epoch: 3 Train Loss 0.043 Time taken: 49.23 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 5.63 secs| Accuracy 98.8

Epoch: 4 Train Loss 0.025 Time taken: 49.21 secs

    Epoch: 4 Validation Loss 0.059 Time taken: 5.67 secs| Accuracy 98.8
T\P      0       1
0       0350        0150        
1       0003        0397        

Accuracy 83.0
Time taken: 6.72 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
239.966 secs

Mon May 16 01:20:47 2022


======================================================================
----------------------------------------------------------------------
                run_ID=890
Mon May 16 01:20:50 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.221 GB
23 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 890,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.742 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.401 Time taken: 49.19 secs

    Epoch: 1 Validation Loss 0.135 Time taken: 5.68 secs| Accuracy 99.067

Epoch: 2 Train Loss 0.116 Time taken: 49.43 secs

    Epoch: 2 Validation Loss 0.107 Time taken: 5.67 secs| Accuracy 97.6

Epoch: 3 Train Loss 0.057 Time taken: 49.46 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 5.67 secs| Accuracy 98.667

Epoch: 4 Train Loss 0.043 Time taken: 49.31 secs

    Epoch: 4 Validation Loss 0.076 Time taken: 5.78 secs| Accuracy 98.0
T\P      0       1
0       0263        0237        
1       0002        0398        

Accuracy 73.444
Time taken: 6.74 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
241.333 secs

Mon May 16 01:25:39 2022


======================================================================
----------------------------------------------------------------------
                run_ID=891
Mon May 16 01:25:42 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.174 GB
22 seconds taken
Example: ... tensor(1.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 891,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.334 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.521 Time taken: 48.7 secs

    Epoch: 1 Validation Loss 0.31 Time taken: 5.58 secs| Accuracy 90.4

Epoch: 2 Train Loss 0.22 Time taken: 48.99 secs

    Epoch: 2 Validation Loss 0.243 Time taken: 5.62 secs| Accuracy 91.467

Epoch: 3 Train Loss 0.136 Time taken: 49.03 secs

    Epoch: 3 Validation Loss 0.22 Time taken: 5.63 secs| Accuracy 91.6

Epoch: 4 Train Loss 0.098 Time taken: 49.08 secs

    Epoch: 4 Validation Loss 0.242 Time taken: 5.64 secs| Accuracy 91.6
T\P      0       1
0       0437        0063        
1       0024        0376        

Accuracy 90.333
Time taken: 6.62 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
238.498 secs

Mon May 16 01:30:26 2022


======================================================================
----------------------------------------------------------------------
                run_ID=892
Mon May 16 01:30:30 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM -3.681 GB
22 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 892,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.866 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.487 Time taken: 49.1 secs

    Epoch: 1 Validation Loss 0.265 Time taken: 5.63 secs| Accuracy 92.8

Epoch: 2 Train Loss 0.221 Time taken: 49.33 secs

    Epoch: 2 Validation Loss 0.25 Time taken: 5.67 secs| Accuracy 91.2

Epoch: 3 Train Loss 0.136 Time taken: 49.31 secs

    Epoch: 3 Validation Loss 0.234 Time taken: 5.65 secs| Accuracy 91.067

Epoch: 4 Train Loss 0.092 Time taken: 49.27 secs

    Epoch: 4 Validation Loss 0.252 Time taken: 5.7 secs| Accuracy 92.8
T\P      0       1
0       0460        0040        
1       0024        0376        

Accuracy 92.889
Time taken: 6.71 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
240.308 secs

Mon May 16 01:35:17 2022


======================================================================
----------------------------------------------------------------------
                run_ID=893
Mon May 16 01:35:20 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.083 GB
21 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 893,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 128,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.874 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.472 Time taken: 48.69 secs

    Epoch: 1 Validation Loss 0.34 Time taken: 5.62 secs| Accuracy 87.867

Epoch: 2 Train Loss 0.218 Time taken: 49.29 secs

    Epoch: 2 Validation Loss 0.223 Time taken: 5.69 secs| Accuracy 91.2

Epoch: 3 Train Loss 0.133 Time taken: 49.6 secs

    Epoch: 3 Validation Loss 0.221 Time taken: 5.59 secs| Accuracy 92.267

Epoch: 4 Train Loss 0.094 Time taken: 49.51 secs

    Epoch: 4 Validation Loss 0.232 Time taken: 5.6 secs| Accuracy 91.333
T\P      0       1
0       0429        0071        
1       0008        0392        

Accuracy 91.222
Time taken: 6.69 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
239.984 secs

Mon May 16 01:40:06 2022


======================================================================
----------------------------------------------------------------------
                run_ID=894
Mon May 16 01:40:09 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 4.08 GB
23 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)tensor(0.)...


{
    "run_ID": 894,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 129,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.204 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.473 Time taken: 49.21 secs

    Epoch: 1 Validation Loss 0.257 Time taken: 5.69 secs| Accuracy 93.2

Epoch: 2 Train Loss 0.216 Time taken: 49.52 secs

    Epoch: 2 Validation Loss 0.227 Time taken: 5.74 secs| Accuracy 92.0

Epoch: 3 Train Loss 0.149 Time taken: 49.56 secs

    Epoch: 3 Validation Loss 0.231 Time taken: 5.62 secs| Accuracy 91.2

Epoch: 4 Train Loss 0.106 Time taken: 49.55 secs

    Epoch: 4 Validation Loss 0.241 Time taken: 5.63 secs| Accuracy 91.733
T\P      0       1
0       0444        0056        
1       0013        0387        

Accuracy 92.333
Time taken: 6.7 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
241.074 secs

Mon May 16 01:44:57 2022


======================================================================
----------------------------------------------------------------------
                run_ID=895
Mon May 16 01:45:01 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 1750 | Validation_set: 750 | Test_set: 900
Train_batches: 110 | Validation_batches: 47 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.179 GB
23 seconds taken
Example: ... tensor(1.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 895,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "nothing_frozen",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 130,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": false,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "dpil",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "cls",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.077 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.492 Time taken: 49.27 secs

    Epoch: 1 Validation Loss 0.288 Time taken: 5.72 secs| Accuracy 90.933

Epoch: 2 Train Loss 0.211 Time taken: 49.42 secs

    Epoch: 2 Validation Loss 0.26 Time taken: 5.74 secs| Accuracy 90.8

Epoch: 3 Train Loss 0.149 Time taken: 49.72 secs

    Epoch: 3 Validation Loss 0.186 Time taken: 5.75 secs| Accuracy 94.133

Epoch: 4 Train Loss 0.089 Time taken: 49.7 secs

    Epoch: 4 Validation Loss 0.274 Time taken: 5.7 secs| Accuracy 91.2
T\P      0       1
0       0425        0075        
1       0007        0393        

Accuracy 90.889
Time taken: 6.74 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
241.614 secs

Mon May 16 01:49:50 2022



======================================================================
----------------------------------------------------------------------
                run_ID=896
Mon May 16 17:48:46 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 270
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 17
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.72 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 896,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 5.06 GB
18 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=896
Mon May 16 17:55:34 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 3.31 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 896,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.65 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 435.74 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 57.19 secs| Accuracy 98.008


======================================================================
----------------------------------------------------------------------
                run_ID=896
Mon May 16 18:09:14 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 2.325 GB
27 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 896,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 126,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 11.489 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.154 Time taken: 436.52 secs

    Epoch: 1 Validation Loss 0.072 Time taken: 57.43 secs| Accuracy 98.008

Epoch: 2 Train Loss 0.053 Time taken: 447.38 secs

    Epoch: 2 Validation Loss 0.054 Time taken: 56.09 secs| Accuracy 98.248

Epoch: 3 Train Loss 0.026 Time taken: 449.57 secs

    Epoch: 3 Validation Loss 0.062 Time taken: 55.91 secs| Accuracy 98.347

Epoch: 4 Train Loss 0.019 Time taken: 443.23 secs

    Epoch: 4 Validation Loss 0.071 Time taken: 55.99 secs| Accuracy 98.404
T\P      0       1
0       0261        0239        
1       0003        0397        

Accuracy 73.111
Time taken: 7.24 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2023.838 secs

Mon May 16 18:43:50 2022


======================================================================
----------------------------------------------------------------------
                run_ID=897
Mon May 16 18:43:53 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.775 GB
26 seconds taken
Example: ... tensor(0.)tensor(0.)tensor(1.)tensor(0.)tensor(1.)tensor(0.)...


{
    "run_ID": 897,
    "message": "401",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 4,
    "rseed": 127,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.989 GB
16 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.152 Time taken: 442.65 secs

    Epoch: 1 Validation Loss 0.067 Time taken: 55.65 secs| Accuracy 98.149

Epoch: 2 Train Loss 0.053 Time taken: 449.04 secs

    Epoch: 2 Validation Loss 0.055 Time taken: 58.69 secs| Accuracy 98.488

Epoch: 3 Train Loss 0.03 Time taken: 445.04 secs

    Epoch: 3 Validation Loss 0.064 Time taken: 56.01 secs| Accuracy 98.404

Epoch: 4 Train Loss 0.017 Time taken: 431.66 secs

    Epoch: 4 Validation Loss 0.052 Time taken: 55.29 secs| Accuracy 98.474
T\P      0       1
0       0317        0183        
1       0004        0396        

Accuracy 79.222
Time taken: 7.46 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
2017.094 secs

Mon May 16 19:18:21 2022



======================================================================
----------------------------------------------------------------------
                run_ID=921
Wed Nov 23 11:14:03 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.858 GB
48 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 921,
    "message": "695 repeat",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "False",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )ce=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.214 GB
17 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.146 Time taken: 1344.74 secs

    Epoch: 1 Validation Loss 0.063 Time taken: 55.5 secs| Accuracy 98.135

Epoch: 2 Train Loss 0.052 Time taken: 1346.62 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 55.07 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.029 Time taken: 1341.7 secs

    Epoch: 3 Validation Loss 0.071 Time taken: 54.76 secs| Accuracy 98.05
T\P      0       1
0       0254        0246        
1       0005        0395        

Accuracy 72.111
Time taken: 7.58 secs

Including asha data now
Batch_size: 16
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 48 | Validation_batches: 19 | Test_batches: 29

Epoch: 4 Train Loss 0.48 Time taken: 898.28 secs

    Epoch: 4 Validation Loss 0.329 Time taken: 3.05 secs| Accuracy 85.43
T\P      0       1
0       0167        0031        
1       0026        0230        

Accuracy 87.445
Time taken: 4.15 secs
\Testing on dpil data now
Batch_size: 16
Test_set: 900
Test_batches: 57
5177.5 secs

Wed Nov 23 12:41:36 2022


======================================================================
----------------------------------------------------------------------
                run_ID=921
Wed Nov 23 13:31:46 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 900
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 57
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.785 GB
36 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 921,
    "message": "695 repeat",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )ce=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.336 GB
40 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)



======================================================================
----------------------------------------------------------------------
                run_ID=921
Wed Nov 23 13:42:25 2022
GPU is available GeForce RTX 2080 Ti 11 GB

Batch_size: 16
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 1106 | Validation_batches: 443 | Test_batches: 664
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.85 GB
60 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 921,
    "message": "695 repeat",
    "createData": true,
    "bs": 16,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )ce=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 3.29 GB
19 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.148 Time taken: 1363.38 secs

    Epoch: 1 Validation Loss 0.065 Time taken: 54.21 secs| Accuracy 97.909

Epoch: 2 Train Loss 0.052 Time taken: 1369.99 secs

    Epoch: 2 Validation Loss 0.053 Time taken: 54.2 secs| Accuracy 98.418

Epoch: 3 Train Loss 0.029 Time taken: 1363.79 secs

    Epoch: 3 Validation Loss 0.06 Time taken: 54.72 secs| Accuracy 98.389
T\P      0       1
0       5263        0046        
1       0095        5213        

Accuracy 98.672
Time taken: 82.08 secs

Including asha data now
Batch_size: 16
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 48 | Validation_batches: 19 | Test_batches: 29


======================================================================
----------------------------------------------------------------------
                run_ID=921
Wed Nov 23 15:28:01 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.869 GB
10 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 921,
    "message": "695 repeat",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )ce=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 1.528 GB
7 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.187 Time taken: 1038.53 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 33.41 secs| Accuracy 98.093

Epoch: 2 Train Loss 0.055 Time taken: 1036.09 secs

    Epoch: 2 Validation Loss 0.058 Time taken: 33.1 secs| Accuracy 98.347

Epoch: 3 Train Loss 0.032 Time taken: 1032.28 secs

    Epoch: 3 Validation Loss 0.063 Time taken: 33.84 secs| Accuracy 98.361
T\P      0       1
0       5239        0070        
1       0080        5228        

Accuracy 98.587
Time taken: 50.05 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.605 Time taken: 744.91 secs

    Epoch: 4 Validation Loss 0.327 Time taken: 1.91 secs| Accuracy 83.775
T\P      0       1
0       0179        0019        
1       0056        0200        

Accuracy 83.48
Time taken: 2.73 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
4017.933 secs

Wed Nov 23 16:35:29 2022


======================================================================
----------------------------------------------------------------------
                run_ID=922
Thu Nov 24 14:40:24 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.952 GB
40 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 922,
    "message": "695 repeat3",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.954 GB
23 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 188.81 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 26.22 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.056 Time taken: 191.78 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 26.58 secs| Accuracy 98.234

Epoch: 3 Train Loss 0.031 Time taken: 191.02 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 27.14 secs| Accuracy 98.319
T\P      0       1
0       5246        0063        
1       0092        5216        

Accuracy 98.54
Time taken: 40.1 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.639 Time taken: 802.22 secs

    Epoch: 4 Validation Loss 0.327 Time taken: 2.06 secs| Accuracy 85.762
T\P      0       1
0       0179        0019        
1       0053        0203        

Accuracy 84.141
Time taken: 2.58 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1537.202 secs

Thu Nov 24 15:07:35 2022


======================================================================
----------------------------------------------------------------------
                run_ID=923
Thu Nov 24 15:09:28 2022
GPU is available GeForce RTX 3090 24 GB

Batch_size: 32
Train set: 17695 | Validation_set: 7078 | Test_set: 10617
Train_batches: 553 | Validation_batches: 222 | Test_batches: 332
Memory taken on GPU 0.0 GB
Memory taken on RAM 0.946 GB
11 seconds taken
Example: ... tensor(0.)tensor(1.)tensor(0.)tensor(0.)tensor(1.)tensor(1.)...


{
    "run_ID": 923,
    "message": "695 repeat4 with validation and testing on inside training",
    "createData": true,
    "bs": 32,
    "BERT_FROZEN": "embedding, layer0",
    "bert_model_name": "bert-base-multilingual-cased",
    "model_options": "ai4bharat/indic-bert, bert-base-multilingual-cased, xlm-roberta-base, bert-base-multilingual-uncased, xlm-mlm-100-1280, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024",
    "alpha": 1e-05,
    "epochs": 3,
    "rseed": 131,
    "nw": 4,
    "cut_off": 0.51,
    "label_smoothing": false,
    "random_negative_sent": true,
    "train_ratio": 0.5,
    "val_ratio": 0.2,
    "max_len": 300,
    "data_source": "inshorts_full",
    "include_asha_data": "after inshorts data",
    "data_options": "asha_sentsim, inshorts_full, inshorts_400pairs",
    "architecture": "fc3",
    "arch_options": "SequenceClassification, fc1, fc2, fc3",
    "which_bert_embedding": "pooler_output",
    "bert_embedding_options": "cls, pooler_output",
    "my_datapath": "data/",
    "criterion": "BCE",
    "query_expansion": false,
    "inflate_negative_n_times": 0,
    "include_curated_negatives": false,
    "embedding_size": 768
}
myBert(
  (model): BertModel( bert-base-multilingual-cased )
  (fc1): Linear(in_features=768, out_features=400, bias=True)
  (activation1): ReLU()
  (fc2): Linear(in_features=400, out_features=100, bias=True)
  (activation2): ReLU()
  (fc3): Linear(in_features=100, out_features=1, bias=True)
  (activation3): Sigmoid()
  (criterion): BCELoss()
)
Memory taken on GPU 0.665 GB
Memory taken on RAM 4.98 GB
24 seconds taken

AdamW (
Parameter Group 0
    betas: (0.9, 0.999)
    correct_bias: True
    eps: 1e-06
    lr: 1e-05
    weight_decay: 0.0
)


Epoch: 1 Train Loss 0.185 Time taken: 263.39 secs

    Epoch: 1 Validation Loss 0.068 Time taken: 27.26 secs| Accuracy 98.05

Epoch: 2 Train Loss 0.052 Time taken: 267.48 secs

    Epoch: 2 Validation Loss 0.059 Time taken: 27.65 secs| Accuracy 98.262

Epoch: 3 Train Loss 0.031 Time taken: 267.01 secs

    Epoch: 3 Validation Loss 0.066 Time taken: 27.73 secs| Accuracy 98.446
T\P      0       1
0       5237        0072        
1       0086        5222        

Accuracy 98.512
Time taken: 41.22 secs

Including asha data now
Batch_size: 32
Train set: 756 | Validation_set: 302 | Test_set: 454
Train_batches: 24 | Validation_batches: 10 | Test_batches: 15

Epoch: 4 Train Loss 0.636 Time taken: 778.89 secs

    Epoch: 4 Validation Loss 0.316 Time taken: 2.43 secs| Accuracy 85.762
T\P      0       1
0       0153        0045        
1       0025        0231        

Accuracy 84.581
Time taken: 3.25 secs
\Testing on dpil data now
Batch_size: 32
Test_set: 900
Test_batches: 29
1712.81 secs

Thu Nov 24 15:38:51 2022

